[
  {
    "objectID": "karate/index.html",
    "href": "karate/index.html",
    "title": "Karate tabs",
    "section": "",
    "text": "This page is dedicated to tabs from the band karate made by fans. These tabs came from the original website (today offline) made by Willem Holthuis.\nGeoff Farina is the Karate’s guitarist, many thanks to him for writing such nice songs. Have fun playing the incredible music made by Karate and please feel free to correct the tabs by sending an email to his author if you find any mistakes or new interpretations."
  },
  {
    "objectID": "karate/index.html#in-the-fisk-tank-2005",
    "href": "karate/index.html#in-the-fisk-tank-2005",
    "title": "Karate tabs",
    "section": "In the Fisk tank (2005)",
    "text": "In the Fisk tank (2005)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nStrange Fruit\nAurelien"
  },
  {
    "objectID": "karate/index.html#pockets-2004",
    "href": "karate/index.html#pockets-2004",
    "title": "Karate tabs",
    "section": "Pockets (2004)",
    "text": "Pockets (2004)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nWith Age\nTomatokiller\n\n\nTow Truck\nAurelien & Philip Tanimura\n\n\nWater\nMito Gegic\n\n\nThe State I’m in aka Goode Buy from Cobbs Creek Park\nMito Gegic\n\n\nPines\nAurelien"
  },
  {
    "objectID": "karate/index.html#some-boots-2002",
    "href": "karate/index.html#some-boots-2002",
    "title": "Karate tabs",
    "section": "Some boots (2002)",
    "text": "Some boots (2002)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nOriginal Spies\nPhilip Tanimura\n\n\nFirst Release\nAurelien Ginolhac & Philip Tanimura\n\n\nAirport\nPhilip Tanimura\n\n\nCorduroy\nPhilip Tanimura\n\n\nRemain Relaxed\nPhilip Tanimura & Aurelien"
  },
  {
    "objectID": "karate/index.html#cancel-sing-2001",
    "href": "karate/index.html#cancel-sing-2001",
    "title": "Karate tabs",
    "section": "Cancel / Sing (2001)",
    "text": "Cancel / Sing (2001)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nSing\nPhilip Tanimura"
  },
  {
    "objectID": "karate/index.html#unsolved-2000",
    "href": "karate/index.html#unsolved-2000",
    "title": "Karate tabs",
    "section": "Unsolved (2000)",
    "text": "Unsolved (2000)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nSmall Fires\nPhilip Tanimura\n\n\nThe lived-but-yet-named\nPhilip Tanimura\n\n\nSever\nPhilip Tanimura\n\n\nThe Roots and the Ruins\nPhilip Tanimura\n\n\nThe Roots and the Ruins (bass)\nCraig Wynne\n\n\nNumber Six\nCraig Wynne\n\n\nNumber Six (bass)\nCraig Wynne\n\n\nOne Less Blues\nAurelien & Antoine L.\n\n\nThe Angels Just Have to Show\nAurelien & Antoine L.\n\n\nThe Halo of the Strange\nAurelien & Philip Tanimura\n\n\nThis Day Next Year\nPhilip Tanimura\n\n\nThis Day Next Year (solo)\nCraig Wynne"
  },
  {
    "objectID": "karate/index.html#the-bed-is-in-the-ocean-1998",
    "href": "karate/index.html#the-bed-is-in-the-ocean-1998",
    "title": "Karate tabs",
    "section": "The Bed is in the Ocean (1998)",
    "text": "The Bed is in the Ocean (1998)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nThere are Ghosts\nPhilip Tanimura\n\n\nDiazapam\nPhilip Tanimura\n\n\nThe Last Wars\nPhilip Tanimura\n\n\nBass Sound\nAurelien\n\n\nUp nigths\nAurelien\n\n\nThe Same Stars\nPhilip Tanimura\n\n\nOutside is the Drama\nPhilip Tanimura\n\n\nFatal Strategies\nAurelien"
  },
  {
    "objectID": "karate/index.html#in-place-of-real-insight-1997",
    "href": "karate/index.html#in-place-of-real-insight-1997",
    "title": "Karate tabs",
    "section": "In Place of Real Insight (1997)",
    "text": "In Place of Real Insight (1997)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nNew Martini\nEeYoRe\n\n\nBass Sound\nAurelien\n\n\nToday or Tomorrow\nAurelien\n\n\nThis Plus Slow Song\nCraig Wynne"
  },
  {
    "objectID": "karate/index.html#operation-sand-1997",
    "href": "karate/index.html#operation-sand-1997",
    "title": "Karate tabs",
    "section": "Operation Sand (1997)",
    "text": "Operation Sand (1997)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nOperation Sand\nPhilip Tanimura"
  },
  {
    "objectID": "karate/index.html#karate-1995",
    "href": "karate/index.html#karate-1995",
    "title": "Karate tabs",
    "section": "Karate (1995)",
    "text": "Karate (1995)\n\n\n\n\n\n\n\n\n\n\nSong\nAuthor\n\n\n\n\nIf You Can Hold Your Breath\nPete DeStefano\n\n\nEvery Sister\nPhilip Tanimura\n\n\nCaffeine or Me?\nPhilip Tanimura"
  },
  {
    "objectID": "posts/vertical-facetting/index.html",
    "href": "posts/vertical-facetting/index.html",
    "title": "Vertical faceting",
    "section": "",
    "text": "The question is: how can we plot one variable for faceting vertically?\nAnd bonus, since we want through time the plot of very different measurements, we must have a free y-scale."
  },
  {
    "objectID": "posts/vertical-facetting/index.html#tldr",
    "href": "posts/vertical-facetting/index.html#tldr",
    "title": "Vertical faceting",
    "section": "TL;DR",
    "text": "TL;DR\nUse either:\n\nThe dir = \"v\" argument of facet_wrap()\nThe rows = vars(variable) argument of facet_grid()"
  },
  {
    "objectID": "posts/vertical-facetting/index.html#demo",
    "href": "posts/vertical-facetting/index.html#demo",
    "title": "Vertical faceting",
    "section": "Demo",
    "text": "Demo\nWe will use a file from the Aranet4 sensor using the mobile app provided by the company. This sensor measures 4 parameters every 2 to 10 minutes which are:\n\nCO2, using the infrared absorption property of greenhouse gases like carbon dioxide.\nTemperature in Celsius degrees.\nRelative humidity (in %).\nAtmospheric pressure in _h Pa_. This latter variable is exported but displayed on the device, we will skip it.\n\nThe CO2 measurement is performed as follows (from the user manual):\n\n\n\nCO2 aranet\n\n\nAccording to the file extension, it is a comma-separated values. But if you look at the file with a text editor, you will see that values are surrounded by double quotes and number are using commas for decimal separator. Moreover, the dates are not in a standard format unfortunately. But it is a real life example data.\nreadr will detect the double quotes and deal with it automatically but the other issues needs to be fixed:\n\nFor the Time(dd/mm/yyyy) column, the datetime format is specified.\nFor the number decimals as commas, you need to set the locale to locale(decimal_mark = \",\")\nSkip the last column (Atmospheric Pressure) as we won’t plot it.\n\n\naranet &lt;- read_csv(\"https://biostat2.uni.lu/practicals/data/Gino_2022-08-14T22_53_06+0200.csv\",\n                   col_types = cols(`Time(dd/mm/yyyy)` = col_datetime(format = \"%d/%m/%Y %H:%M:%S\")),\n                   locale = locale(decimal_mark = \",\"),\n                   col_select = -`Atmospheric pressure(hPa)`) |&gt; \n  rename(Time = `Time(dd/mm/yyyy)`)\naranet\n\n# A tibble: 4,033 × 4\n   Time                `Carbon dioxide(ppm)` `Temperature(°C)`\n   &lt;dttm&gt;                              &lt;dbl&gt;             &lt;dbl&gt;\n 1 2022-07-31 22:53:43                   457              24  \n 2 2022-07-31 22:58:43                   444              24.1\n 3 2022-07-31 23:03:43                   457              24  \n 4 2022-07-31 23:08:43                   444              24  \n 5 2022-07-31 23:13:43                   445              24  \n 6 2022-07-31 23:18:43                   454              24.1\n 7 2022-07-31 23:23:43                   432              24.1\n 8 2022-07-31 23:28:43                   442              24.1\n 9 2022-07-31 23:33:43                   469              24  \n10 2022-07-31 23:38:43                   455              24  \n# ℹ 4,023 more rows\n# ℹ 1 more variable: `Relative humidity(%)` &lt;dbl&gt;"
  },
  {
    "objectID": "posts/vertical-facetting/index.html#reshape-the-variables-in-the-long-format",
    "href": "posts/vertical-facetting/index.html#reshape-the-variables-in-the-long-format",
    "title": "Vertical faceting",
    "section": "Reshape the variables in the long format",
    "text": "Reshape the variables in the long format\n\naranet |&gt;\n  pivot_longer(cols = -Time,\n               names_to = \"measure\",\n               values_to = \"value\") -&gt; aranet_long\naranet_long\n\n# A tibble: 12,099 × 3\n   Time                measure              value\n   &lt;dttm&gt;              &lt;chr&gt;                &lt;dbl&gt;\n 1 2022-07-31 22:53:43 Carbon dioxide(ppm)  457  \n 2 2022-07-31 22:53:43 Temperature(°C)       24  \n 3 2022-07-31 22:53:43 Relative humidity(%)  49  \n 4 2022-07-31 22:58:43 Carbon dioxide(ppm)  444  \n 5 2022-07-31 22:58:43 Temperature(°C)       24.1\n 6 2022-07-31 22:58:43 Relative humidity(%)  49  \n 7 2022-07-31 23:03:43 Carbon dioxide(ppm)  457  \n 8 2022-07-31 23:03:43 Temperature(°C)       24  \n 9 2022-07-31 23:03:43 Relative humidity(%)  49  \n10 2022-07-31 23:08:43 Carbon dioxide(ppm)  444  \n# ℹ 12,089 more rows"
  },
  {
    "objectID": "posts/vertical-facetting/index.html#plot-the-values-per-time-and-facet-per-measurement",
    "href": "posts/vertical-facetting/index.html#plot-the-values-per-time-and-facet-per-measurement",
    "title": "Vertical faceting",
    "section": "Plot the values per Time and facet per measurement",
    "text": "Plot the values per Time and facet per measurement\nMeasurement are one of the 3 variables: CO2 ppm, Temp and Relative Humidity.\nIn the facet command, you should free the y-axis with scales = \"free_y\"\nWe want variables in rows. Either facet_wrap() dir argument or facet_grid() rows = do the job.\n\nfacet_wrap() solution\nThe key part is to change the dir argument for vertical\n\nggplot(aranet_long, aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(vars(measure), scales = \"free_y\",\n             # panel titles next to the y axis\n             strip.position = \"bottom\", # use left for mimicking facet_grid below\n             dir = \"v\") +\n  scale_x_datetime(breaks = scales::date_breaks(\"1 day\"),\n                   date_labels = \"%b %d\") +\n  labs(x = NULL, y = NULL) +\n  theme_minimal(14)\n\n\n\n\n\n\n\n\n\n\n\n\nfacet_grid() solution\nfaacet_grid() is designed for two variables faceting, but we can omit one.\nThe key part is to use only the `rows`` argument for vertical panels.\n\nggplot(aranet_long, aes(x = Time, y = value)) +\n  geom_line() +\n  facet_grid(rows = vars(measure), scales = \"free_y\",\n             # panel titles next to the y axis\n             switch = \"y\") +\n  scale_x_datetime(breaks = scales::date_breaks(\"1 day\"),\n                   date_labels = \"%b %d\") +\n  labs(x = NULL, y = NULL) +\n  theme_minimal(14)"
  },
  {
    "objectID": "posts/targets-demos/index.html",
    "href": "posts/targets-demos/index.html",
    "title": "{targets} demos",
    "section": "",
    "text": "Targets is an  package that is\n\nFunction-oriented Make-like declarative workflows for \n\nMain author: William Landau. See the targets manual for an extensive documentation.\nThe goal is to create so-called targets that are significant steps that linked between one another. Those links create the dependencies, and once one target run successfully and its upstream dependencies are up-to-date, they are no reason to run it again. Time/Computing intensive steps are then cached in the store.\nInvalidation of a target arises when:\n\nUpstream targets invalidate (or input files checksum for special format = \"file\")\nCode of the targets changed\nPackage used was updated"
  },
  {
    "objectID": "posts/targets-demos/index.html#a-workflow-manager-for",
    "href": "posts/targets-demos/index.html#a-workflow-manager-for",
    "title": "{targets} demos",
    "section": "",
    "text": "Targets is an  package that is\n\nFunction-oriented Make-like declarative workflows for \n\nMain author: William Landau. See the targets manual for an extensive documentation.\nThe goal is to create so-called targets that are significant steps that linked between one another. Those links create the dependencies, and once one target run successfully and its upstream dependencies are up-to-date, they are no reason to run it again. Time/Computing intensive steps are then cached in the store.\nInvalidation of a target arises when:\n\nUpstream targets invalidate (or input files checksum for special format = \"file\")\nCode of the targets changed\nPackage used was updated"
  },
  {
    "objectID": "posts/targets-demos/index.html#example-dataset-datasaurus",
    "href": "posts/targets-demos/index.html#example-dataset-datasaurus",
    "title": "{targets} demos",
    "section": "Example dataset: datasauRus ",
    "text": "Example dataset: datasauRus \nThe great package datasauRus offers a fake table which consists of 13 dataset (each of 142 observations) with 2 values x and y:\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# … with 1,836 more rows\nFor each the 4 demos, we use different versions of the same data:\n\nOne tabulated-separated-value (tsv) file of 1847 lines (1846 observations + 1 header)\nSame as before but with plotting functions in a separate  script\nOne folder that contains 13 tsv of 143 lines\nThree folders of 2, 4 and 7 tsv of 143 lines each"
  },
  {
    "objectID": "posts/targets-demos/index.html#multiple-projects-in-one-folder",
    "href": "posts/targets-demos/index.html#multiple-projects-in-one-folder",
    "title": "{targets} demos",
    "section": "Multiple projects in one folder",
    "text": "Multiple projects in one folder\nThis is supported by targets and described in the manual: projects. A config YAML file, _targets.yaml describe the 4 different projects, specifying the name of both:\n\nthe targets  script (actual definition of target)\nthe store folder name (where objects are cached and described)\n\nAdditional options or inheritance can be specified too.\nContent:\nds_linear:\n  store: _ds_1\n  script: _targets_ds_1.R\nds_fun_linear:\n  store: _ds_fun1\n  script: _targets_ds_fun1.R\nds_dynamic:\n  store: _ds_2\n  script: _targets_ds_2.R\nds_static:\n  store: _ds_3\n  script: _targets_ds_3.R\n  reporter_make: verbose_positives # do not display skipped targets"
  },
  {
    "objectID": "posts/targets-demos/index.html#packages-needed",
    "href": "posts/targets-demos/index.html#packages-needed",
    "title": "{targets} demos",
    "section": "Packages needed",
    "text": "Packages needed\nThose demos are using several packages, you can get the necessary ones by using renv. Once the repo cloned/downloaded:\nrenv::restore()\nto install a local library of the key packages."
  },
  {
    "objectID": "posts/targets-demos/index.html#one-file-linear-pipeline",
    "href": "posts/targets-demos/index.html#one-file-linear-pipeline",
    "title": "{targets} demos",
    "section": "One file, linear pipeline",
    "text": "One file, linear pipeline\nUsing the original tsv from the package datasauRus itself. targets allows to track the timestamp of an URL.\nHere we track https://raw.githubusercontent.com/jumpingrivers/datasauRus/main/inst/extdata/DatasaurusDozen-Long.tsv.\nSee the complete targets  script in _targets_ds_1.R and displayed dependencies as directed acyclic graph:\n\n\n\nds1\n\n\ntargets encourages using literate programing where a Rmarkdown document higher level comments and code, dependencies are based on the parsing of the tar_read() and tar_load() calls within it. This can be used as smart caching system where help focusing on the analysis report, leaving the computation for the  script _targets.R.\nFor this first example, the corresponding Rmd is ds1.Rmd. It will be rendered by the pipeline (target definition in tar_render()).\nTo run this example:\n# Specify which projet to use\nSys.setenv(TAR_PROJECT = \"ds_linear\")\n# Run what is needed like make in a Makefile\ntargets::tar_make()\nFor the first run, `tar_make()1 should output something like:\n&gt; targets::tar_make()\n• start target ds_file\n• built target ds_file [0.695 seconds]\n• start target ds\n• built target ds [0.176 seconds]\n• start target anim\n• built target anim [48.762 seconds]\n• start target all_facets\n• built target all_facets [0.007 seconds]\n• start target gif\n• built target gif [0.005 seconds]\n• start target report\n• built target report [4.144 seconds]\n• end pipeline [54.083 seconds]\nThe GIF animation takes roughly one minute, so it would be cumbersome to wait this time at each Rmarkdown knitting process. It is a good case for targets, the GIF will be re-run only if needed while you polish the Rmd report.\nSee the output of re-reruning tar_make() again:\n&gt; targets::tar_make()\n✔ skip target ds_file\n✔ skip target ds\n✔ skip target anim\n✔ skip target all_facets\n✔ skip target gif\n✔ skip target report\n✔ skip pipeline [0.27 seconds]\n0.27 seconds versus 54."
  },
  {
    "objectID": "posts/targets-demos/index.html#cleaner-coding-with-sourcing-functions",
    "href": "posts/targets-demos/index.html#cleaner-coding-with-sourcing-functions",
    "title": "{targets} demos",
    "section": "Cleaner coding with sourcing functions",
    "text": "Cleaner coding with sourcing functions\n_targets_ds_fun1.R is similar as _targets_ds_1.R except that plotting functions were placed in R/plotting.R which is sourced before the targets definition\nTargets definition is then cleaner to read:\nlist(\n  # track if distant file has changed\n  tar_url(ds_file, \"https://raw.githubusercontent.com/jumpingrivers/datasauRus/main/inst/extdata/DatasaurusDozen-Long.tsv\"),\n  tar_target(ds, read_tsv(ds_file, show_col_types = FALSE)),\n  tar_target(all_facets, facet_ds(ds)),\n  # animation is worth caching  ~ 1 min\n  tar_target(anim, anim_ds(ds), \n             packages = c(\"ggplot2\", \"gganimate\", \"gifski\")),\n  tar_file(gif, {\n    anim_save(\"ds.gif\", animation = anim, title_frame = TRUE)\n    # anim_save returns NULL, we need to get the file output path\n    \"ds.gif\"},\n             packages = c(\"gganimate\")),\n  tar_render(report, \"ds1.Rmd\")\n)"
  },
  {
    "objectID": "posts/targets-demos/index.html#one-folder-dynamic-branching",
    "href": "posts/targets-demos/index.html#one-folder-dynamic-branching",
    "title": "{targets} demos",
    "section": "One folder, dynamic branching",
    "text": "One folder, dynamic branching\nOften, input files are more than one. Of course, you don’t want to list them by hand and one want to apply similar treatment to each of them. Moving away from for loops, we embrace functional programming and let targets branching over the list of files and dynamically for it adapts to how many are present.\nThis is called dynamic branching and it contains the magic aggregation (like bind_rows()) when calling the target name.\nOn the filesystem, the folder data contains 13 files:\ndata/\n├── dset_10.tsv\n├── dset_11.tsv\n├── dset_12.tsv\n├── dset_13.tsv\n├── dset_1.tsv\n├── dset_2.tsv\n├── dset_3.tsv\n├── dset_4.tsv\n├── dset_5.tsv\n├── dset_6.tsv\n├── dset_7.tsv\n├── dset_8.tsv\n└── dset_9.tsv\nWhich once tracked by targets are:\n&gt; tar_read(dset) |&gt; \n    enframe()\n# A tibble: 13 × 2\n   name          value           \n   &lt;chr&gt;         &lt;chr&gt;           \n 1 dset_e814d3a7 data/dset_1.tsv \n 2 dset_96886f69 data/dset_10.tsv\n 3 dset_a4c9d9df data/dset_11.tsv\n 4 dset_02e8e253 data/dset_12.tsv\n 5 dset_39f18392 data/dset_13.tsv\n 6 dset_d74af7a4 data/dset_2.tsv \n 7 dset_55280675 data/dset_3.tsv \n 8 dset_80822375 data/dset_4.tsv \n 9 dset_020f0640 data/dset_5.tsv \n10 dset_0577d04d data/dset_6.tsv \n11 dset_66982b15 data/dset_7.tsv \n12 dset_3c9c9095 data/dset_8.tsv \n13 dset_fe11a7b7 data/dset_9.tsv \nFinally, the DAG is:\n\n\n\nds2\n\n\nYou see that all targets appears in blue, so outdated. This is the expected behavior of tar_files(). We don’t know in advance how many (if any) files are present, so the listing is checked all the time and downstream targets are then also outdated.\nHowever, the downstream targets are re-run only if needed\n\nIf input files changed\nIf code for those targets changed.\n\nSee example of re-running tar_make(). dset_files was run again, but no files were different so all the rest is skipped and the whole pipeline took 1.1 second.\n&gt; targets::tar_make()\n• start target dset_files\n• built target dset_files [0.702 seconds]\n✔ skip branch dset_6630d1f3\n✔ skip branch dset_f10c2c43\n✔ skip branch dset_c79e8ff6\n✔ skip branch dset_b1eac8ed\n[...]\n✔ skip branch plots_01ca2c35\n✔ skip branch plots_9fc19e45\n✔ skip branch plots_01f427e3\n✔ skip pattern plots\n✔ skip target report\n• end pipeline [1.172 seconds]\nDynamic branching scales great on the DAG since the number of branches can be reported, no additional items are created. to avoid this, we can switch to tar_files_input() which also automatically groups input files into batches to reduce overhead and increase the efficiency of parallel processing..\nSee the DAG with tar_files_input():\n\n\n\nds2"
  },
  {
    "objectID": "posts/targets-demos/index.html#several-folders-dynamic-within-static-branching",
    "href": "posts/targets-demos/index.html#several-folders-dynamic-within-static-branching",
    "title": "{targets} demos",
    "section": "Several folders, dynamic within static branching",
    "text": "Several folders, dynamic within static branching\nWe created a folder structure as we often have to deal with, 3 sub-folders of data:\ncircles\n├── dset_2.tsv\n└── dset_3.tsv\nlines\n├── dset_11.tsv\n├── dset_12.tsv\n├── dset_13.tsv\n├── dset_6.tsv\n├── dset_7.tsv\n├── dset_8.tsv\n└── dset_9.tsv\nothers\n├── dset_10.tsv\n├── dset_1.tsv\n├── dset_4.tsv\n└── dset_5.tsv\nEspecially with static branching, it is meaningful to check which commands are planned. See the manifest for this example:\n&gt; tar_manifest() |&gt; print(n = Inf)\n# A tibble: 21 × 3\n   name                 command                                                                                                   pattern\n   &lt;chr&gt;                &lt;chr&gt;                                                                                                     &lt;chr&gt;  \n 1 filenames_circles    \"fs::dir_ls(\\\"circles\\\", glob = \\\"*tsv\\\")\"                                                                NA     \n 2 filenames_others     \"fs::dir_ls(\\\"others\\\", glob = \\\"*tsv\\\")\"                                                                 NA     \n 3 filenames_lines      \"fs::dir_ls(\\\"lines\\\", glob = \\\"*tsv\\\")\"                                                                  NA     \n 4 files_circles        \"filenames_circles\"                                                                                       map(fi…\n 5 files_others         \"filenames_others\"                                                                                        map(fi…\n 6 files_lines          \"filenames_lines\"                                                                                         map(fi…\n 7 ds_circles           \"read_tsv(files_circles, show_col_types = FALSE)\"                                                         map(fi…\n 8 ds_others            \"read_tsv(files_others, show_col_types = FALSE)\"                                                          map(fi…\n 9 ds_lines             \"read_tsv(files_lines, show_col_types = FALSE)\"                                                           map(fi…\n10 summary_stat_circles \"summarise(ds_circles, m_x = mean(x), m_y = mean(y))\"                                                     map(ds…\n11 plots_circles        \"ggplot(ds_circles, aes(x, y)) + geom_point()\"                                                            map(ds…\n12 summary_stat_others  \"summarise(ds_others, m_x = mean(x), m_y = mean(y))\"                                                      map(ds…\n13 plots_others         \"ggplot(ds_others, aes(x, y)) + geom_point()\"                                                             map(ds…\n14 plots_lines          \"ggplot(ds_lines, aes(x, y)) + geom_point()\"                                                              map(ds…\n15 summary_stat_lines   \"summarise(ds_lines, m_x = mean(x), m_y = mean(y))\"                                                       map(ds…\n16 patch_plots_circles  \"wrap_plots(plots_circles) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\" NA     \n17 patch_plots_others   \"wrap_plots(plots_others) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\"  NA     \n18 patch_plots_lines    \"wrap_plots(plots_lines) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\"   NA     \n19 stat_summaries       \"dplyr::bind_rows(summary_stat_lines = summary_stat_lines, \\n     summary_stat_circles = summary_stat_ci… NA     \n20 plots_agg            \"wrap_plots(list(patch_plots_lines = patch_plots_lines, \\n     patch_plots_circles = patch_plots_circles… NA     \n21 report               \"tarchetypes::tar_render_run(path = \\\"ds3.Rmd\\\", args = list(input = \\\"ds3.Rmd\\\", \\n     knit_root_dir =… NA    \nYou see that we get meaningful names based on the 3 folders listed. Still we get dynamic branching for reading files inside each folder. The same treatment is performed on each 3 input folders but when we want/need to combine the parallel branches for a relevant aggregation, we use tar_combine(). Example of both aggregating tibbles or plots are exemplified as depicted below:\n\n\n\nds3"
  },
  {
    "objectID": "posts/targets-demos/index.html#demo-repository",
    "href": "posts/targets-demos/index.html#demo-repository",
    "title": "{targets} demos",
    "section": "Demo repository",
    "text": "Demo repository\nAll the corresponding files are available in the Gitlab repository"
  },
  {
    "objectID": "posts/panel-size/index.html",
    "href": "posts/panel-size/index.html",
    "title": "Same dot sizes",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ek.plot) # renv::install(\"koncina/ek.plot\")\n\nThis is how to create dots of the same size and optimized width using Eric Koncina package: ek.plot.\n\ndot_size &lt;- 4\nset.seed(1)\np &lt;- tidyr::expand_grid(row = letters[1:5], column = as.character(1:3)) |&gt; \n    mutate(size = sample(0:5, size = 15, replace = TRUE)) |&gt; \n    ggplot(aes(x = column, y = row, size = size)) +\n    geom_point() +\n    geom_point(shape = 21, size = dot_size, fill = NA, colour = \"gray\") +\n    scale_size(range = c(0, dot_size), limits = c(0, 5), breaks = c(0, 2, 5)) +\n    theme_bw() +\n    theme(\n        legend.key.height =  unit(0.5, \"lines\")\n    ) \np\n\n\n\n\n\n\n\n\n\nset_panel_size(p,\n                             width = 3 * unit(dot_size * ggplot2:::.pt, \"pt\"),\n                             height = 5 * unit(dot_size * ggplot2:::.pt, \"pt\")) |&gt; \n    write_plot(\"panel-size.png\")\n\n[1] \"panel-size.png\""
  },
  {
    "objectID": "posts/teething-process/index.html",
    "href": "posts/teething-process/index.html",
    "title": "teething",
    "section": "",
    "text": "Teething can be tough for kids, and so for the parents. For our first kid, it was actually pretty painful. Then, I wanted to see how synchronize the second kid would be with the first one."
  },
  {
    "objectID": "posts/teething-process/index.html#input-data",
    "href": "posts/teething-process/index.html#input-data",
    "title": "teething",
    "section": "Input data",
    "text": "Input data\nThis was an old input, now I would use the datapasta rstudio addin by Miles Mc Bain to get a tribble from any spreadsheet.\n\ntibble::tribble(\n      ~kid,        ~date,                ~teeth,\n   \"Anouk\", \"2014-03-31\", \"incisive inf cent D\",\n   \"Anouk\", \"2014-04-18\", \"incisive inf cent G\",\n   \"Anouk\", \"2014-07-02\", \"incisive sup cent D\",\n   \"Anouk\", \"2014-06-09\", \"incisive sup cent G\",\n   \"Anouk\", \"2014-07-03\",  \"incisive sup lat G\",\n   \"Anouk\", \"2014-07-26\",  \"incisive sup lat D\",\n   \"Anouk\", \"2014-08-12\",  \"incisive inf lat D\",\n   \"Anouk\", \"2014-10-12\",  \"incisive inf lat G\",\n   \"Anouk\", \"2015-01-04\",        \"prem 1 sup G\",\n   \"Anouk\", \"2015-01-11\",        \"prem 1 sup G\",\n   \"Anouk\", \"2015-01-11\",        \"prem 1 inf D\",\n   \"Anouk\", \"2015-02-01\",        \"prem 1 inf D\",\n   \"Anouk\", \"2015-02-01\",        \"canine sup G\",\n   \"Anouk\", \"2015-02-12\",        \"canine sup D\",\n   \"Anouk\", \"2015-03-28\",        \"canine inf D\",\n   \"Anouk\", \"2015-04-04\",        \"canine inf G\", # imputed data\n   \"Anouk\", \"2015-10-15\",        \"prem 2 inf G\",\n   \"Anouk\", \"2015-10-15\",        \"prem 2 inf D\",\n   \"Anouk\", \"2015-12-21\",        \"prem 2 sup D\",\n   \"Anouk\", \"2016-03-01\",        \"prem 2 sup G\",\n   \"Irène\", \"2017-05-11\", \"incisive inf cent D\",\n   \"Irène\", \"2017-05-18\", \"incisive inf cent G\",\n   \"Irène\", \"2017-07-11\", \"incisive sup cent D\",\n   \"Irène\", \"2017-07-25\", \"incisive sup cent G\",\n   \"Irène\", \"2017-08-03\", \"incisive sup lat D\",\n   \"Irène\", \"2017-08-04\", \"incisive inf lat D\",\n   \"Irène\", \"2017-08-04\", \"incisive inf lat G\",\n   \"Irène\", \"2017-08-23\", \"incisive sup lat G\",\n   \"Irène\", \"2017-09-30\",        \"prem 1 inf D\",\n   \"Irène\", \"2017-09-20\",        \"prem 1 inf G\",\n   \"Irène\", \"2017-10-22\",        \"prem 1 sup D\",\n   \"Irène\", \"2017-10-20\",        \"prem 1 sup G\",\n   \"Irène\", \"2018-04-04\",        \"canine sup D\",\n   \"Irène\", \"2018-04-28\",        \"canine inf D\",\n   \"Irène\", \"2018-05-15\",        \"canine sup G\",\n   \"Irène\", \"2018-05-25\",        \"prem 2 inf G\",\n   \"Irène\", \"2018-06-01\",        \"canine inf G\",\n   \"Irène\", \"2018-06-07\",        \"prem 2 inf D\",\n   \"Irène\", \"2018-09-04\",        \"prem 2 sup D\") -&gt; teeth\n\n\ncount(teeth, kid)\n\n# A tibble: 2 × 2\n  kid       n\n  &lt;chr&gt; &lt;int&gt;\n1 Anouk    20\n2 Irène    19\n\n\nhttps://en.wikipedia.org/wiki/Dental_notation\nPrimary Dentition\n        upper right - 5             upper left - 6\n             55 54 53 52 51 | 61 62 63 64 65 \n          R --------------------------------- L\n             85 84 83 82 81 | 71 72 73 74 75 \n        lower right - 8             lower left - 7\n\n I - incisor\n C - canine\n P - premolar\n M - molar"
  },
  {
    "objectID": "posts/teething-process/index.html#plotting",
    "href": "posts/teething-process/index.html#plotting",
    "title": "teething",
    "section": "Plotting",
    "text": "Plotting\n\nteeth %&gt;%\n  mutate(date = parse_date(date),\n         t_teeth = word(teeth, 1),\n         age = case_when(kid == \"Anouk\" ~ date - ymd(\"20130828\"),\n                         kid == \"Irène\" ~ date - ymd(\"20160922\")),\n         age2 = as.numeric(age) / 365) -&gt; teeth_tp\nteeth_tp %&gt;%\n  group_by(kid) %&gt;%\n  arrange(age) %&gt;%\n  mutate(n_teeth = row_number()) %&gt;% \n  ggplot(aes(x = age2, y = n_teeth, colour = kid), width = 20) +\n  geom_line() +\n  geom_point(aes(shape = t_teeth)) +\n  scale_y_continuous(breaks = c(1, seq(5, 20, 5))) +\n  theme_bw(14) +\n  labs(x = \"age (years)\",\n       y = \"# teeth\",\n       shape = \"type\")"
  },
  {
    "objectID": "posts/teething-process/index.html#animate",
    "href": "posts/teething-process/index.html#animate",
    "title": "teething",
    "section": "Animate",
    "text": "Animate\n\nlibrary(gganimate)\nteeth_tp %&gt;%\n   group_by(kid) %&gt;%\n   arrange(age) %&gt;%\n   mutate(n_teeth = row_number()) %&gt;% \n   ggplot(aes(x = age2, y = n_teeth, colour = kid)) +\n   geom_line() +\n   # to try transition_reveal()\n   transition_reveal(kid, n_teeth) +\n   #ease_aes(\"linear\") +\n   shadow_trail() +\n   geom_point(aes(shape = t_teeth), size = 5) +\n   scale_y_continuous(breaks = c(1, seq(5, 20, 5))) +\n   scale_color_manual(values = wesanderson::wes_palette(\"GrandBudapest1\")[3:4]) + \n   theme_bw(14) +\n   labs(x = \"age (years)\",\n        y = \"# teeth\",\n        shape = \"type\") -&gt; tp\nanimate(tp, nframes = 60)\nanim_save(\"teething.gif\", last_animation())"
  },
  {
    "objectID": "posts/tweening-a-poisson-distribution/index.html",
    "href": "posts/tweening-a-poisson-distribution/index.html",
    "title": "tweening a Poisson distribution",
    "section": "",
    "text": "with gganimate and tweenr\nusing \\(\\lambda\\) from 5 to 55 with a step of 5.\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(tweenr) # devtools::install_github(\"thomasp85/tweenr\")\n\ncrossing(tibble(x = 1:50),\n         tibble(lambda = c(2, seq(5, 55, 5)))) %&gt;%\n  mutate(dpoi = dpois(x, lambda = lambda)) -&gt; poi_df\n\nmy_list &lt;- map(unique(poi_df$lambda), ~ filter(poi_df, lambda == .x) %&gt;% as.data.frame())\n\ntween_states(my_list, tweenlength = 7, statelength = 1, ease = \"cubic-in-out\", nframes = 50) %&gt;%\n  \n  ggplot(aes(x, dpoi, frame = .frame)) + \n  geom_col(position = \"dodge\", width = 2.0) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = NULL) +\n  theme_classic(16) -&gt; p\n\ngganimate(p, title_frame = FALSE, ani.width = 600, ani.height = 450, interval = 0.1, filename = \"poisson.gif\")"
  },
  {
    "objectID": "posts/latex-modern-cv/index.html#install-missing-packages",
    "href": "posts/latex-modern-cv/index.html#install-missing-packages",
    "title": "LaTex modern CV",
    "section": "install missing packages",
    "text": "install missing packages\ntlmgr install moderncv\ntlmgr install xcolor\ntlmgr install colortbl\ntlmgr install fancyhdr\ntlmgr install microtype\ntlmgr install pgf # to install tikz\ntlmgr install textgreek\ntlmgr install fontawesome\ntlmgr install lastpage\ntlmgr install marvosym \ntlmgr install greek-fontenc\ntlmgr install babel-greek\ntlmgr update --self --all\ntlmgr path add"
  },
  {
    "objectID": "posts/latex-modern-cv/index.html#issue-with-fontawesome",
    "href": "posts/latex-modern-cv/index.html#issue-with-fontawesome",
    "title": "LaTex modern CV",
    "section": "issue with fontawesome",
    "text": "issue with fontawesome\n\nreload / install fonts fmtutil-sys --all\n\nside effect, the Roboto condensed for Robert Rudis, ggplot2 theme is now working nicely!"
  },
  {
    "objectID": "posts/latex-modern-cv/index.html#solve-issue-of-nfss-corrupted",
    "href": "posts/latex-modern-cv/index.html#solve-issue-of-nfss-corrupted",
    "title": "LaTex modern CV",
    "section": "solve issue of NFSS corrupted",
    "text": "solve issue of NFSS corrupted\nfollowing this answer:\nthe error was completed with For encoding scheme LGR the defaults cmr/m/n do not form a valid font shape\ntlmgr install cbfonts\nallows to get the Greek letters working. Why the greek-fontenc and babel-greek was not sufficient? I don’t know"
  },
  {
    "objectID": "posts/latex-modern-cv/index.html#workflow",
    "href": "posts/latex-modern-cv/index.html#workflow",
    "title": "LaTex modern CV",
    "section": "Workflow",
    "text": "Workflow\n\nbibliography from pubmed to .bib using http://www.bioinformatics.org/texmed/ as biblio.bib\nfix gamma character {I}{F}{N}γ responses to {I}{F}{N}{$\\gamma$} responses\nfirst compilation, generate biblio.aux\nin plainyr_rev.bst, specify \"Ginolhac, A\" so my name gets underlined and bolded\nrun bibtex biblio to generate biblio.bbl\nsecond compilation to generate the pdf"
  },
  {
    "objectID": "posts/starting-learning-rust/index.html",
    "href": "posts/starting-learning-rust/index.html",
    "title": "Starting to learn Rust",
    "section": "",
    "text": "I am very happy with  but I want to learn something new, another programming language. The desire comes with several needs\n\n\n\nA compiled language\n\nSpeed\nStrict syntax, almost pedantic\n\nStrongly typed\n\n\n\n\n\nFull of functional programming\nGreat packages offered by a great community (crates)\n\nTurns out this language is Rust."
  },
  {
    "objectID": "posts/starting-learning-rust/index.html#rationale",
    "href": "posts/starting-learning-rust/index.html#rationale",
    "title": "Starting to learn Rust",
    "section": "",
    "text": "I am very happy with  but I want to learn something new, another programming language. The desire comes with several needs\n\n\n\nA compiled language\n\nSpeed\nStrict syntax, almost pedantic\n\nStrongly typed\n\n\n\n\n\nFull of functional programming\nGreat packages offered by a great community (crates)\n\nTurns out this language is Rust."
  },
  {
    "objectID": "posts/starting-learning-rust/index.html#advantages",
    "href": "posts/starting-learning-rust/index.html#advantages",
    "title": "Starting to learn Rust",
    "section": "Advantages",
    "text": "Advantages\n\nCompiler\nIt is incredibly helpful. I am using VScode with the extension rust-analyser that provides immediate feedback, great suggestions to both warnings and errors.\n\n\nBonus: the  logo\nIt has the two things I like a lot:\n\nA  from  (a chainring)\nA big R that looks very much like \n\n\n\n\nRust logo"
  },
  {
    "objectID": "posts/starting-learning-rust/index.html#learning-material",
    "href": "posts/starting-learning-rust/index.html#learning-material",
    "title": "Starting to learn Rust",
    "section": "Learning material",
    "text": "Learning material\n\n\n\n\n\n\n\n\n\nI am using so far three resources:\n\nThe Programming Rust book (Ed. O’Reilly). From a recommendation by Stefan Baumgartner\nThe official documentation that includes a lot of example.\nThe “How to learn Rust” course by Tim McManara (only $25). From a recommendation of Jonathan Caroll"
  },
  {
    "objectID": "posts/starting-learning-rust/index.html#example-of-syntax",
    "href": "posts/starting-learning-rust/index.html#example-of-syntax",
    "title": "Starting to learn Rust",
    "section": "Example of syntax",
    "text": "Example of syntax\nPrograms are managed the cargo utility. It creates, run, test, optimize Rust code. Here is one of official documentation example. We need the rand crate dependency. Here we specify the wanted version 0.8.5.\nThe code for the main guessing is included.\n\nCargo.tomlmain.rs\n\n\n[package]\nname = \"guessing_game\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\nrand = \"0.8.5\"\n\n\nuse std::io;\nuse std::cmp::Ordering;\nuse rand::Rng;\n\nfn main() {\n    println!(\"Guess the number!\");\n    \n    let max = 50u32;\n    // to keep track of the number of guesses\n    let mut count = 0u32;\n\n    // create an integer randomly from 1 to 50\n    let secret_number = rand::thread_rng().gen_range(1..=max);\n    // loop over until the number is found\n    loop { \n        count += 1;   \n        println!(\"Please input your guess. (an integer from 1 to {max})\");\n        \n        let mut guess: String = String::new();\n        \n        io::stdin()\n        .read_line(&mut guess)\n        .expect(\"Failed to read line\");\n        // coerce to an integer\n        let guess: u32 = match guess.trim().parse() {\n            Ok(num) =&gt; num,\n            // but offer possibily to change\n            Err(_) =&gt; {\n                println!(\"You entered {}, not a number\", guess.trim());\n                continue;\n            }\n        };\n        \n        println!(\"You guessed: {guess}\");  //while the secret was {secret_number}\");\n        \n        match guess.cmp(&secret_number) {\n            Ordering::Less =&gt; println!(\"Too small!\"),\n            Ordering::Greater =&gt; println!(\"Too big!\"),\n            Ordering::Equal =&gt; {\n                println!(\"You win! in {count} trials\");\n                break;\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/starting-learning-rust/index.html#first-project-umi-trimming",
    "href": "posts/starting-learning-rust/index.html#first-project-umi-trimming",
    "title": "Starting to learn Rust",
    "section": "First project: UMI trimming",
    "text": "First project: UMI trimming\nUMI stands for Unique Molecular Identifier.\nThe goal is reproduce some features from umi-tools especially the extract command.\nFor example, convert the following read\n@VH00666:90:AAAWVCCHV:1:1101:24026:1000\nGTCAGTTATAGCGGGCGCGCAAAAAAAAAAAAAAAAAAAGATCGGAAGAGCACACGTCTGAACTCCAGTCACTCCC\n[...]\ninto:\n@VH00666:90:AAAWVCCHV:1:1101:24026:1000_GTCAGT\nGCGGGCGCGCAAAAAAAAAAAAAAAAAAAGATCGGAAGAGCACACGTCTGAACTCCAGTCACTCCC\nThe UMI was GTCAGT and appended to the read name, while being removed from the sequence along with the TATA linker.\n\nRust-bio\nThis library rust-bio provides many features, and I am only using the\n\nalphabets to check the letters are actually IUPAC / DNA\nfastq for Reading/Writing FASTQ reads.\n\n\n\nThe Command Line Interface (CLI) utility: clap\nclap is an awesome library that helps making a CLI fun and easy.\nThe help output looks like (options actually appear in bold:\n$ umi_trim -h\nMove Unique Molecular Identifier from seq to name and trim a linker motif (TATA)\n\nUsage: umi_trim [OPTIONS] --input &lt;INPUT&gt; --output &lt;OUTPUT&gt;\n\nOptions:\n  -i, --input &lt;INPUT&gt;            FASTQ filename to read from\n  -o, --output &lt;OUTPUT&gt;          Filename to write to\n  -u, --umi-length &lt;UMI_LENGTH&gt;  UMI length in characters [default: 6]\n  -l, --linker &lt;LINKER&gt;          Linker UMI-READ to be discarded [default: TATA]\n  -h, --help                     Print help\n  -V, --version                  Print version\nThis project is here on"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ginolhac’s blog",
    "section": "",
    "text": "Denmark, ginolhac\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nDeploying  and  packages\n\n\n\n\n\n\n\npython\n\n\nR\n\n\nCI/CD\n\n\n\n\nwith  Gitlab Continuous Integration\n\n\n\n\n\n\nSep 14, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Archlinux\n\n\n\n\n\n\n\nlinux\n\n\n\n\nMoving away from Ubuntu\n\n\n\n\n\n\nSep 11, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Snakemake template\n\n\n\n\n\n\n\npython\n\n\nworkflow managers\n\n\nCI/CD\n\n\n\n\nusing Gitlab Continuous Integration\n\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting to learn Rust\n\n\n\n\n\n\n\nRust\n\n\n\n\nTrimming UMI\n\n\n\n\n\n\nAug 17, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nSame dot sizes\n\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical faceting\n\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrop Circles\n\n\n\n\n\n\n\nR\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n{targets} demos\n\n\n\n\n\n\n\nR\n\n\nworkflow managers\n\n\npackages\n\n\n\n\nA marvelous workflow manger for \n\n\n\n\n\n\nFeb 5, 2023\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaTex modern CV\n\n\n\n\n\n\n\nacademia\n\n\n\n\nHow to setup latex using tinytex\n\n\n\n\n\n\nMar 26, 2018\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nhome surveillance monitored via telegram\n\n\n\n\n\n\n\nPython\n\n\nDIY\n\n\nRasPI\n\n\n\n\nusing telepot and the NoIR camera\n\n\n\n\n\n\nJan 27, 2018\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\ntweening a Poisson distribution\n\n\n\n\n\n\n\nR\n\n\nanimation\n\n\n\n\nusing tweenr and ggplot2\n\n\n\n\n\n\nDec 8, 2016\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nteething\n\n\n\n\n\n\n\nR\n\n\nanimation\n\n\n\n\nCompare teething for two kids\n\n\n\n\n\n\nJul 31, 2016\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nwinter is coming\n\n\n\n\n\n\n\nbike\n\n\n\n\nsnow on bike\n\n\n\n\n\n\nJan 25, 2015\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/crop-circle/index.html",
    "href": "posts/crop-circle/index.html",
    "title": "Crop Circles",
    "section": "",
    "text": "From the repo on GitHub.\n\nlibrary(cropcircles)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ggimage)\n\nLoading required package: ggplot2\n\nlibrary(ragg)\n\n# breaking bad images\nx &lt;- c(1, 3, 9, 8)\nimages &lt;- glue::glue(\"https://openpsychometrics.org/tests/characters/test-resources/pics/BB/{x}.jpg\")\n\n# border colours\nborder_cols &lt;- colorRampPalette(c(\"black\", \"brown4\"))(4)\n  \ndf &lt;- tibble(y = 1:4, images = images) |&gt; \n  mutate(images_circle = circle_crop(images, border_size = 16, border_colour = border_cols))\n\ndf |&gt; \n  ggplot() +\n  geom_image(aes(1.5, y, image = images), size = 0.15) +\n  geom_image(aes(3.5, y, image = images_circle), size = 0.15) +\n  xlim(0, 5) +\n  ylim(0, 5) +\n  coord_fixed()"
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html",
    "title": "home surveillance monitored via telegram",
    "section": "",
    "text": "The raspberry pi has always been appealing to me, but I needed a project to really get involved. After discussing with Eric Koncina who made several great applications with Pis, I decided to go for a home surveillance system.\nThe main objective was to see how often the neighbor cats are coming to our garden, because they are scaring our cat. It’s not a big deal, rather a justification for the pi project."
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#rationale",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#rationale",
    "title": "home surveillance monitored via telegram",
    "section": "",
    "text": "The raspberry pi has always been appealing to me, but I needed a project to really get involved. After discussing with Eric Koncina who made several great applications with Pis, I decided to go for a home surveillance system.\nThe main objective was to see how often the neighbor cats are coming to our garden, because they are scaring our cat. It’s not a big deal, rather a justification for the pi project."
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#materials",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#materials",
    "title": "home surveillance monitored via telegram",
    "section": "Materials",
    "text": "Materials\nI bought a Pi3 starter budget kit that contains:\n\nPi3\npower, 5V, 2.5A\ncase\nSD card 16 Go\n\nAdditionally, I purchased:\n\nPi camera NoIR\n\nWas hoping to get some decent pictures / videos with low light. Turned out that IR leds are needed. That goes in the TODO section.\nHere is an example of picture with low interior light. Colors are off, but quality is fine to me"
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#setting-up-the-pi",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#setting-up-the-pi",
    "title": "home surveillance monitored via telegram",
    "section": "Setting-up the pi",
    "text": "Setting-up the pi\nI won’t go into details, I mostly followed the instructions in this tutorial. Briefly, here are the main steps\n\ndownload raspbian lite\nSince I have no screen, no keyboard and the pi comes with a WiFi controller, the stretch lite is sufficient. Image can be found at raspberrypi.org\n\n\nformat SD card\nusing disk utility, choose MS-DOS FAT file system\n\n\ninstall raspbian\nEnsure your SD card is the second disk (/dev/disk2), otherwise do adapt to the correct one!\nunzip 2017-11-29-raspbian-stretch-lite.zip\nsudo dd bs=1m if=2017-11-29-raspbian-stretch-lite.img of=/dev/rdisk2\n\n\nenable ssh\nOnce copied, you can enable ssh by creating an empty file at the SD card root\ncd /Volumes/boot/\ntouch ssh\n\n\nenable wifi\nIn order to connect to the pi without screen / keyboard, wifi needs to be configured right away. At the same location (/Volumes/boot) add a file named wpa_supplicant.conf\nwhich contains:\nnetwork={\n        ssid=\"your_network_ssid\"\n        psk=\"xxx\"\n        key_mgmt=WPA-PSK\n}\nOf note, I recently acquired a pi zeroWH, for which I had to add 3 lines (StackExchange question).\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\ncountry=FR\nnetwork={\n        ssid=\"your_network_ssid\"\n        psk=\"xxx\"\n        key_mgmt=WPA-PSK\n}\n\n\nconnect to pi\nonce the raspberrypi booted, try to find its IP\nnmap -sn 192.168.1.0/24\nwhich gives:\nStarting Nmap 7.60 ( https://nmap.org ) at 2017-12-08 22:44 CET\nNmap scan report for 192.168.1.27\nHost is up (0.0071s latency).\nNmap scan report for 192.168.1.254\nHost is up (0.0048s latency).\nNmap done: 256 IP addresses (2 hosts up) scanned in 11.71 seconds\n192.168.1.254 was the rooter, so pi was assigned 192.168.1.27\nssh pi@192.168.1.27 works.\nyou can also assign a fixed IP to your pi\n\n\nfinal configuration\nonce connected to the pi:\n\nrun sudo raspi-config to activate the camera\nchange password for the pi user\nset up locales and timezone\nupdate && upgrade raspbian stretch\nadd public ssh key to .ssh/authorized_keys for password less connection"
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#install-the-surveillance-system",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#install-the-surveillance-system",
    "title": "home surveillance monitored via telegram",
    "section": "install the surveillance system",
    "text": "install the surveillance system\nEven if, I’d like to have openCV like in this tutorial, it was way more work. Hence, the choice of motion\n\nmotion software\nI followed the instructions provided in this great tutorial by Bouvet. With some changes described below.\n\ncompilation\nsee from here https://motion-project.github.io/motion_build.html\nwget https://github.com/Motion-Project/motion/releases/download/release-4.1.1/pi_stretch_motion_4.1.1-1_armhf.deb\nsudo apt-get install gdebi-core\nsudo gdebi pi_stretch_motion_4.1.1-1_armhf.deb\n\n\n\nrun motion\nfirst, as Bouvet suggested, I copied the main config file\nmkdir ~/motion && cp /etc/motion/motion.conf ~/motion/\nand alter the new copy.\nrunning motion:\nmotion -c ~/motion/motion.conf\n\n\ntweaks to the initial tutorial\nI choose to get the videos and several other changes made to the motion version 4.1.1. and are reported in the complete diff page below\ndiff page for motion.conf\nThe initial values are reported in red, in green: the new ones.\n\n\n\nsee live streaming\nwith this configuration, you should see the live streaming from this URL: http://192.168.1.27:8081\nthe web control on port 8080 is disabled apart outside the pi, since we’ll use telegram to control motion\n\n\ndetection\nHere is an example of my kid being detected. The red rectangle works nicely\n\n\n\n\n\n\n\nBut of course, there are false alarms, such as when the light comes in/out suddenly\n\n\n\n\n\n\n\nthe parameter lightswitch 80 reduced the issue but it still exists."
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#communication-with-motion-via-telegram",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#communication-with-motion-via-telegram",
    "title": "home surveillance monitored via telegram",
    "section": "Communication with motion via telegram",
    "text": "Communication with motion via telegram\nNow comes the fun part. Receiving the motion detection by emails is fine, but it can be done via Telegram and the awesome API telepot. Eric told me about telegram bots and it looked promising. Actually, you can even send commands to your pi using your phone using those telegram bots.\nThe useful feature I implemented are:\n\nalerts. A motion is detected. Send the best picture to your telegram account.\npause / resume motion detection. Imagine you are away and for some reason (shadows, your own cat) you keep receiving alerts, you may want to remotely pause the detection. And of course, being able to resume it. Those commands are already in motion, we just need to talk to it.\nstatus. You haven’t received alerts, is the system running smoothly? You can ask for a confirmation that detection is on. Also, check if the camera is on.\nsnapshot. No alerts, but you’d like to get a snapshot at any time.\nvideo. Maybe the nicest feature IMHO. Sending picture to your phone for every detection is fine, but not all videos. Based on the picture you see, you’d like to get the video of the detection. Once again, by a command to a telegram bot, you receive the last video recorded.\n\n\ncreate mybot\nEric gave me the link to this tutorial\nOf course, I am assuming you already have our own telegram account.\nTalk to the BotFather and create mybot, you will receive a private token.\n\n\ninstall telepot\nback on the pi, install telepot with pip, assuming you installed python and pip.\npip install telepot\n\n\ntest sending message\n\nget the bot id\nimport telepot\nbot = telepot.Bot('your-token')\nbot.getMe()\nreturns {u'username': u'mybot', u'first_name': u'cat tracker', u'is_bot': True, u'id': 00000008}\n\n\nget your telegram id:\n\nsend a messages from telegram to mybot\nfetch your message on the pi\n\nfrom pprint import pprint\nresponse = bot.getUpdates()\npprint(response)\nyour id appears, such as: u’id’: 00000004\n\n\nbasic tests\n\nfor text\n\nbot.sendMessage(00000004, 'Hey!')\n\nfor picture\n\nbot.sendPhoto(00000004, photo=open('/home/pi/motion/detected/07-2018-01-06_205746-13.jpg', 'rb'), caption='motion detected')\n\n\ncreate commands for the bot\nAfter sending /setcommands to the BotFather:\ntime - Returns current time on pi\ncheck - Returns status of the camera\nstatus - Returns status of motion\npause - Pauses the motion detection\nresume - Resumes motion detection\nsnapshot - Returns current image\nvideo - Returns last recorded video\nOf note, it doesn’t prevent the bot to receive other commands, it just helps to display commands and select them in telegram.\n\n\npython script that listen\nhere the script listen_bot.py, derived from the telepot documentation.\nSome comments:\n\nthe last video when requested is fetched from the sub-folder vids. If we use the main folder of detection, the last video could be an incomplete one form a newer detection. Hence, the command in motion.conf to move a finished video to the vids folder.\nI failed to restrict the bot to communicate only with me. Might not be a big deal, but the code if chat_id != 00000008 is not working.\nthe webcontrol was set in the RAW mode. Then the retrieved text can be directly send to your telegram account\n\n#!/usr/bin/python2.7\n\nimport datetime\nimport telepot\nimport time\nimport requests\nimport os\nimport glob\nfrom telepot.loop import MessageLoop\n\ndef webcontrol(chat_id, type, cmd):\n    req = 'http://localhost:8080/0/'+type+'/'+cmd\n    res = requests.get(req)\n    bot.sendMessage(chat_id, res.text)\n\ndef handle(msg):\n    chat_id = msg['chat']['id']\n    command = msg['text']\n    #should work thanks to Winston\n    if msg['from']['id'] != 00000008:\n        bot.sendMessage(chat_id, \"Sorry this is a personal bot. Access Denied!\")\n        exit(1)\n\n    print 'Got command: %s' % command\n\n    if command == '/snapshot':\n        requests.get('http://localhost:8080/0/action/snapshot')\n    elif command == '/status':\n        webcontrol(chat_id, 'detection', 'status')\n    elif command == '/pause':\n        webcontrol(chat_id, 'detection', 'pause')\n    elif command == '/resume':\n        webcontrol(chat_id, 'detection', 'start')\n    elif command == '/check':\n        webcontrol(chat_id, 'detection', 'connection')\n    elif command == '/time':\n        bot.sendMessage(chat_id, 'now is '+str(datetime.datetime.now()))\n    elif command == '/video':\n        # the most recent video in this particular folder of complete vids\n        video = max(glob.iglob('/home/pi/motion/detected/vids/*.mp4'), key=os.path.getctime)\n        # send video, adapt the the first argument to your own telegram id\n        bot.sendVideo(00000008, video=open(video, 'rb'), caption='last video')\n    else:\n        bot.sendMessage(chat_id, \"sorry, I don't know the command \"+command)\n# adapt the following to the bot_id:bot_token\nbot = telepot.Bot('0000000004:bot_token')\n\nMessageLoop(bot, handle).run_as_thread()\nprint 'I am listening ...'\n\nwhile 1:\n    time.sleep(10)\nNow if the both listen_bot.py and motion -c ~/motion/motion.conf are running, the system should work.\n\n\nsending scripts\nthey are called in the motion.conf file.\nfirst, send_detection.py\n#!/usr/bin/python2.7\n\nimport telepot\nimport sys\n\nbot = telepot.Bot('0000000004:bot_token')\n\npic = sys.argv[1]\n\n# change caption if it is a snapshot or motion\nif pic.endswith(\"snapshot.jpg\"):\n    cap = 'snapshot'\nelse:\n    cap = 'motion detected'\n\nbot.sendPhoto(00000004, photo=open(pic, 'rb'), caption=cap)\n\nexit(0)\nsecond, send_message.py\n#!/usr/bin/python2.7\n\nimport telepot\nimport sys\n\nbot = telepot.Bot('0000000004:bot_token')\n\ntext = sys.argv[1]\n\nbot.sendMessage(00000004, text)\n\n\nrun scripts at startup\nEdit 2018-06-19\n\nfor listening\nI am now using systemd. Cleaner and safer.\nHere, the following is working, but I am sure this is the right way to do, so use we care.\nadd the file listen.service in the folder /etc/systemd/system:\n[Unit]\nDescription=Listen to foo telegram bot\nAfter=network.target\n\n[Service]\nExecStart=/usr/bin/python2.7 -u listen_bot.py\nWorkingDirectory=/home/pi/motion/\nStandardOutput=inherit\nStandardError=inherit\nRestart=always\nUser=motion\n\n[Install]\nWantedBy=multi-user.target\nnow as example, you can check the status of this new service listen\npi@raspberrypi:/etc/systemd/system $ service listen status\n● listen.service - Listen to foo telegram bot\n   Loaded: loaded (/etc/systemd/system/listen.service; enabled; vendor preset: enabled)\n   Active: active (running) since Sun 2018-05-13 21:51:50 CEST; 1 months 6 days ago\n Main PID: 502 (python2.7)\n   CGroup: /system.slice/listen.service\n           └─502 /usr/bin/python2.7 -u listen_bot.py\n\nMay 13 21:51:50 raspberrypi systemd[1]: Started Listen to foo telegram bot.\nMay 13 21:51:51 raspberrypi python2.7[502]: I am listening ...\nJun 19 20:52:17 raspberrypi python2.7[502]: Got command: /time\nAnd, safer since both motion and listen are running as the user motion who does not have sudo rights.\n\n\nFor motion\nI am now using the daemon mode to only benefit from systemd\n\npoint the usual conf file towards the tweaked version in your home\n\ncd /etc/motion\nmv motion.conf motion.conf.bak\nln -s /home/pi/motion/motion.conf .\nbe sure to have the correct smblink\nmotion.conf -&gt; /home/pi/motion/motion.conf\n\n\n\nScreenshots\nHere are some examples of the telegram window\n\nreceived a notification and later on, /check if connection is still on.\n\n\n\n\n\n\n\n\n\nreceived notification and ask for the corresponding video\n\n\n\n\n\n\n\n\nthis video works as a GIF directly in the window:\n\n\n\ngif file\n\n\n\n\npause the detection, and since no motion can be detected, ask for a snapshot\n\n\n\n\n\n\n\n\n\n\n\nTODO\nDespite a functional system, some improvements I’d like to achieve:\n\nrestrict the bot to one user: see Winston Smith recommendation in comments, seems to work nicely\nadd gracefull stop for listen service\nI bought a IP camera, and motion should work with both. Haven’t spent enough time configuring it\nremove pics/videos older than xx days to save space\nrun the 2 services as a user without sudo rights\nlook into better settings for NoIR camera using this thread"
  },
  {
    "objectID": "posts/diy-raspberry-monitored-via-telegram/index.html#conclusion",
    "href": "posts/diy-raspberry-monitored-via-telegram/index.html#conclusion",
    "title": "home surveillance monitored via telegram",
    "section": "Conclusion",
    "text": "Conclusion\nBeyond the initial goal, catch the neighbor cats coming in, which actually I don’t care about, it was fun to set-up the whole thing. Moreover, telegram offers a great service and offers a great interface for many applications. I knew about the TeleR bot, that is actually easy to set-up.\nDon’t hesitate to leave a comment below for any remarks or improvements that I overseen."
  },
  {
    "objectID": "posts/renv-python-ci/index.html",
    "href": "posts/renv-python-ci/index.html",
    "title": "Deploying  and  packages",
    "section": "",
    "text": "Synchronize both  and  packages between users and  runners that actually render a teaching website using Quarto."
  },
  {
    "objectID": "posts/renv-python-ci/index.html#aim",
    "href": "posts/renv-python-ci/index.html#aim",
    "title": "Deploying  and  packages",
    "section": "",
    "text": "Synchronize both  and  packages between users and  runners that actually render a teaching website using Quarto."
  },
  {
    "objectID": "posts/renv-python-ci/index.html#introduction",
    "href": "posts/renv-python-ci/index.html#introduction",
    "title": "Deploying  and  packages",
    "section": "Introduction",
    "text": "Introduction\n{renv} is a package for managing  packages at the project scale.\n\n\n\nrenv overview (Kevin Ushey)\n\n\n\n Python packages\nThis article is succinct but describes that {renv} can manage Python packages."
  },
  {
    "objectID": "posts/renv-python-ci/index.html#setup",
    "href": "posts/renv-python-ci/index.html#setup",
    "title": "Deploying  and  packages",
    "section": "Setup",
    "text": "Setup\n\nInitialization\nFollowing instructions from the renv website. Using the virtual environment solution and not conda as the first one is included in python3.\nYou need to have installed:\n\nPython: python3.11-venv\nR:\n\n{reticulate}\n{renv}\n\n\n&gt; renv::use_python()\nPlease select a version of Python to use with this project: \n\n1: /usr/bin/python3\n\nSelection: 1\n- Selected \"/usr/bin/python3\" [Python 3.11.12].\n- Creating virtual environment 'renv-python-3.11' ... Done!\n- Updating Python packages ... Done!\n- Lockfile written to \"renv.lock\".\n- Activated Python 3.11.12 [virtualenv; ./renv/python/virtualenvs/renv-python-3.11]\n\n\nInstalling Python packages\nFor example, installing pandas\n&gt; reticulate::py_install(\"pandas\")\nUsing virtual environment '/xxxx/xxxx/renv/python/virtualenvs/renv-python-3.11' ...\n+ /xxxx/xxxx/renv/python/virtualenvs/renv-python-3.10/bin/python -m pip install --upgrade --no-user pandas\nCollecting pandas\n  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/fb/4f/4a4372b2e24439f559b73318683486831d75e59544ae02bf8dec8dd6f48b/pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting numpy&gt;=1.22.4 (from pandas)\n  Obtaining dependency information for numpy&gt;=1.22.4 from https://files.pythonhosted.org/packages/9b/5a/f265a1ba3641d16b5480a217a6aed08cceef09cd173b568cd5351053472a/numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.5/58.5 kB 4.0 MB/s eta 0:00:00\nCollecting python-dateutil&gt;=2.8.2 (from pandas)\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 10.5 MB/s eta 0:00:00\nCollecting pytz&gt;=2020.1 (from pandas)\n  Obtaining dependency information for pytz&gt;=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata&gt;=2022.1 (from pandas)\n  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 10.6 MB/s eta 0:00:00\nCollecting six&gt;=1.5 (from python-dateutil&gt;=2.8.2-&gt;pandas)\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 11.0 MB/s eta 0:00:00\nDownloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 10.9 MB/s eta 0:00:00\nDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.5/502.5 kB 10.5 MB/s eta 0:00:00\nInstalling collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\nSuccessfully installed numpy-1.26.0 pandas-2.1.0 python-dateutil-2.8.2 pytz-2023.3.post1 six-1.16.0 tzdata-2023.3\n\n\nSnapshot\nNow, register with {renv} what we have just installed.\nThe main function renv::snapshot() will regiter both  and  packages.\n&gt; renv::snapshot()\nThe following package(s) will be updated in the lockfile:\n\n# CRAN -----------------------------------------------------------------------\n- renv   [* -&gt; 1.0.2]\n\nDo you want to proceed? [Y/n]: \n- Lockfile written to \"/xxxx/xxxx/renv.lock\".\nThe following will be written to requirements.txt:\n- numpy==1.26.0\n- pandas==2.1.0\n- python-dateutil==2.8.2\n- pytz==2023.3.post1\n- six==1.16.0\n- tzdata==2023.3\n\nDo you want to proceed? [Y/n]: \n- Wrote Python packages to \"/xxxx/xxxx/requirements.txt\".\nChecking on the written files, requirements.txt is a rather simple text file:\n\n\nrequirements.txt\n\nnumpy==1.26.0\npandas==2.1.0\npython-dateutil==2.8.2\npytz==2023.3.post1\nsix==1.16.0\ntzdata==2023.3\n\nrenv.lock, the  relevant part:\n\n\nrenv.lock\n\n[...]\n  \"Python\": {\n    \"Version\": \"3.11.12\",\n    \"Type\": \"virtualenv\",\n    \"Name\": \"./renv/python/virtualenvs/renv-python-3.11\"\n  },\n[...]\n\n\n\nQuick test\n&gt; pd &lt;- reticulate::import(\"pandas\")\n&gt; pd$Series(list(1, 3, 6, 8))\n0 1 2 3 \n1 3 6 8\n\n\n.Rprofile\nIn theory, the sourced .Rprofile should only contains one necessary line generated by renv:\nsource(\"renv/activate.R\")\nHowever, if you encountered issue with  not finding the  environment, it could be useful to have those env variables defined:\nSys.setenv(RETICULATE_MINICONDA_PATH = \"renv/python/virtualenvs/renv-python-3.11/\")\nreticulate::use_python(\"renv/python/virtualenvs/renv-python-3.11/bin/python\")"
  },
  {
    "objectID": "posts/renv-python-ci/index.html#ci-config",
    "href": "posts/renv-python-ci/index.html#ci-config",
    "title": "Deploying  and  packages",
    "section": "CI config",
    "text": "CI config\nNow the Continuous Integration (CI) triggered by every commit needs to:\n\nInstall the missing packages, especially the  ones\nCache them to avoid re-installing at each commit\nRun the pipeline of building the R code (using {targets} (see previous post)\nRender the website using quarto\n\n\nCaching\nThe config is:\n  cache:\n    key: $CI_JOB_NAME\n    paths:\n      - _targets/\n      - _site/\n      - lectures_rendered/\n      - renv/python/virtualenvs/\n      - ${RENV_PATHS_CACHE}\n      - ${RENV_PATHS_LIBRARY}\n\n_targets/ folder that contains the metadata and objects done by {targets}, necessary otherwise everything is redone each time.\n_site output folder of the website, default name by quarto.\nrenv/python/virtualenvs/ cache the python install + packages\n\nRest is classic and is described here\n\n\nBuilding\n  before_script:\n    - Rscript -e \"renv::restore()\"\n  script:\n    - ./run.R\n    - source renv/python/virtualenvs/renv-python-3.11/bin/activate\n    - quarto render\n  tags:\n    - shared-cache\n  when: always\n\n./run.R runs targets::tar_make()\nThe key part is this source [venv] line 5. It allows the next line with the quarto call to be aware of where is the  install / packages 1.\nquarto render then build the website\n\n\n\nResults\nIn this screenshot, you see the first time the CI sees the need to install packages for both languages\n\n\n\nInstalling (and caching) packages in CI\n\n\nAnd now, after a successful caching, no install is required:\n\n\n\nBuilding website in CI\n\n\n\nline 40-59 is the {targets} pipeline\nline 61-63 the beginning of the quarto rendering process that involved  Python code."
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html",
    "href": "posts/snakemake-test-ci/index.html",
    "title": "Testing Snakemake template",
    "section": "",
    "text": "Testing software is necessary and I always do it too late. Looking William Landau coding (through watching his {targets} repo) it is clear that the best way to develop is to write as the same time:\n\ncode\nunit tests\ndocumentation\n\nWhen struggling on the first item, it appears difficult to concomitantly write the corresponding tests. Not to mention that documentation could be not even considered in the first place.\nThis works somehow but suddenly, one repo could gain attention and documentation becomes an extended README. Then later, a fresh release is proven to break things, and that’s bad when this is reported by users and not detected by the author.\nLong story short, I wanted my snakemake template for bulk RNA-seq to be tested, at least a short pipeline on fake data to catch obvious mistakes.\nBy the way, Snakemake is a Python framework for managing bioinformatic workflow. That would deserve a post on itself."
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html#rationale",
    "href": "posts/snakemake-test-ci/index.html#rationale",
    "title": "Testing Snakemake template",
    "section": "",
    "text": "Testing software is necessary and I always do it too late. Looking William Landau coding (through watching his {targets} repo) it is clear that the best way to develop is to write as the same time:\n\ncode\nunit tests\ndocumentation\n\nWhen struggling on the first item, it appears difficult to concomitantly write the corresponding tests. Not to mention that documentation could be not even considered in the first place.\nThis works somehow but suddenly, one repo could gain attention and documentation becomes an extended README. Then later, a fresh release is proven to break things, and that’s bad when this is reported by users and not detected by the author.\nLong story short, I wanted my snakemake template for bulk RNA-seq to be tested, at least a short pipeline on fake data to catch obvious mistakes.\nBy the way, Snakemake is a Python framework for managing bioinformatic workflow. That would deserve a post on itself."
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html#inspiration-from-the-experienced-people",
    "href": "posts/snakemake-test-ci/index.html#inspiration-from-the-experienced-people",
    "title": "Testing Snakemake template",
    "section": "Inspiration from the experienced people",
    "text": "Inspiration from the experienced people\nThe template is derived from this one and they implemented testing. Apart from the great looking green badge, it is what I am after: run the pipeline on small data of the Yeast genome.\n\n\n\n\n\nTest passed!\n\n\nLooking at how it is done in their  GitHub Action file, the relevant part is:\n\n    - name: Test workflow (basic model, no batch_effects)\n      uses: snakemake/snakemake-github-action@v1.22.0\n      with:\n        directory: .test\n        snakefile: workflow/Snakefile\n        args: \"--configfile .test/config_basic/config.yaml --use-conda --show-failed-logs --cores 2 --conda-cleanup-pkgs cache\"\n\nThey are doing more testing but wit the same structure. The annoying part to me is the:\nuses: snakemake/snakemake-github-action@v1.22.0\nBecause it abstracts the real pipeline, it works but it has some magic inside and even going through the repo is not giving details.\nThe same happens for  with {renv} cache / restore, quarto render and publishing on GitHub pages like for this very blog.\nBut, some great things are useful, like the option --show-failed-logs that I didn’t know and that is especially relevant here.\nAnyway, the Snakemake template is on  GitLab , so the CI/CD has to happen there."
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html#cicd-configuration",
    "href": "posts/snakemake-test-ci/index.html#cicd-configuration",
    "title": "Testing Snakemake template",
    "section": " CI/CD configuration",
    "text": "CI/CD configuration\nAs said, on GiLab, the Continuous Integration/Development has no magic recipes like on . One needs to declare every step.\nThe content of the config .gitlab-ci.yml is:\nservices:\n    - name: docker:dind\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"\"\n\nstages:\n  - test\n\ntesting:\n  stage: test\n  image: snakemake/snakemake:stable\n  cache:\n    key: ${CI_JOB_NAME}\n    paths:\n      - .test/.snakemake/\n  script:\n    - cd .test\n    - snakemake --configfile config_basic/config.yaml \n      --snakefile Snakefile_test -j1 --use-singularity --show-failed-logs\n    - snakemake --configfile config_basic/config.yaml \n      --snakefile Snakefile_test -j1 --report\n    - NB_DEG=`wc -l results/diffexp/KO_vs_WT.diffexp.tsv | cut -f1 -d ' '`\n    - test \"$NB_DEG\" -eq 94 || exit 1\n    - diff results/diffexp/KO_vs_WT.diffexp.tsv expected/KO_vs_WT.diffexp.tsv\n    - diff  &lt;(grep -v seed trimmed_se/A1-1.settings)  &lt;(grep -v seed expected/A1-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/B1-1.settings)  &lt;(grep -v seed expected/B1-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/A2-1.settings)  &lt;(grep -v seed expected/A2-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/B2-1.settings)  &lt;(grep -v seed expected/B2-1.settings)\n\n  tags:\n    - shared-cache\n  when: always\nI abstracted the specificity of our  Gitlab, suppressing this line:\n      command: [\"--registry-mirror\", \"https://docker-registry.lcsb.uni.lu\"]\nLet’s break it down\n\nDocker in Docker (dind) service\nLines 1 to 6. Nothing special, copied from our template\n\n\nStages\nLines 8-9. Here only one, but could have a second one for testing the complex design like on \n\n\nMain part\n\nimage copied over from the Github Action, useful that have a  image done.\ncache (lines 14-17). I am not using conda but singularity, the  Docker for HPC. Thus it is useful to cache the image to avoid downloading it every single time. Snakemake caches it in .snakemake/singularity/hashsum.sif checking if a new one if available. Lines 32-33 indicates that the cache is shared, so any  runner is able to access the cached data.\nscript comes in 2 steps:\n\nRunning snakemake: lines 20-23\nTesting the results obtained: lines 24-30 Testing the number of lines of the differential expression TSV, redundant with the exact expected file with a diff. And testing also the expected trimming stats. Random seed are different so need to be excluded.\n\nwhen indicates for which action the CI is triggered. Could be only on pull request, release, manual etc. Here we triggered it for any commit push to the repo."
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html#demo",
    "href": "posts/snakemake-test-ci/index.html#demo",
    "title": "Testing Snakemake template",
    "section": "Demo",
    "text": "Demo\nA testing run lasts for roughly 4 minutes which is reasonable.\n\n\n\ndemo running test\n\n\n\n\n\ndemo testing results\n\n\nWithout the cache, the download of the singularity brings the time to 6 minutes 26 seconds. See the additional pulling notification (line 4):\n[...]\n$ snakemake --configfile config_basic/config.yaml --snakefile Snakefile_test -j1 --use-singularity --show-failed-logs\nBuilding DAG of jobs...\nPulling singularity image docker://ginolhac/snake-rna-seq:0.7.\n[...]"
  },
  {
    "objectID": "posts/snakemake-test-ci/index.html#gitlab-repository",
    "href": "posts/snakemake-test-ci/index.html#gitlab-repository",
    "title": "Testing Snakemake template",
    "section": " Gitlab repository",
    "text": "Gitlab repository\nThis repo is public and available here"
  },
  {
    "objectID": "posts/winter-is-coming/index.html",
    "href": "posts/winter-is-coming/index.html",
    "title": "winter is coming",
    "section": "",
    "text": "Snow during winter time. How unexpected this could be?\n\n\n\nVon backhaus in white"
  },
  {
    "objectID": "posts/install-archlinux/index.html",
    "href": "posts/install-archlinux/index.html",
    "title": "Install Archlinux",
    "section": "",
    "text": "I started using GNU/Linux in 2000, discovering it in a master, it was RedHat on those shared machines. At this point, I got hooked and installed a now dead distribution called Mandrake (then rebranded Mandriva)\n\n\n\n\n\nMandriva logo\n\n\n\n\n\nLater in 2003, Christophe Badoit installed for the biotech company I was working in: debian. I knew he was then using Archlinux but it took many years to make this switch.\n\n\n\n\n\nDebian logo\n\n\nDebian was absolutely great, and I was an happy user until 2010. Some people in the company started to use Ubuntu but I stick to the original. My main issue was the latency between the publications of new versions their arrival as debian packages, even in unstable.\nIn 2010, I quit the company and returned to academia. On a brand-new laptop, I installed Ubuntu. It became my main distro until today. It was a good journey, much easier to install than debian, oki-sh release of new versions and the apt system that was robust and nice to use.\n\n\n\n\n\nUbntu logo\n\n\n\n\nTime passing by, I got fed up with several aspects of Ubuntu:\n\nBloat software, who needs those pre-installed stuff?\nGNOME overlay, this Dock and forked gnome experience started to feel weird\nWayland integration was still not fine\nThe snap world, no thanks, I was here for apt\nDelay to get new version.\n\nTo expand on point 2-4, during lock-downs, sharing screen through the madness of webex, teams etc.. in Wayland was not great. If you had the snap firefox, it could not see some files on a ssh mounted folder. For point 5. for example  version 4.3.0 was released in April 2023. Using Ubuntu version dev 23.10 in September 2023 I was still stuck with  4.2.2 from October 2022. I know I could set up ppa repository of some users but with snap it became all too far from the great apt system I enjoyed for long.\nTurns out I need a rolling release ditribution."
  },
  {
    "objectID": "posts/install-archlinux/index.html#history-of-distribution-usage",
    "href": "posts/install-archlinux/index.html#history-of-distribution-usage",
    "title": "Install Archlinux",
    "section": "",
    "text": "I started using GNU/Linux in 2000, discovering it in a master, it was RedHat on those shared machines. At this point, I got hooked and installed a now dead distribution called Mandrake (then rebranded Mandriva)\n\n\n\n\n\nMandriva logo\n\n\n\n\n\nLater in 2003, Christophe Badoit installed for the biotech company I was working in: debian. I knew he was then using Archlinux but it took many years to make this switch.\n\n\n\n\n\nDebian logo\n\n\nDebian was absolutely great, and I was an happy user until 2010. Some people in the company started to use Ubuntu but I stick to the original. My main issue was the latency between the publications of new versions their arrival as debian packages, even in unstable.\nIn 2010, I quit the company and returned to academia. On a brand-new laptop, I installed Ubuntu. It became my main distro until today. It was a good journey, much easier to install than debian, oki-sh release of new versions and the apt system that was robust and nice to use.\n\n\n\n\n\nUbntu logo\n\n\n\n\nTime passing by, I got fed up with several aspects of Ubuntu:\n\nBloat software, who needs those pre-installed stuff?\nGNOME overlay, this Dock and forked gnome experience started to feel weird\nWayland integration was still not fine\nThe snap world, no thanks, I was here for apt\nDelay to get new version.\n\nTo expand on point 2-4, during lock-downs, sharing screen through the madness of webex, teams etc.. in Wayland was not great. If you had the snap firefox, it could not see some files on a ssh mounted folder. For point 5. for example  version 4.3.0 was released in April 2023. Using Ubuntu version dev 23.10 in September 2023 I was still stuck with  4.2.2 from October 2022. I know I could set up ppa repository of some users but with snap it became all too far from the great apt system I enjoyed for long.\nTurns out I need a rolling release ditribution."
  },
  {
    "objectID": "posts/install-archlinux/index.html#archlinux",
    "href": "posts/install-archlinux/index.html#archlinux",
    "title": "Install Archlinux",
    "section": "Archlinux",
    "text": "Archlinux\n\nThe pkg.tar.xz world\n\n\n\nArchlinux logo\n\n\nWelcome to pacman and other great stuff. For example, disk encryption, where with Ubuntu it was not obvious how to set it up. This is now under control.\npacman is dealing with your installation and dependencies (just like apt). Software and utilities are coming from either\n\ncore\nextra\n\nBut what about RStudio? Signal? Slack?\nAll those accessory[^Of note, RStudio for me it absolutely not accessory, but they not for ] software can be found in the AUR: Arch User Repository.\nThen, for dealing with those 3 sources I was advice to use yay. After a couple of months, I enjoy typing yay in the Terminal and get the source updated + the packages. right now, I have only one AUR package to update.\n$ yay\n[sudo] password for xxxxx: \n:: Synchronizing package databases...\n core is up to date\n extra                                                   8.3 MiB  10.6 MiB/s 00:01 [################################################] 100%\n:: Searching AUR for updates...\n:: Searching databases for updates...\n -&gt; Flagged Out Of Date AUR Packages: telegram-desktop-bin\n:: 1 package to upgrade/install.\n1  aur/signal-desktop-beta-bin  6.29.0beta.1-1 -&gt; 6.31.0beta.1-1\n==&gt; Packages to exclude: (eg: \"1 2 3\", \"1-3\", \"^4\" or repo name)\n -&gt; Excluding packages may cause partial upgrades and break systems\n==&gt; \nBut next time, I will be notified of any update for open-ssh, rstudio or quarto.\nOf note, for the later, you can choose if you want to install the release version, the pre-release, binaries or compile things yourself. So far it fits completely my needs.\nOf note, I got the  version of I wanted without any tweaks:\n$ pacman -Q r\nr 4.3.1-2\n\nExample of a bigger upgrade\n:: Synchronizing package databases...\n core                                                        129.3 KiB   994 KiB/s 00:00 [###################################################] 100%\n extra                                                         8.3 MiB  26.1 MiB/s 00:00 [###################################################] 100%\n:: Searching AUR for updates...\n:: Searching databases for updates...\n:: 56 packages to upgrade/install.\n56  core/glib2                                   2.76.5-1         -&gt; 2.78.0-1\n55  core/glib2-docs                              2.76.5-1         -&gt; 2.78.0-1\n54  core/iana-etc                                20230803-1       -&gt; 20230907-1\n53  core/iproute2                                6.4.0-1          -&gt; 6.5.0-1\n52  core/linux                                   6.4.12.arch1-1   -&gt; 6.5.2.arch1-1\n51  core/openssh                                 9.4p1-3          -&gt; 9.4p1-4\n50  core/python                                  3.11.5-1         -&gt; 3.11.5-2\n49  core/shadow                                  4.13-2           -&gt; 4.13-3\n48  core/systemd                                 254.1-1          -&gt; 254.3-1\n47  core/systemd-libs                            254.1-1          -&gt; 254.3-1\n46  core/systemd-sysvcompat                      254.1-1          -&gt; 254.3-1\n45  extra/code                                   1.81.1-1         -&gt; 1.82.0-1\n44  extra/freerdp                                2:2.10.0-4       -&gt; 2:2.11.1-1\n[...]\n 8  extra/npm                                    9.8.1-1          -&gt; 10.1.0-1\n 7  extra/openpmix                               4.2.5-1          -&gt; 4.2.6-1\n 6  extra/pandoc-cli                             0.1.1-41         -&gt; 0.1.1-43\n 5  extra/vulkan-icd-loader                      1.3.255-1        -&gt; 1.3.263-1\n 4  aur/oh-my-posh-bin                           18.7.0-1         -&gt; 18.8.1-1\n 3  aur/quarto-cli-bin-pre-release               1.4.352-6        -&gt; 1.4.358-6\n 2  aur/rstudio-desktop-bin                      2023.06.1.524-1  -&gt; 2023.06.2.561-1\n 1  aur/signal-desktop-beta-bin                  6.29.0beta.1-1   -&gt; 6.31.0beta.1-1\n\n\n\nArch installation with full disk encryptiom\nI first installed Arch on my personal laptop (ThinkPad P14s) for testing (without encryption). I then proceed with my work laptop (XPS 7390), mostly following this install tutorial by mjnaderi. The encryption of the full disk works well using LVM2 and GRUB.\nThe differences were:\n\nInstall the linux-lts kernel in case something gets broken with the current one (thanks Hyacinthe!)\nNo dual boot with \n\nThe most tricky part is GRUB with efi but this tutorial steps were clear and correct.\n\n\nPost-install adjustments\n\ndocker\n\nyay docker\nsudo systemctl enable docker\nsudo usermod -aG docker login\n\nDisable PC speaker using gsettings set org.gnome.desktop.wm.preferences audible-bell false\nMissing fonts\n\nyay noto-fonts noto-fonts-emoji ttf-dejavu ttf-liberation ttf-meslo-nerd-font-powerlevel10k\n\nUseful packages\n\nyay git-lfs cmake udunits gcc-fortran rustup sshfs\n\nChromium (needed for {renderthis}): yay ungoogled-chromium-bin\nJottacloud: yay jotta-cli\nSlack: yay slack-desktop\nSignal and Telegram: yay telegram-desktop-bin signal-desktop-beta-bin\nSystem tray notifications yay extension-manager, then start App Extension Manager and install:\n\n\n\n\nAppIndicator\n\n\nThen it displays things like this:\n\n\n\nSystem tray\n\n\n\nFor the  package {V8}, needs to activate the static lib before installing:\n\nSys.setenv(DOWNLOAD_STATIC_LIBV8 = 1)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Github\n  \n  \n    \n     Mastodon\n  \n\n      \n\n\nI am working at the University of Luxembourg; in the Departement of Life Sciences and Medecine, providing services to the researchers.\n— Acknowledgment provided by acknowledge-the-climate-crisis.org\nImage of the 🌏 🌡️ 1850-2022 from ShowYourStripes"
  },
  {
    "objectID": "about.html#licence",
    "href": "about.html#licence",
    "title": "About",
    "section": "Licence",
    "text": "Licence\n  This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License except for the borrowed and mentioned with proper source: statements."
  }
]