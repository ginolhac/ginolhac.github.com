[
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html",
    "href": "posts/2023-11-27_stringr_group/index.html",
    "title": "Extract substrings from characters",
    "section": "",
    "text": "Opening help pages of functions used regularly is worth it. often, one discovers some hidden gems in argument functions. Here is an example with the str_extract() function from the package stringr.\n\n\n\n\n\nstringr logo"
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#hidden-gem",
    "href": "posts/2023-11-27_stringr_group/index.html#hidden-gem",
    "title": "Extract substrings from characters",
    "section": "",
    "text": "Opening help pages of functions used regularly is worth it. often, one discovers some hidden gems in argument functions. Here is an example with the str_extract() function from the package stringr.\n\n\n\n\n\nstringr logo"
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#aim-extract-the-second-group-of-two-digit-from-strings",
    "href": "posts/2023-11-27_stringr_group/index.html#aim-extract-the-second-group-of-two-digit-from-strings",
    "title": "Extract substrings from characters",
    "section": "Aim: extract the second group of two digit from strings",
    "text": "Aim: extract the second group of two digit from strings\nLet’s look at some strings that encode some code:\n\nFirst 2 digits are the year\nSecond 2 digits are the lab id\nThird 2 digits are the project id\n\nPlus some other strings that are not relevant here like INFO.\n\nlibrary(stringr)\n\ncodes &lt;- c(\"140102\", \"210301\", \"220501\", \"220502\", \"230101\", \"INFO\", \"230102\")\ncodes\n\n[1] \"140102\" \"210301\" \"220501\" \"220502\" \"230101\" \"INFO\"   \"230102\"\n\n\nFrom those we want to obtain the following:\n\"01\" \"03\" \"05\" \"05\" \"01\" NA   \"01\""
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#wrong-solution",
    "href": "posts/2023-11-27_stringr_group/index.html#wrong-solution",
    "title": "Extract substrings from characters",
    "section": "Wrong solution",
    "text": "Wrong solution\nIf we use str_sub() we can extract the second group of two digits, it does not work for INFO that should be NA.\n\nstr_sub(codes, 3, 4)\n\n[1] \"01\" \"03\" \"05\" \"05\" \"01\" \"FO\" \"01\"\n\n\nWe need a regular expression that specify 6 digits and take the second group of two digits."
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#using-regex",
    "href": "posts/2023-11-27_stringr_group/index.html#using-regex",
    "title": "Extract substrings from characters",
    "section": "Using regex",
    "text": "Using regex\nstr_extract() takes a regular expression as argument. We can use the following regex to extract the second group of two digits:\n\nstr_extract(codes, \"..\\\\d{2}..\")\n\n[1] \"140102\" \"210301\" \"220501\" \"220502\" \"230101\" NA       \"230102\"\n\n\nFine for detecting the INFO and exclude it but we don’t extract since the 2 previous and 2 following any characters (.) are included in the regex."
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#using-look-around-in-the-regex",
    "href": "posts/2023-11-27_stringr_group/index.html#using-look-around-in-the-regex",
    "title": "Extract substrings from characters",
    "section": "Using look around in the regex",
    "text": "Using look around in the regex\nLook arounds are extremely powerful but very complex. I never get it right the first time. I always need to look up the help pages.\n\nstr_extract(codes, \"(?&lt;=\\\\d{2})\\\\d{2}\")\n\n[1] \"01\" \"03\" \"05\" \"05\" \"01\" NA   \"01\"\n\n\nBut already we see the grouping popping up."
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#using-grouping-in-the-regex",
    "href": "posts/2023-11-27_stringr_group/index.html#using-grouping-in-the-regex",
    "title": "Extract substrings from characters",
    "section": "Using grouping in the regex",
    "text": "Using grouping in the regex\n\nstr_extract(codes, \"..(\\\\d{2})..\")\n\n[1] \"140102\" \"210301\" \"220501\" \"220502\" \"230101\" NA       \"230102\"\n\n\nThis is where I was usually stopped and return to look arounds.\nBut, str_extract() has a group argument that allows to extract the group of interest.\n\nstr_extract(codes, \"..(\\\\d{2})..\", group = 1)\n\n[1] \"01\" \"03\" \"05\" \"05\" \"01\" NA   \"01\"\n\n\nWorks! and easy to understand."
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#with-more-groups",
    "href": "posts/2023-11-27_stringr_group/index.html#with-more-groups",
    "title": "Extract substrings from characters",
    "section": "With more groups",
    "text": "With more groups\nFor example, the second group (5th and 6th characters):\n\nstr_extract(codes, \"..(\\\\d{2})(..)\", group = 2)\n\n[1] \"02\" \"01\" \"01\" \"02\" \"01\" NA   \"02\""
  },
  {
    "objectID": "posts/2023-11-27_stringr_group/index.html#conclusion",
    "href": "posts/2023-11-27_stringr_group/index.html#conclusion",
    "title": "Extract substrings from characters",
    "section": "Conclusion",
    "text": "Conclusion\nReading help pages help!"
  },
  {
    "objectID": "posts/2023-05-17_panel-size/index.html",
    "href": "posts/2023-05-17_panel-size/index.html",
    "title": "Same dot sizes",
    "section": "",
    "text": "library(ggplot2)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ek.plot)\nThis is how to create dots of the same size and optimized width using Eric Koncina package: ek.plot.\n\ndot_size &lt;- 4\nset.seed(1)\np &lt;- tidyr::expand_grid(row = letters[1:5], column = as.character(1:3)) |&gt; \n    mutate(size = sample(0:5, size = 15, replace = TRUE)) |&gt; \n    ggplot(aes(x = column, y = row, size = size)) +\n    geom_point() +\n    geom_point(shape = 21, size = dot_size, fill = NA, colour = \"gray\") +\n    scale_size(range = c(0, dot_size), limits = c(0, 5), breaks = c(0, 2, 5)) +\n    theme_bw() +\n    theme(\n        legend.key.height =  unit(0.5, \"lines\")\n    ) \np\nset_panel_size(p,\n                             width = 3 * unit(dot_size * ggplot2:::.pt, \"pt\"),\n                             height = 5 * unit(dot_size * ggplot2:::.pt, \"pt\")) |&gt; \n    write_plot(\"panel-size.png\")"
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html",
    "href": "posts/2023-08-17_starting-learning-rust/index.html",
    "title": "Starting to learn Rust",
    "section": "",
    "text": "I am very happy with  but I want to learn something new, another programming language. The desire comes with several needs\n\n\n\nA compiled language\n\nSpeed\nStrict syntax, almost pedantic\n\nStrongly typed\n\n\n\n\n\nFull of functional programming\nGreat packages offered by a great community (crates)\n\nTurns out this language is Rust."
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html#rationale",
    "href": "posts/2023-08-17_starting-learning-rust/index.html#rationale",
    "title": "Starting to learn Rust",
    "section": "",
    "text": "I am very happy with  but I want to learn something new, another programming language. The desire comes with several needs\n\n\n\nA compiled language\n\nSpeed\nStrict syntax, almost pedantic\n\nStrongly typed\n\n\n\n\n\nFull of functional programming\nGreat packages offered by a great community (crates)\n\nTurns out this language is Rust."
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html#advantages",
    "href": "posts/2023-08-17_starting-learning-rust/index.html#advantages",
    "title": "Starting to learn Rust",
    "section": "Advantages",
    "text": "Advantages\n\nCompiler\nIt is incredibly helpful. I am using VScode with the extension rust-analyser that provides immediate feedback, great suggestions to both warnings and errors.\n\n\nBonus: the  logo\nIt has the two things I like a lot:\n\nA  from  (a chainring)\nA big R that looks very much like \n\n\n\n\nRust logo"
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html#learning-material",
    "href": "posts/2023-08-17_starting-learning-rust/index.html#learning-material",
    "title": "Starting to learn Rust",
    "section": "Learning material",
    "text": "Learning material\n\n\n\nI am using so far three resources:\n\nThe Programming Rust book (Ed. O’Reilly). From a recommendation by Stefan Baumgartner\nThe official documentation that includes a lot of example.\nThe “How to learn Rust” course by Tim McManara (only $25). From a recommendation of Jonathan Caroll"
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html#example-of-syntax",
    "href": "posts/2023-08-17_starting-learning-rust/index.html#example-of-syntax",
    "title": "Starting to learn Rust",
    "section": "Example of syntax",
    "text": "Example of syntax\nPrograms are managed by the cargo utility. It creates, run, test, optimize Rust code. Here is one of official documentation example. We need the rand crate dependency. Here we specify the wanted version 0.8.5.\nThe code for the main guessing is included.\n\nCargo.tomlmain.rs\n\n\n[package]\nname = \"guessing_game\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\nrand = \"0.8.5\"\n\n\nuse std::io;\nuse std::cmp::Ordering;\nuse rand::Rng;\n\nfn main() {\n    println!(\"Guess the number!\");\n    \n    let max = 50u32;\n    // to keep track of the number of guesses\n    let mut count = 0u32;\n\n    // create an integer randomly from 1 to 50\n    let secret_number = rand::thread_rng().gen_range(1..=max);\n    // loop over until the number is found\n    loop { \n        count += 1;   \n        println!(\"Please input your guess. (an integer from 1 to {max})\");\n        \n        let mut guess: String = String::new();\n        \n        io::stdin()\n        .read_line(&mut guess)\n        .expect(\"Failed to read line\");\n        // coerce to an integer\n        let guess: u32 = match guess.trim().parse() {\n            Ok(num) =&gt; num,\n            // but offer possibily to change\n            Err(_) =&gt; {\n                println!(\"You entered {}, not a number\", guess.trim());\n                continue;\n            }\n        };\n        \n        println!(\"You guessed: {guess}\");  //while the secret was {secret_number}\");\n        \n        match guess.cmp(&secret_number) {\n            Ordering::Less =&gt; println!(\"Too small!\"),\n            Ordering::Greater =&gt; println!(\"Too big!\"),\n            Ordering::Equal =&gt; {\n                println!(\"You win! in {count} trials\");\n                break;\n            }\n        }\n    }\n}"
  },
  {
    "objectID": "posts/2023-08-17_starting-learning-rust/index.html#first-project-umi-trimming",
    "href": "posts/2023-08-17_starting-learning-rust/index.html#first-project-umi-trimming",
    "title": "Starting to learn Rust",
    "section": "First project: UMI trimming",
    "text": "First project: UMI trimming\nUMI stands for Unique Molecular Identifier.\nThe goal is reproduce some features from umi-tools especially the extract command.\nFor example, convert the following read\n@VH00666:90:AAAWVCCHV:1:1101:24026:1000\nGTCAGTTATAGCGGGCGCGCAAAAAAAAAAAAAAAAAAAGATCGGAAGAGCACACGTCTGAACTCCAGTCACTCCC\n[...]\ninto:\n@VH00666:90:AAAWVCCHV:1:1101:24026:1000_GTCAGT\nGCGGGCGCGCAAAAAAAAAAAAAAAAAAAGATCGGAAGAGCACACGTCTGAACTCCAGTCACTCCC\nThe UMI was GTCAGT and appended to the read name, while being removed from the sequence along with the TATA linker.\n\nRust-bio\nThis library rust-bio provides many features, and I am only using the\n\nalphabets to check the letters are actually IUPAC / DNA\nfastq for Reading/Writing FASTQ reads.\n\n\n\nThe Command Line Interface (CLI) utility: clap\nclap is an awesome library that helps making a CLI fun and easy.\nThe help output looks like (options actually appear in bold:\n$ umi_trim -h\nMove Unique Molecular Identifier from seq to name and trim a linker motif (TATA)\n\nUsage: umi_trim [OPTIONS] --input &lt;INPUT&gt; --output &lt;OUTPUT&gt;\n\nOptions:\n  -i, --input &lt;INPUT&gt;            FASTQ filename to read from\n  -o, --output &lt;OUTPUT&gt;          Filename to write to\n  -u, --umi-length &lt;UMI_LENGTH&gt;  UMI length in characters [default: 6]\n  -l, --linker &lt;LINKER&gt;          Linker UMI-READ to be discarded [default: TATA]\n  -h, --help                     Print help\n  -V, --version                  Print version\nThis project is here on"
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html",
    "title": "home surveillance monitored via telegram",
    "section": "",
    "text": "The raspberry pi has always been appealing to me, but I needed a project to really get involved. After discussing with Eric Koncina who made several great applications with Pis, I decided to go for a home surveillance system.\nThe main objective was to see how often the neighbor cats are coming to our garden, because they are scaring our cat. It’s not a big deal, rather a justification for the pi project."
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#rationale",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#rationale",
    "title": "home surveillance monitored via telegram",
    "section": "",
    "text": "The raspberry pi has always been appealing to me, but I needed a project to really get involved. After discussing with Eric Koncina who made several great applications with Pis, I decided to go for a home surveillance system.\nThe main objective was to see how often the neighbor cats are coming to our garden, because they are scaring our cat. It’s not a big deal, rather a justification for the pi project."
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#materials",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#materials",
    "title": "home surveillance monitored via telegram",
    "section": "Materials",
    "text": "Materials\nI bought a Pi3 starter budget kit that contains:\n\nPi3\npower, 5V, 2.5A\ncase\nSD card 16 Go\n\nAdditionally, I purchased:\n\nPi camera NoIR\n\nWas hoping to get some decent pictures / videos with low light. Turned out that IR leds are needed. That goes in the TODO section.\nHere is an example of picture with low interior light. Colors are off, but quality is fine to me"
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#setting-up-the-pi",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#setting-up-the-pi",
    "title": "home surveillance monitored via telegram",
    "section": "Setting-up the pi",
    "text": "Setting-up the pi\nI won’t go into details, I mostly followed the instructions in this tutorial. Briefly, here are the main steps\n\ndownload raspbian lite\nSince I have no screen, no keyboard and the pi comes with a WiFi controller, the stretch lite is sufficient. Image can be found at raspberrypi.org\n\n\nformat SD card\nusing disk utility, choose MS-DOS FAT file system\n\n\ninstall raspbian\nEnsure your SD card is the second disk (/dev/disk2), otherwise do adapt to the correct one!\nunzip 2017-11-29-raspbian-stretch-lite.zip\nsudo dd bs=1m if=2017-11-29-raspbian-stretch-lite.img of=/dev/rdisk2\n\n\nenable ssh\nOnce copied, you can enable ssh by creating an empty file at the SD card root\ncd /Volumes/boot/\ntouch ssh\n\n\nenable wifi\nIn order to connect to the pi without screen / keyboard, wifi needs to be configured right away. At the same location (/Volumes/boot) add a file named wpa_supplicant.conf\nwhich contains:\nnetwork={\n        ssid=\"your_network_ssid\"\n        psk=\"xxx\"\n        key_mgmt=WPA-PSK\n}\nOf note, I recently acquired a pi zeroWH, for which I had to add 3 lines (StackExchange question).\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\ncountry=FR\nnetwork={\n        ssid=\"your_network_ssid\"\n        psk=\"xxx\"\n        key_mgmt=WPA-PSK\n}\n\n\nconnect to pi\nonce the raspberrypi booted, try to find its IP\nnmap -sn 192.168.1.0/24\nwhich gives:\nStarting Nmap 7.60 ( https://nmap.org ) at 2017-12-08 22:44 CET\nNmap scan report for 192.168.1.27\nHost is up (0.0071s latency).\nNmap scan report for 192.168.1.254\nHost is up (0.0048s latency).\nNmap done: 256 IP addresses (2 hosts up) scanned in 11.71 seconds\n192.168.1.254 was the rooter, so pi was assigned 192.168.1.27\nssh pi@192.168.1.27 works.\nyou can also assign a fixed IP to your pi\n\n\nfinal configuration\nonce connected to the pi:\n\nrun sudo raspi-config to activate the camera\nchange password for the pi user\nset up locales and timezone\nupdate && upgrade raspbian stretch\nadd public ssh key to .ssh/authorized_keys for password less connection"
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#install-the-surveillance-system",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#install-the-surveillance-system",
    "title": "home surveillance monitored via telegram",
    "section": "install the surveillance system",
    "text": "install the surveillance system\nEven if, I’d like to have openCV like in this tutorial, it was way more work. Hence, the choice of motion\n\nmotion software\nI followed the instructions provided in this great tutorial by Bouvet. With some changes described below.\n\ncompilation\nsee from here https://motion-project.github.io/motion_build.html\nwget https://github.com/Motion-Project/motion/releases/download/release-4.1.1/pi_stretch_motion_4.1.1-1_armhf.deb\nsudo apt-get install gdebi-core\nsudo gdebi pi_stretch_motion_4.1.1-1_armhf.deb\n\n\n\nrun motion\nfirst, as Bouvet suggested, I copied the main config file\nmkdir ~/motion && cp /etc/motion/motion.conf ~/motion/\nand alter the new copy.\nrunning motion:\nmotion -c ~/motion/motion.conf\n\n\ntweaks to the initial tutorial\nI choose to get the videos and several other changes made to the motion version 4.1.1. and are reported in the complete diff page below\ndiff page for motion.conf\nThe initial values are reported in red, in green: the new ones.\n\n\n\nsee live streaming\nwith this configuration, you should see the live streaming from this URL: http://192.168.1.27:8081\nthe web control on port 8080 is disabled apart outside the pi, since we’ll use telegram to control motion\n\n\ndetection\nHere is an example of my kid being detected. The red rectangle works nicely\n\nBut of course, there are false alarms, such as when the light comes in/out suddenly\n\nthe parameter lightswitch 80 reduced the issue but it still exists."
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#communication-with-motion-via-telegram",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#communication-with-motion-via-telegram",
    "title": "home surveillance monitored via telegram",
    "section": "Communication with motion via telegram",
    "text": "Communication with motion via telegram\nNow comes the fun part. Receiving the motion detection by emails is fine, but it can be done via Telegram and the awesome API telepot. Eric told me about telegram bots and it looked promising. Actually, you can even send commands to your pi using your phone using those telegram bots.\nThe useful feature I implemented are:\n\nalerts. A motion is detected. Send the best picture to your telegram account.\npause / resume motion detection. Imagine you are away and for some reason (shadows, your own cat) you keep receiving alerts, you may want to remotely pause the detection. And of course, being able to resume it. Those commands are already in motion, we just need to talk to it.\nstatus. You haven’t received alerts, is the system running smoothly? You can ask for a confirmation that detection is on. Also, check if the camera is on.\nsnapshot. No alerts, but you’d like to get a snapshot at any time.\nvideo. Maybe the nicest feature IMHO. Sending picture to your phone for every detection is fine, but not all videos. Based on the picture you see, you’d like to get the video of the detection. Once again, by a command to a telegram bot, you receive the last video recorded.\n\n\ncreate mybot\nEric gave me the link to this tutorial\nOf course, I am assuming you already have our own telegram account.\nTalk to the BotFather and create mybot, you will receive a private token.\n\n\ninstall telepot\nback on the pi, install telepot with pip, assuming you installed python and pip.\npip install telepot\n\n\ntest sending message\n\nget the bot id\nimport telepot\nbot = telepot.Bot('your-token')\nbot.getMe()\nreturns {u'username': u'mybot', u'first_name': u'cat tracker', u'is_bot': True, u'id': 00000008}\n\n\nget your telegram id:\n\nsend a messages from telegram to mybot\nfetch your message on the pi\n\nfrom pprint import pprint\nresponse = bot.getUpdates()\npprint(response)\nyour id appears, such as: u’id’: 00000004\n\n\nbasic tests\n\nfor text\n\nbot.sendMessage(00000004, 'Hey!')\n\nfor picture\n\nbot.sendPhoto(00000004, photo=open('/home/pi/motion/detected/07-2018-01-06_205746-13.jpg', 'rb'), caption='motion detected')\n\n\ncreate commands for the bot\nAfter sending /setcommands to the BotFather:\ntime - Returns current time on pi\ncheck - Returns status of the camera\nstatus - Returns status of motion\npause - Pauses the motion detection\nresume - Resumes motion detection\nsnapshot - Returns current image\nvideo - Returns last recorded video\nOf note, it doesn’t prevent the bot to receive other commands, it just helps to display commands and select them in telegram.\n\n\npython script that listen\nhere the script listen_bot.py, derived from the telepot documentation.\nSome comments:\n\nthe last video when requested is fetched from the sub-folder vids. If we use the main folder of detection, the last video could be an incomplete one form a newer detection. Hence, the command in motion.conf to move a finished video to the vids folder.\nI failed to restrict the bot to communicate only with me. Might not be a big deal, but the code if chat_id != 00000008 is not working.\nthe webcontrol was set in the RAW mode. Then the retrieved text can be directly send to your telegram account\n\n#!/usr/bin/python2.7\n\nimport datetime\nimport telepot\nimport time\nimport requests\nimport os\nimport glob\nfrom telepot.loop import MessageLoop\n\ndef webcontrol(chat_id, type, cmd):\n    req = 'http://localhost:8080/0/'+type+'/'+cmd\n    res = requests.get(req)\n    bot.sendMessage(chat_id, res.text)\n\ndef handle(msg):\n    chat_id = msg['chat']['id']\n    command = msg['text']\n    #should work thanks to Winston\n    if msg['from']['id'] != 00000008:\n        bot.sendMessage(chat_id, \"Sorry this is a personal bot. Access Denied!\")\n        exit(1)\n\n    print 'Got command: %s' % command\n\n    if command == '/snapshot':\n        requests.get('http://localhost:8080/0/action/snapshot')\n    elif command == '/status':\n        webcontrol(chat_id, 'detection', 'status')\n    elif command == '/pause':\n        webcontrol(chat_id, 'detection', 'pause')\n    elif command == '/resume':\n        webcontrol(chat_id, 'detection', 'start')\n    elif command == '/check':\n        webcontrol(chat_id, 'detection', 'connection')\n    elif command == '/time':\n        bot.sendMessage(chat_id, 'now is '+str(datetime.datetime.now()))\n    elif command == '/video':\n        # the most recent video in this particular folder of complete vids\n        video = max(glob.iglob('/home/pi/motion/detected/vids/*.mp4'), key=os.path.getctime)\n        # send video, adapt the the first argument to your own telegram id\n        bot.sendVideo(00000008, video=open(video, 'rb'), caption='last video')\n    else:\n        bot.sendMessage(chat_id, \"sorry, I don't know the command \"+command)\n# adapt the following to the bot_id:bot_token\nbot = telepot.Bot('0000000004:bot_token')\n\nMessageLoop(bot, handle).run_as_thread()\nprint 'I am listening ...'\n\nwhile 1:\n    time.sleep(10)\nNow if the both listen_bot.py and motion -c ~/motion/motion.conf are running, the system should work.\n\n\nsending scripts\nthey are called in the motion.conf file.\nfirst, send_detection.py\n#!/usr/bin/python2.7\n\nimport telepot\nimport sys\n\nbot = telepot.Bot('0000000004:bot_token')\n\npic = sys.argv[1]\n\n# change caption if it is a snapshot or motion\nif pic.endswith(\"snapshot.jpg\"):\n    cap = 'snapshot'\nelse:\n    cap = 'motion detected'\n\nbot.sendPhoto(00000004, photo=open(pic, 'rb'), caption=cap)\n\nexit(0)\nsecond, send_message.py\n#!/usr/bin/python2.7\n\nimport telepot\nimport sys\n\nbot = telepot.Bot('0000000004:bot_token')\n\ntext = sys.argv[1]\n\nbot.sendMessage(00000004, text)\n\n\nrun scripts at startup\nEdit 2018-06-19\n\nfor listening\nI am now using systemd. Cleaner and safer.\nHere, the following is working, but I am sure this is the right way to do, so use we care.\nadd the file listen.service in the folder /etc/systemd/system:\n[Unit]\nDescription=Listen to foo telegram bot\nAfter=network.target\n\n[Service]\nExecStart=/usr/bin/python2.7 -u listen_bot.py\nWorkingDirectory=/home/pi/motion/\nStandardOutput=inherit\nStandardError=inherit\nRestart=always\nUser=motion\n\n[Install]\nWantedBy=multi-user.target\nnow as example, you can check the status of this new service listen\npi@raspberrypi:/etc/systemd/system $ service listen status\n● listen.service - Listen to foo telegram bot\n   Loaded: loaded (/etc/systemd/system/listen.service; enabled; vendor preset: enabled)\n   Active: active (running) since Sun 2018-05-13 21:51:50 CEST; 1 months 6 days ago\n Main PID: 502 (python2.7)\n   CGroup: /system.slice/listen.service\n           └─502 /usr/bin/python2.7 -u listen_bot.py\n\nMay 13 21:51:50 raspberrypi systemd[1]: Started Listen to foo telegram bot.\nMay 13 21:51:51 raspberrypi python2.7[502]: I am listening ...\nJun 19 20:52:17 raspberrypi python2.7[502]: Got command: /time\nAnd, safer since both motion and listen are running as the user motion who does not have sudo rights.\n\n\nFor motion\nI am now using the daemon mode to only benefit from systemd\n\npoint the usual conf file towards the tweaked version in your home\n\ncd /etc/motion\nmv motion.conf motion.conf.bak\nln -s /home/pi/motion/motion.conf .\nbe sure to have the correct smblink\nmotion.conf -&gt; /home/pi/motion/motion.conf\n\n\n\nScreenshots\nHere are some examples of the telegram window\n\nreceived a notification and later on, /check if connection is still on.\n\n\n\nreceived notification and ask for the corresponding video\n\n\nthis video works as a GIF directly in the window:\n\n\n\ngif file\n\n\n\n\npause the detection, and since no motion can be detected, ask for a snapshot\n\n\n\n\n\nTODO\nDespite a functional system, some improvements I’d like to achieve:\n\nrestrict the bot to one user: see Winston Smith recommendation in comments, seems to work nicely\nadd gracefull stop for listen service\nI bought a IP camera, and motion should work with both. Haven’t spent enough time configuring it\nremove pics/videos older than xx days to save space\nrun the 2 services as a user without sudo rights\nlook into better settings for NoIR camera using this thread"
  },
  {
    "objectID": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#conclusion",
    "href": "posts/2018-01-27_diy-raspberry-monitored-via-telegram/index.html#conclusion",
    "title": "home surveillance monitored via telegram",
    "section": "Conclusion",
    "text": "Conclusion\nBeyond the initial goal, catch the neighbor cats coming in, which actually I don’t care about, it was fun to set-up the whole thing. Moreover, telegram offers a great service and offers a great interface for many applications. I knew about the TeleR bot, that is actually easy to set-up.\nDon’t hesitate to leave a comment below for any remarks or improvements that I overseen."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ginolhac’s blog",
    "section": "",
    "text": "Denmark, ginolhac\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nInstall Archlinux again\n\n\n\n\n\n\nlinux\n\n\n\nOn carbon X1\n\n\n\n\n\nJul 29, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCreating modules with Easybuild \n\n\n\n\n\n\nlinux\n\n\nHPC\n\n\n\nGetting rid of conda/mamba/venv\n\n\n\n\n\nMar 25, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling LineageOS on an OnePlus \n\n\n\n\n\n\nlinux\n\n\nDIY\n\n\n\nMoving to an ungoogled-ROM\n\n\n\n\n\nJan 18, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nr2u for faster continuous integration\n\n\n\n\n\n\nR\n\n\nCI/CD\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nExtract substrings from characters\n\n\n\n\n\n\nbioinfo\n\n\nR\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling LineageOS on an old  S5\n\n\n\n\n\n\nlinux\n\n\nDIY\n\n\n\nFlashing an ungoogled-ROM\n\n\n\n\n\nNov 14, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n Maps Games\n\n\n\n\n\n\nbike\n\n\n\nIncentive to explore new places \n\n\n\n\n\nOct 9, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nResume long parallel jobs\n\n\n\n\n\n\nhpc\n\n\nbioinfo\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying  and  packages\n\n\n\n\n\n\npython\n\n\nR\n\n\nCI/CD\n\n\n\nwith  Gitlab Continuous Integration\n\n\n\n\n\nSep 14, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n Maintenance on the Ventoux, part 4\n\n\n\n\n\n\nbike\n\n\nDIY\n\n\n\nNew headset\n\n\n\n\n\nSep 14, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Archlinux\n\n\n\n\n\n\nlinux\n\n\n\nMoving away from Ubuntu\n\n\n\n\n\nSep 11, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Snakemake template\n\n\n\n\n\n\npython\n\n\nworkflow managers\n\n\nCI/CD\n\n\n\nusing Gitlab Continuous Integration\n\n\n\n\n\nSep 7, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarting to learn Rust\n\n\n\n\n\n\nRust\n\n\n\nTrimming UMI\n\n\n\n\n\nAug 17, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSame dot sizes\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n Maintenance on the Ventoux, part 3\n\n\n\n\n\n\nbike\n\n\nDIY\n\n\n\nReplacing for better parts\n\n\n\n\n\nApr 29, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nVertical faceting\n\n\n\n\n\n\nR\n\n\ndataviz\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCrop Circles\n\n\n\n\n\n\nR\n\n\npackages\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n{targets} demos\n\n\n\n\n\n\nR\n\n\nworkflow managers\n\n\npackages\n\n\n\nA marvelous workflow manager for \n\n\n\n\n\nFeb 5, 2023\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\n Rebuilding the Ventoux, part 2\n\n\n\n\n\n\nbike\n\n\nDIY\n\n\n\nReplacing the bottom bracket and more\n\n\n\n\n\nJul 14, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n Rebuilding the Ventoux, part 1\n\n\n\n\n\n\nbike\n\n\nDIY\n\n\n\nLock-down project: learning basic bike mechanics\n\n\n\n\n\nApr 17, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLaTex modern CV\n\n\n\n\n\n\nacademia\n\n\n\nHow to setup latex using tinytex\n\n\n\n\n\nMar 26, 2018\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nhome surveillance monitored via telegram\n\n\n\n\n\n\nPython\n\n\nDIY\n\n\nRasPI\n\n\n\nusing telepot and the NoIR camera\n\n\n\n\n\nJan 27, 2018\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nwinter is coming\n\n\n\n\n\n\nbike\n\n\n\nsnow on bike\n\n\n\n\n\nJan 25, 2015\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html",
    "href": "posts/2024-07-29_x1-archlinux/index.html",
    "title": "Install Archlinux again",
    "section": "",
    "text": "I had a Dell XPS13 for the last 5 years. It is a great laptop despite a sightly too small screen for my eyesight now. Also, the arrow keys mixed with the Page Up/Down was annoying, too close, too small.\nDuring one teaching class, one student had a Lenovo Carbon X1 and while helping I used the keyboard and liked it a lot. The not shiny screen looked good and 14 inches is an ideal compromise. Turns out it is also quite compact and weight less than the XPS13!\nI ordered high specs and with Ubuntu to be sure components worked nicely. Especially some webcam model is not supported [as reported before](https://wiki.archlinux.org/title/Talk:Lenovo_ThinkPad_X1_Carbon_(Gen_12). The screen is LED 1920x1200 which is just fine for work.\n\n\nThis is third laptop I am installing Arch linux on. A previous post was published.\n\n\ncurl -O https://mirror.alwyzon.net/archlinux/iso/2024.07.01/archlinux-2024.07.01-x86_64.iso\ncurl -O https://mirror.alwyzon.net/archlinux/iso/2024.07.01/archlinux-2024.07.01-x86_64.iso.sig\n# check signature\ngpg --keyserver-options auto-key-retrieve --verify  archlinux-2024.07.01-x86_64.iso.sig\nWorks:\ngpg: assuming signed data in 'archlinux-2024.07.01-x86_64.iso'\ngpg: Signature made Mon 01 Jul 2024 08:11:25 PM CEST\ngpg:                using EDDSA key 3E80CA1A8B89F69CBA57D98A76A5EF9054449A5C\ngpg:                issuer \"pierre@archlinux.org\"\ngpg: Good signature from \"Pierre Schmitz &lt;pierre@archlinux.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 3E80 CA1A 8B89 F69C BA57  D98A 76A5 EF90 5444 9A5C\nRight-click in GNOME Files on the ISO and open it with Disk Image Writer and start restoring on the 32GB USB stick.\n\n\n\n\nDisable Secure Boot in BIOS\nBoot on stick.\n\nOnce in the terminal prompt is root@archiso:\n\nincrease font size setfont ter-132b\n\n\n\n\nIn uni, Ethernet cable is not sufficient since requires 802.1X identity and certificate. Used instead WIFI with the wlan0 interface and the utils iwdctl (phone shared connection).\n\n\n\nFollowed the great guide from mjnaderi on Gist GH with some modifications.\nCheck the efi folder listed correctly and that 64 in firmware.\nDisk partitions:\nfdisk /dev/nvme0n1\n[...]\nResults after partioning.\nCommand (m for help): p\n\nDisk /dev/nvme0n1: 1.86 IiB, 2048209543168 bytes, 4000797360 sectors\nDisk model: KXG8AZNV2T04 LA KIOXIA\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: xxxxx\n\nDevice           Start         End     Sectors   Size Type\n/dev/nvme0n1p1    2048      206847      204800   100M EFI System\n/dev/nvme0n1p2  206848     2303999     2097152     1G Linux filesystem\n/dev/nvme0n1p3 2304000  3000796671 39998492672   1.9T Linux filesystem\nLike mjnaderi, no swap, we will set zwap later.\n\nFormat /dev/nvme0n1p1 as FAT32\nFormat /dev/nvme0n1p2 as ext4\n\nThen use cryptsetup for the main partition:\ncryptsetup --use-random luksFormat /dev/nvme0n1p3\ncryptsetup luksOpen /dev/nvme0n1p3 cryptlvm\n\n\npvcreate /dev/mapper/cryptlvm\nvgcreate vg0 /dev/mapper/cryptlvm\n\n\n\nPrevious install shows hat the root partition was a little small, so increase compare to mjnaderi.\nlvcreate --size 300G vg0 --name root \nlvcreate -l +100%FREE vg0 --name home\nlvreduce --size -256M vg0/home\nthen create ext4 file systems on root and home.\nmkfs.ext4 /dev/vg0/root\nmkfs.ext4 /dev/vg0/home\n\n\n\n\nmount /dev/vg0/root /mnt\nmount --mkdir /dev/nvme0n1p1 /mnt/efi\nmount --mkdir /dev/nvme0n1p2 /mnt/boot\nmount --mkdir /dev/vg0/home /mnt/home\n\n\n\nPlus some extras. For Intel CPUs the last one (for the module microcode)\npacstrap -K /mnt base linux linux-lts linux-firmware openssh less git vim sudo intel-ucode\n\n\n\ngenfstab -U /mnt &gt;&gt; /mnt/etc/fstab\n\n\n\narch-chroot /mnt /bin/bash\n\n\n\nmkinitcpio -P\n\n\n\npacman -S grub efibootmgr\ngrub-install --target=x86_64-efi --efi-directory=/efi --bootloader-id=GRUB\nIn /etc/default/grub edit\nGRUB_CMDLINE_LINUX=\"cryptdevice=/dev/nvme0n1p3:cryptlvm root=/dev/vg0/root\"\nand uncomment GRUB_ENABLE_CRYTODISK=y\nNow generate the main GRUB configuration file:\ngrub-mkconfig -o /boot/grub/grub.cfg\nIn case of problem, the manual command to open the encrypted partition\ncryptsetup luksOpen /dev/nvme0n1p3 cryptlvm\n\n\n\n\n\n\nIn case of stuck at \"Loading ramdisk\"\n\n\n\nDisable Memory protection in the BIOS\n\n\n\n\n\n\nAfter decryption you can log in as yourself.\nInstall GNOME and enable gdm:\nsudo pacman -S gnome gdm\nAnd enable gdm service:\nsudo systemctl enable gdm\n\n\n\nShould see the gdm login\n\n\n\nArch gdm login\n\n\n\n\ninstead of a swap partition. Using the AUR package zswapd and settings in etc/default/zramd\nthat max size = 8192MB (uncomment line)\n\nEnable service sudo systemctl enable zramd\n\n\n\n\n\n\nConnect to the wired network\n\nfollow SIU guidelines\n\nInstallation evolution-ews and backup from previous laptop / restore to get all settings imported\nInstall gnome-tweaks and disable CAPS LOCK key\nInstall extension-manager to have gnome Appindicator and KStatusNotiferItem Support increase icon size to 24\nSettings &gt; Keyboard\n\nAdd English Intl. with AltGr dead keys (not needed eventually)\nAlternate Character Keys -&gt; Right Super\nCompose Key -&gt; Right Alt\n\nthen right Alt + single quote, than e gives é\nAlso generate the locale en_US ISO-8859-1\nFor the sound to work, install ALSA with yay sof-firware and reboot\nInstall yay bluez-utils and enable service sudo systemctl enable bluetooth\nInstall from Github the starship prompt and yay fd: alternative to find\nMissing nerd fonts (especially in RStudio, default is blurry serif font)\n\nyay -S noto-fonts noto-fonts-emoji ttf-dejavu ttf-liberation ttf-meslo-nerd-font-powerlevel10k\n\nDisable PC speaker\n\ngsettings set org.gnome.desktop.wm.preferences audible-bell false\n\nConfig jotta-cli. follow dedicated AUR article at jottacloud.com\n\n\n\ncreate as root a file /etc/profile.d/disable-leds.sh which contains:\n!#/bin/bash\n\n# lenovo laptops have working mute mic/speaker keys\n# but leds are constant on. Disable leds at startup\n\necho off &gt; /sys/class/sound/ctl-led/mic/mode\necho off &gt; /sys/class/sound/ctl-led/speaker/mode\n\n\n\nOnce you recover your .ssh/ folder\nFor GNOME to avoid typing the passphrase, using its ssh-agent\n\nIn .bashrc add export SSH_AUTH_SOCK=$XDG_RUNTIME_DIR/grc/ssh\nSource the .bashrc\nyay seahorse\nsystemctl enable grc-ssh-agent.socket --user\nsystemctl start grc-ssh-agent.socket --user\nssh-add -L # add all keys\n\nThen keys are stored.\n\n\n\nExport both pub and secret from one machine:\ngpg --list-secret-keys --keyid-format LONG # to get the ID\ngpg --export-secret-keys -a 2835D53DC2D201D3 &gt; gpg-sc.asc\ngpg --export -a 2835D53DC2D201D3 &gt; gpg-pub.asc\nThen import on the new machine with gpg --import each file\n\n\n\nInstall with yay fprintd libprint-tod-git\nThen used fprintd-enroll to register a new fingerprint (5 times are done)\nCheck it is listed with fprintd-list yourlogin\nAnd test it manually with fprintd-verify\nAdd to sudo /etc/pam.d/gdm-fingerprint on top the following line:\nauth            sufficient      pam_fprintd.so\nthe next login in GDM offers the fingerprint. The keyring stills need to be open with a password\n\n\n\n\n\nTest the Xe driver\n\nwith the current kernel (6.10.2.arch1-1), when heavy multi-threading is happening (pak::pak(\"polars\") for example), the X server freeze and only a hard reboot works. Booting on linu-lts is fine (6.6.42-1).\n$ lspci -nn | grep VGA\n00:02.0 VGA compatible controller [0300]: Intel Corporation Meteor Lake-P [Intel Graphics] [8086:7d45] (rev 08)\nMeaning according to Arch wiki I should add the following Kernel parameters\n... i915.force_probe=!7d45 xe.force_probe=7d45\n\nFingerprint for opening the keyring and/or sudo commands?\n\nMight not be a good idea."
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html#install-arch-linux",
    "href": "posts/2024-07-29_x1-archlinux/index.html#install-arch-linux",
    "title": "Install Archlinux again",
    "section": "",
    "text": "This is third laptop I am installing Arch linux on. A previous post was published.\n\n\ncurl -O https://mirror.alwyzon.net/archlinux/iso/2024.07.01/archlinux-2024.07.01-x86_64.iso\ncurl -O https://mirror.alwyzon.net/archlinux/iso/2024.07.01/archlinux-2024.07.01-x86_64.iso.sig\n# check signature\ngpg --keyserver-options auto-key-retrieve --verify  archlinux-2024.07.01-x86_64.iso.sig\nWorks:\ngpg: assuming signed data in 'archlinux-2024.07.01-x86_64.iso'\ngpg: Signature made Mon 01 Jul 2024 08:11:25 PM CEST\ngpg:                using EDDSA key 3E80CA1A8B89F69CBA57D98A76A5EF9054449A5C\ngpg:                issuer \"pierre@archlinux.org\"\ngpg: Good signature from \"Pierre Schmitz &lt;pierre@archlinux.org&gt;\" [unknown]\ngpg: WARNING: The key's User ID is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 3E80 CA1A 8B89 F69C BA57  D98A 76A5 EF90 5444 9A5C\nRight-click in GNOME Files on the ISO and open it with Disk Image Writer and start restoring on the 32GB USB stick.\n\n\n\n\nDisable Secure Boot in BIOS\nBoot on stick.\n\nOnce in the terminal prompt is root@archiso:\n\nincrease font size setfont ter-132b\n\n\n\n\nIn uni, Ethernet cable is not sufficient since requires 802.1X identity and certificate. Used instead WIFI with the wlan0 interface and the utils iwdctl (phone shared connection).\n\n\n\nFollowed the great guide from mjnaderi on Gist GH with some modifications.\nCheck the efi folder listed correctly and that 64 in firmware.\nDisk partitions:\nfdisk /dev/nvme0n1\n[...]\nResults after partioning.\nCommand (m for help): p\n\nDisk /dev/nvme0n1: 1.86 IiB, 2048209543168 bytes, 4000797360 sectors\nDisk model: KXG8AZNV2T04 LA KIOXIA\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: xxxxx\n\nDevice           Start         End     Sectors   Size Type\n/dev/nvme0n1p1    2048      206847      204800   100M EFI System\n/dev/nvme0n1p2  206848     2303999     2097152     1G Linux filesystem\n/dev/nvme0n1p3 2304000  3000796671 39998492672   1.9T Linux filesystem\nLike mjnaderi, no swap, we will set zwap later.\n\nFormat /dev/nvme0n1p1 as FAT32\nFormat /dev/nvme0n1p2 as ext4\n\nThen use cryptsetup for the main partition:\ncryptsetup --use-random luksFormat /dev/nvme0n1p3\ncryptsetup luksOpen /dev/nvme0n1p3 cryptlvm\n\n\npvcreate /dev/mapper/cryptlvm\nvgcreate vg0 /dev/mapper/cryptlvm\n\n\n\nPrevious install shows hat the root partition was a little small, so increase compare to mjnaderi.\nlvcreate --size 300G vg0 --name root \nlvcreate -l +100%FREE vg0 --name home\nlvreduce --size -256M vg0/home\nthen create ext4 file systems on root and home.\nmkfs.ext4 /dev/vg0/root\nmkfs.ext4 /dev/vg0/home\n\n\n\n\nmount /dev/vg0/root /mnt\nmount --mkdir /dev/nvme0n1p1 /mnt/efi\nmount --mkdir /dev/nvme0n1p2 /mnt/boot\nmount --mkdir /dev/vg0/home /mnt/home\n\n\n\nPlus some extras. For Intel CPUs the last one (for the module microcode)\npacstrap -K /mnt base linux linux-lts linux-firmware openssh less git vim sudo intel-ucode\n\n\n\ngenfstab -U /mnt &gt;&gt; /mnt/etc/fstab\n\n\n\narch-chroot /mnt /bin/bash\n\n\n\nmkinitcpio -P\n\n\n\npacman -S grub efibootmgr\ngrub-install --target=x86_64-efi --efi-directory=/efi --bootloader-id=GRUB\nIn /etc/default/grub edit\nGRUB_CMDLINE_LINUX=\"cryptdevice=/dev/nvme0n1p3:cryptlvm root=/dev/vg0/root\"\nand uncomment GRUB_ENABLE_CRYTODISK=y\nNow generate the main GRUB configuration file:\ngrub-mkconfig -o /boot/grub/grub.cfg\nIn case of problem, the manual command to open the encrypted partition\ncryptsetup luksOpen /dev/nvme0n1p3 cryptlvm\n\n\n\n\n\n\nIn case of stuck at \"Loading ramdisk\"\n\n\n\nDisable Memory protection in the BIOS"
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html#reboot-to-your-session-text",
    "href": "posts/2024-07-29_x1-archlinux/index.html#reboot-to-your-session-text",
    "title": "Install Archlinux again",
    "section": "",
    "text": "After decryption you can log in as yourself.\nInstall GNOME and enable gdm:\nsudo pacman -S gnome gdm\nAnd enable gdm service:\nsudo systemctl enable gdm"
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html#reboot-to-gnome",
    "href": "posts/2024-07-29_x1-archlinux/index.html#reboot-to-gnome",
    "title": "Install Archlinux again",
    "section": "",
    "text": "Should see the gdm login\n\n\n\nArch gdm login\n\n\n\n\ninstead of a swap partition. Using the AUR package zswapd and settings in etc/default/zramd\nthat max size = 8192MB (uncomment line)\n\nEnable service sudo systemctl enable zramd"
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html#post-install",
    "href": "posts/2024-07-29_x1-archlinux/index.html#post-install",
    "title": "Install Archlinux again",
    "section": "",
    "text": "Connect to the wired network\n\nfollow SIU guidelines\n\nInstallation evolution-ews and backup from previous laptop / restore to get all settings imported\nInstall gnome-tweaks and disable CAPS LOCK key\nInstall extension-manager to have gnome Appindicator and KStatusNotiferItem Support increase icon size to 24\nSettings &gt; Keyboard\n\nAdd English Intl. with AltGr dead keys (not needed eventually)\nAlternate Character Keys -&gt; Right Super\nCompose Key -&gt; Right Alt\n\nthen right Alt + single quote, than e gives é\nAlso generate the locale en_US ISO-8859-1\nFor the sound to work, install ALSA with yay sof-firware and reboot\nInstall yay bluez-utils and enable service sudo systemctl enable bluetooth\nInstall from Github the starship prompt and yay fd: alternative to find\nMissing nerd fonts (especially in RStudio, default is blurry serif font)\n\nyay -S noto-fonts noto-fonts-emoji ttf-dejavu ttf-liberation ttf-meslo-nerd-font-powerlevel10k\n\nDisable PC speaker\n\ngsettings set org.gnome.desktop.wm.preferences audible-bell false\n\nConfig jotta-cli. follow dedicated AUR article at jottacloud.com\n\n\n\ncreate as root a file /etc/profile.d/disable-leds.sh which contains:\n!#/bin/bash\n\n# lenovo laptops have working mute mic/speaker keys\n# but leds are constant on. Disable leds at startup\n\necho off &gt; /sys/class/sound/ctl-led/mic/mode\necho off &gt; /sys/class/sound/ctl-led/speaker/mode\n\n\n\nOnce you recover your .ssh/ folder\nFor GNOME to avoid typing the passphrase, using its ssh-agent\n\nIn .bashrc add export SSH_AUTH_SOCK=$XDG_RUNTIME_DIR/grc/ssh\nSource the .bashrc\nyay seahorse\nsystemctl enable grc-ssh-agent.socket --user\nsystemctl start grc-ssh-agent.socket --user\nssh-add -L # add all keys\n\nThen keys are stored.\n\n\n\nExport both pub and secret from one machine:\ngpg --list-secret-keys --keyid-format LONG # to get the ID\ngpg --export-secret-keys -a 2835D53DC2D201D3 &gt; gpg-sc.asc\ngpg --export -a 2835D53DC2D201D3 &gt; gpg-pub.asc\nThen import on the new machine with gpg --import each file\n\n\n\nInstall with yay fprintd libprint-tod-git\nThen used fprintd-enroll to register a new fingerprint (5 times are done)\nCheck it is listed with fprintd-list yourlogin\nAnd test it manually with fprintd-verify\nAdd to sudo /etc/pam.d/gdm-fingerprint on top the following line:\nauth            sufficient      pam_fprintd.so\nthe next login in GDM offers the fingerprint. The keyring stills need to be open with a password"
  },
  {
    "objectID": "posts/2024-07-29_x1-archlinux/index.html#todo",
    "href": "posts/2024-07-29_x1-archlinux/index.html#todo",
    "title": "Install Archlinux again",
    "section": "",
    "text": "Test the Xe driver\n\nwith the current kernel (6.10.2.arch1-1), when heavy multi-threading is happening (pak::pak(\"polars\") for example), the X server freeze and only a hard reboot works. Booting on linu-lts is fine (6.6.42-1).\n$ lspci -nn | grep VGA\n00:02.0 VGA compatible controller [0300]: Intel Corporation Meteor Lake-P [Intel Graphics] [8086:7d45] (rev 08)\nMeaning according to Arch wiki I should add the following Kernel parameters\n... i915.force_probe=!7d45 xe.force_probe=7d45\n\nFingerprint for opening the keyring and/or sudo commands?\n\nMight not be a good idea."
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "",
    "text": "Following the previous attempt (successful) of installing the LineageOS described here, I decided to really change my previous phone from 2018 which last security update was 2020-06-01. The hardware was still working, but the lack of updates was really annoying, stuck with Android 10 and all security holes."
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html#candidate-phone",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html#candidate-phone",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "Candidate phone",
    "text": "Candidate phone\nMy sister had a One Plus 6T in a drawer collecting dust. She wiped it and gave it to me, nice!"
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html#lineageos-on-the-one-plus-6t",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html#lineageos-on-the-one-plus-6t",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "LineageOS on the One Plus 6T",
    "text": "LineageOS on the One Plus 6T\nLineageOS supports so many (and old) devices it’s amazing. The One Plus 6T corresponds to the fajita release and builds are here.\n\nInstallation\nThe 8-steps instructions are clearly described in the wiki.\nUsing the OxygenOS from OnePlus, I had to update from Android 10 to 11 as asked.\nAfter activating the USB debugging, I had to add my login to the plugdev group while it was not necessary for the Samsung S5.\nusermod -aG plugdev login\nHad to restart the laptop, check you are part of this group with id.\n\n\n\n\n\n\nOf note\n\n\n\nAll commands are run as root.\n\n\n\n\nUnlock OEM\nAfter rebooting in proprietary recovery:\n\nThe following commands:\nfastboot devices\nfastboot oem unlock\nare super fast:\n\nResult:\n\n\n\nInstall lineage recovery\nfastboot flash boot.img\n\nCheck that after rebooting in recovery mode that you have the LineageOS recovery:\n\nThen select Apply update on the phone using volume Up/Down and Power to confirm. On the laptop, sideload the main lineage image, 2 accessories and MindtheGapps if you want to keep Google Play.\n\n\n\n\n\n\nWatch out\n\n\n\nTo use the right architecture and Android version (13 here).\n\n\nI am not sure what the dtbo and vbmeta images actually are but I did follow all steps.\n\n\nadb -d sideload lineage-20.0-20240116-nightly-fajita-signed.zip \nadb -d sideload dtbo.img \nadb -d sideload vbmeta.img \nadb -d sideload MindTheGapps-13.0.0-arm64-20231025_200931.zip\n\nand reboot system."
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html#aftermath",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html#aftermath",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "Aftermath",
    "text": "Aftermath\nWe started from Android 11, now we have 13. And we continue receiving updates (already one a week after this installation). Everything is working. The only annoying part is the Fingerprint reader which success reading rate is around 50%. Don’t know what is wrong, the sensor or the software treating information.\nThe bank apps are working, Luxtrust and so on as one friend was telling me that un-rooted ROM prevents usage of these apps.\nWhile installing messaging apps, it was interesting to see the different levels of security for restoring/transferring accounts.\n\nSignal It is not allowed to have the app working on both phones using the same phone number. It makes sense, even if I have only Wifi on the old phone, it could be used to send messages but not with Signal. A procedure is proposed to transfer the content (contacts / messages / groups) from the old -&gt; new phone using Bluetooth. It worked perfectly. Of note, all linked Signal Desktop were un-linked and I had to re-link them using the new phone.\nTelegram Very little security. On the new phone, installed the app, received a confirmation by SMS and that’s it. All chats were restored, and the old phone can still be used along with the Desktop apps.\nSimplex Chat This messaging app is nice as it does not require any account. Transferring data was feasible, I had to archive an archive with a passphrase, copy over this file and restore the database after unlocking it with the passphrase.\n\nEach boot is showing this warning, I gladly ignore."
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html#edit-android-14",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html#edit-android-14",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "Edit: Android 14",
    "text": "Edit: Android 14\nLineageOS offered on 2024-02-16 the upgrade to Android 14, called LineageOS version 21. I did it, and it was smooth. No data loss, no settings lost. The phone is still working fine. I followed the instructions for adb sideload and it worked."
  },
  {
    "objectID": "posts/2024-01-18_lineageos-oneplus/index.html#whats-next",
    "href": "posts/2024-01-18_lineageos-oneplus/index.html#whats-next",
    "title": "Installing LineageOS on an OnePlus ",
    "section": "What’s next?",
    "text": "What’s next?\nGetting rid of Google Play. Or Google account altogether. Next goal is to try CalyxOS and the Aurora store. I have restored some security with LineageOS but no privacy since I kept Google Play."
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html",
    "href": "posts/2023-11-16_lineagesos-s5/index.html",
    "title": "Installing LineageOS on an old  S5",
    "section": "",
    "text": "Bought a Samsung S5  in October 2014. It’d served me well for 4 years and since was lying in a drawer. Now the next and current smartphone is 5 years old and show some issues. Nothing dramatic but one is annoying, it is not receiving updates, especially security ones for 3 years now. I don’t have much knowledge about the consequences but clearly it is not great. Thus, here are four motivations for installing an open-source OS on a phone:"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#current-android-and-state",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#current-android-and-state",
    "title": "Installing LineageOS on an old  S5",
    "section": "Current Android and state",
    "text": "Current Android and state\nThe phone was as expected at 0% battery. Stayed turn off for 5 years. After charging it for a few moments it booted nicely, everything seems to work. Android version was 6.01"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#lineageos",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#lineageos",
    "title": "Installing LineageOS on an old  S5",
    "section": "LineageOS",
    "text": "LineageOS\nI know nothing about alternative Android OS. I show someone on Mastodon installing LineageOS so I went for this one.\n\n\n\n\n\nLineage logo\n\n\nLuckily, the S5 is a supported devices and I am supposed to install klte Lineage version 18.1. I followed the steps and of course it went wrong, I assumed the Odin step was optional. It is not."
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#workflow",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#workflow",
    "title": "Installing LineageOS on an old  S5",
    "section": "Workflow",
    "text": "Workflow\nThe recovery mode is protected. If we reboot in this mode, it is to recover the Samsung Android OS. Did not understand that the Odin step was about erasing this by the Lineage recovery image. That explained the different images available for klte.\n\nSetup the laptop with the necessary tools:\n\nadb android development\nodin, the  worked nicely, not true we need the  official software\n\nReboot the phone in download mode\nInstall the lineage recovery image\nReboot quicky in recovery mode (otherwise Samsung erases step 3., happened twice….)\nSideload the lineage OS (previously converted from .img -&gt; .tar)\nReboot, LneageOs get installed.\n\nI tried the install of  with MindTheGaps but it failed and turned out it is available anyway. Well I don’t know what happened here.\nLet’s see in details the different steps"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#mandatory-software",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#mandatory-software",
    "title": "Installing LineageOS on an old  S5",
    "section": "Mandatory software",
    "text": "Mandatory software\nThe instructions are available here.\nOn Arch  what was needed:\nyay odin4-cli android-sdk-platform-tools\n\n\n\n\n\n\nNote\n\n\n\nOf note, android-sdk-platform-tools provides both adb and fastboot but there are located in /opt/android-sdk/platform-tools/. I created symbolic links for both tools in ~/bin/\n\n\nThen I followed the instructions for setting adb\nTo use adb with your device, you’ll need to enable developer options and USB debugging:\n\nOpen Settings, and select “About”.\nTap on “Build number” seven times.\nGo back, and select “Developer options”.\nScroll down, and check the “Android debugging” or “USB debugging” entry under “Debugging”.\nPlug your device into your computer.\nOn the computer, open up a terminal/command prompt and type adb devices.\nA dialog should show on your device, asking you to allow usb debugging. Check “always allow”, and choose OK\n\nI did not remember doing the step 7. Worked anyway."
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#download-images",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#download-images",
    "title": "Installing LineageOS on an old  S5",
    "section": "Download images",
    "text": "Download images\nFollowing instructions, images are there.\nI downloaded\n\nlineage-18.1-20231102-nightly-klte-signed.zip\nrecovery.img\n\nand check their sha256sums.\nThe recovery image is the first one needed, but the tar version. To convert img to tar:\ntar --format=ustar -cvf recovery.tar recovery.img"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#install-the-custom-recovery-mode",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#install-the-custom-recovery-mode",
    "title": "Installing LineageOS on an old  S5",
    "section": "Install the custom recovery mode",
    "text": "Install the custom recovery mode\nPlug in the phone in your laptop and be sure that adb devices returns a number. Otherwise, it means that the USB debugging is not enabled on the phone.\n\n\n\n\n\n\nMandatory step!\n\n\n\nI first missed this part. The factory recovery mode will not allow you to install anything. You must complete this step AND prevent the rebooting that re-install the factory one.\n\n\nThis implied rebooting in download mode, can be done with either Volume down + Home + Power or\nadb reboot download\nThen once ready, to install the custom recovery from Lineage\n\n\n\n\n\n\nRoot permissions\n\n\n\nodin4 must be executed as root, took me too much yime to figure this out.\n\n\nsudo odin4 -a recovery.tar\nThe screen looks like this:\n\n\n\nDownload mode while installing a custom recovery\n\n\nWhen it is done, I removed the battery, unplugged the phone to be sure it does not erase this custom recovery.\nNow boot into this custom recovery mode. With the device powered off, hold Volume Up + Home + Power. When the blue text appears, release the buttons.\nIt must look like this, with the Lineage OS logo:\n\n\n\nCustom recovery"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#upload-the-lineage-os-image",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#upload-the-lineage-os-image",
    "title": "Installing LineageOS on an old  S5",
    "section": "Upload the Lineage OS image",
    "text": "Upload the Lineage OS image\nOn your laptop, upload aka sideload the image, go to Apply Update\nand on the laptop type:\nadb sideload lineage-18.1-20231102-nightly-klte-signed.zip\nOnce done, navigate on the phone to Reboot system now and the new OS is being installed after the reboot, it should last less than 15 minutes (took not even 5 for me)."
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#screenshot",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#screenshot",
    "title": "Installing LineageOS on an old  S5",
    "section": "Screenshot",
    "text": "Screenshot\nAfter installing F-droid and some apps, everything works great and the phone is still very responsive! Android version went from 6.01 to 11.0.\n\n\n\nLineageOS 18.1"
  },
  {
    "objectID": "posts/2023-11-16_lineagesos-s5/index.html#edit-2024-02-03-the-screen-curtain-drama",
    "href": "posts/2023-11-16_lineagesos-s5/index.html#edit-2024-02-03-the-screen-curtain-drama",
    "title": "Installing LineageOS on an old  S5",
    "section": "Edit 2024-02-03: The Screen Curtain drama",
    "text": "Edit 2024-02-03: The Screen Curtain drama\nMy daughter is using this S5 happily for 2 months now. No email account, no google apps but F-droid is great. Then today, she played with many options as usual when in the screen accessibility, she activated the screen curtain. Meant for blind people to preserve their privacy, the whole screen turns totally black once you enter your PIN. Going on the Internet, the best thread was on reddit. I tried the scrcpy (which is a great software!), the adb shell but all those solutions involved the USB debugging on that was not anymore the case. The only bit that work was scrcpy --otg but I did not get what was needed to navigate, or to send relevant characters. So in end, I rebooted in the recovery mode (Vol Up + Home + Power). adb shell worked, I was able to enable it but, even mounting in /mnt/system the data/ folder was empty. Thus, I wiped out the partition and sideloaded LineageOS again:\nlineage-18.1-20240201-nightly-klte-signed.zip\nAnd we are able to use the phone again. All custom / app / pictures gone but she will remember this lesson apparently."
  },
  {
    "objectID": "posts/2023-09-17_ventoux-4/index.html",
    "href": "posts/2023-09-17_ventoux-4/index.html",
    "title": " Maintenance on the Ventoux, part 4",
    "section": "",
    "text": "Post series\n\n\n\nThis post belongs to a series about the Ventoux:\n\nMaintenance 1 on the 2020-04-27\nMaintenance 2 on the 2020-07-14\nMaintenance 3 on the 2023-04-29\nMaintenance 4 this post"
  },
  {
    "objectID": "posts/2023-09-17_ventoux-4/index.html#headset",
    "href": "posts/2023-09-17_ventoux-4/index.html#headset",
    "title": " Maintenance on the Ventoux, part 4",
    "section": "Headset",
    "text": "Headset\nIn 2020, the headset was serviced minimally and without much knowledge by me. After unmounting the headset, I cleaned all little balls (lost one!) and mounted back with new grease.\n\n\n\nCleaning up the bearings\n\n\nIn 2023, the up cup made of plastic started to display a crack that became bigger and bigger. I decided to have done by a professional who changed the full headset, for 19 euros material and 30 for man hour. Feels much more solid, all metallic cups and new steering feeling.\n\n\n\nNew headset"
  },
  {
    "objectID": "posts/2023-09-17_ventoux-4/index.html#front-105-caliper",
    "href": "posts/2023-09-17_ventoux-4/index.html#front-105-caliper",
    "title": " Maintenance on the Ventoux, part 4",
    "section": "Front 105 caliper",
    "text": "Front 105 caliper\nSince the rear 105 brought satisfaction, I invested for its complement for the front brake. Bye bye Sora, you served me well."
  },
  {
    "objectID": "posts/2023-09-11_install-archlinux/index.html",
    "href": "posts/2023-09-11_install-archlinux/index.html",
    "title": "Install Archlinux",
    "section": "",
    "text": "I started using GNU/Linux in 2000, discovering it in a master, it was RedHat on those shared machines. At this point, I got hooked and installed a now dead distribution called Mandrake (then rebranded Mandriva)\n\n\n\n\n\nMandriva logo\n\n\n\n\n\nLater in 2003, Christophe Badoit installed for the biotech company I was working in: debian. I knew he was then using Archlinux but it took many years to make this switch.\n\n\n\n\n\nDebian logo\n\n\nDebian was absolutely great, and I was an happy user until 2010. Some people in the company started to use Ubuntu but I stick to the original. My main issue was the latency between the publications of new versions their arrival as debian packages, even in unstable.\nIn 2010, I quit the company and returned to academia. On a brand-new laptop, I installed Ubuntu. It became my main distro until today. It was a good journey, much easier to install than debian, oki-sh release of new versions and the apt system that was robust and nice to use.\n\n\n\n\n\nUbntu logo\n\n\n\n\nTime passing by, I got fed up with several aspects of Ubuntu:\n\nBloat software, who needs those pre-installed stuff?\nGNOME overlay, this Dock and forked gnome experience started to feel weird\nWayland integration was still not fine\nThe snap world, no thanks, I was here for apt\nDelay to get new version.\n\nTo expand on point 2-4, during lock-downs, sharing screen through the madness of webex, teams etc.. in Wayland was not great. If you had the snap firefox, it could not see some files on a ssh mounted folder. For point 5. for example  version 4.3.0 was released in April 2023. Using Ubuntu version dev 23.10 in September 2023 I was still stuck with  4.2.2 from October 2022. I know I could set up ppa repository of some users but with snap it became all too far from the great apt system I enjoyed for long.\nTurns out I need a rolling release distribution."
  },
  {
    "objectID": "posts/2023-09-11_install-archlinux/index.html#history-of-distribution-usage",
    "href": "posts/2023-09-11_install-archlinux/index.html#history-of-distribution-usage",
    "title": "Install Archlinux",
    "section": "",
    "text": "I started using GNU/Linux in 2000, discovering it in a master, it was RedHat on those shared machines. At this point, I got hooked and installed a now dead distribution called Mandrake (then rebranded Mandriva)\n\n\n\n\n\nMandriva logo\n\n\n\n\n\nLater in 2003, Christophe Badoit installed for the biotech company I was working in: debian. I knew he was then using Archlinux but it took many years to make this switch.\n\n\n\n\n\nDebian logo\n\n\nDebian was absolutely great, and I was an happy user until 2010. Some people in the company started to use Ubuntu but I stick to the original. My main issue was the latency between the publications of new versions their arrival as debian packages, even in unstable.\nIn 2010, I quit the company and returned to academia. On a brand-new laptop, I installed Ubuntu. It became my main distro until today. It was a good journey, much easier to install than debian, oki-sh release of new versions and the apt system that was robust and nice to use.\n\n\n\n\n\nUbntu logo\n\n\n\n\nTime passing by, I got fed up with several aspects of Ubuntu:\n\nBloat software, who needs those pre-installed stuff?\nGNOME overlay, this Dock and forked gnome experience started to feel weird\nWayland integration was still not fine\nThe snap world, no thanks, I was here for apt\nDelay to get new version.\n\nTo expand on point 2-4, during lock-downs, sharing screen through the madness of webex, teams etc.. in Wayland was not great. If you had the snap firefox, it could not see some files on a ssh mounted folder. For point 5. for example  version 4.3.0 was released in April 2023. Using Ubuntu version dev 23.10 in September 2023 I was still stuck with  4.2.2 from October 2022. I know I could set up ppa repository of some users but with snap it became all too far from the great apt system I enjoyed for long.\nTurns out I need a rolling release distribution."
  },
  {
    "objectID": "posts/2023-09-11_install-archlinux/index.html#archlinux",
    "href": "posts/2023-09-11_install-archlinux/index.html#archlinux",
    "title": "Install Archlinux",
    "section": "Archlinux",
    "text": "Archlinux\nI since installed Arch on a new machine, see updated post\n\nThe pkg.tar.xz world\n\n\n\nArchlinux logo\n\n\nWelcome to pacman and other great stuff. For example, disk encryption, where with Ubuntu it was not obvious how to set it up. This is now under control.\npacman is dealing with your installation and dependencies (just like apt). Software and utilities are coming from either\n\ncore\nextra\n\nBut what about RStudio? Signal? Slack?\nAll those accessory[^Of note, RStudio for me it absolutely not accessory, but they not for ] software can be found in the AUR: Arch User Repository.\nThen, for dealing with those 3 sources I was advice to use yay. After a couple of months, I enjoy typing yay in the Terminal and get the source updated + the packages. right now, I have only one AUR package to update.\n$ yay\n[sudo] password for xxxxx: \n:: Synchronizing package databases...\n core is up to date\n extra                                                   8.3 MiB  10.6 MiB/s 00:01 [################################################] 100%\n:: Searching AUR for updates...\n:: Searching databases for updates...\n -&gt; Flagged Out Of Date AUR Packages: telegram-desktop-bin\n:: 1 package to upgrade/install.\n1  aur/signal-desktop-beta-bin  6.29.0beta.1-1 -&gt; 6.31.0beta.1-1\n==&gt; Packages to exclude: (eg: \"1 2 3\", \"1-3\", \"^4\" or repo name)\n -&gt; Excluding packages may cause partial upgrades and break systems\n==&gt; \nBut next time, I will be notified of any update for open-ssh, rstudio or quarto.\nOf note, for the later, you can choose if you want to install the release version, the pre-release, binaries or compile things yourself. So far it fits completely my needs.\nOf note, I got the  version of I wanted without any tweaks:\n$ pacman -Q r\nr 4.3.1-2\n\nExample of a bigger upgrade\n:: Synchronizing package databases...\n core                                                        129.3 KiB   994 KiB/s 00:00 [###################################################] 100%\n extra                                                         8.3 MiB  26.1 MiB/s 00:00 [###################################################] 100%\n:: Searching AUR for updates...\n:: Searching databases for updates...\n:: 56 packages to upgrade/install.\n56  core/glib2                                   2.76.5-1         -&gt; 2.78.0-1\n55  core/glib2-docs                              2.76.5-1         -&gt; 2.78.0-1\n54  core/iana-etc                                20230803-1       -&gt; 20230907-1\n53  core/iproute2                                6.4.0-1          -&gt; 6.5.0-1\n52  core/linux                                   6.4.12.arch1-1   -&gt; 6.5.2.arch1-1\n51  core/openssh                                 9.4p1-3          -&gt; 9.4p1-4\n50  core/python                                  3.11.5-1         -&gt; 3.11.5-2\n49  core/shadow                                  4.13-2           -&gt; 4.13-3\n48  core/systemd                                 254.1-1          -&gt; 254.3-1\n47  core/systemd-libs                            254.1-1          -&gt; 254.3-1\n46  core/systemd-sysvcompat                      254.1-1          -&gt; 254.3-1\n45  extra/code                                   1.81.1-1         -&gt; 1.82.0-1\n44  extra/freerdp                                2:2.10.0-4       -&gt; 2:2.11.1-1\n[...]\n 8  extra/npm                                    9.8.1-1          -&gt; 10.1.0-1\n 7  extra/openpmix                               4.2.5-1          -&gt; 4.2.6-1\n 6  extra/pandoc-cli                             0.1.1-41         -&gt; 0.1.1-43\n 5  extra/vulkan-icd-loader                      1.3.255-1        -&gt; 1.3.263-1\n 4  aur/oh-my-posh-bin                           18.7.0-1         -&gt; 18.8.1-1\n 3  aur/quarto-cli-bin-pre-release               1.4.352-6        -&gt; 1.4.358-6\n 2  aur/rstudio-desktop-bin                      2023.06.1.524-1  -&gt; 2023.06.2.561-1\n 1  aur/signal-desktop-beta-bin                  6.29.0beta.1-1   -&gt; 6.31.0beta.1-1\n\n\n\nArch installation with full disk encryptiom\nI first installed Arch on my personal laptop (ThinkPad P14s) for testing (without encryption). I then proceed with my work laptop (XPS 7390), mostly following this install tutorial by mjnaderi. The encryption of the full disk works well using LVM2 and GRUB.\nThe differences were:\n\nInstall the linux-lts kernel in case something gets broken with the current one (thanks Hyacinthe!)\nNo dual boot with \n\nThe most tricky part is GRUB with efi but this tutorial steps were clear and correct.\n\n\nPost-install adjustments\n\ndocker\n\nyay docker\nsudo systemctl enable docker\nsudo usermod -aG docker login\n\nDisable PC speaker using gsettings set org.gnome.desktop.wm.preferences audible-bell false\nMissing fonts\n\nyay noto-fonts noto-fonts-emoji ttf-dejavu ttf-liberation ttf-meslo-nerd-font-powerlevel10k\n\nUseful packages\n\nyay git-lfs cmake udunits gcc-fortran rustup sshfs\n\nChromium (needed for {renderthis}): yay ungoogled-chromium-bin\nJottacloud: yay jotta-cli\nSlack: yay slack-desktop\nSignal and Telegram: yay telegram-desktop-bin signal-desktop-beta-bin\nSystem tray notifications yay extension-manager, then start App Extension Manager and install:\n\n\n\n\nAppIndicator\n\n\nThen it displays things like this:\n\n\n\nSystem tray\n\n\n\nFor the  package {V8}, needs to activate the static lib before installing:\n\nSys.setenv(DOWNLOAD_STATIC_LIBV8 = 1)\n\nAvoid typing the GPG passphrase and save it in GNOME\n\nin ~/.config/gnupg/gpg-agent.conf\npinentry-program /usr/bin/pinentry-gnome3\nallow-preset-passphrase\nmax-cache-ttl 60480000\ndefault-cache-ttl 604480000"
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html",
    "href": "posts/2023-04-29_ventoux-3/index.html",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "",
    "text": "Post series\n\n\n\nThis post belongs to a series about the Ventoux:\n\nMaintenance 1 on the 2020-04-27\nMaintenance 2 on the 2020-07-14\nMaintenance 3 this post\nMaintenance 4 on the 2023-09-17\nAfter some years, the bike is really a good commuter and brings much joy. However, like every machine it needs maintenance. And this offers chances for better spare items. Especially, the Sora series from Shimano is cheap but show some clear weaknesses to cold, salt and water. They got rusty and not so pleasant to use, so today we need better:\nLike my famous Danish friend, who I quote frequently, says:\nI must mention, he actually uses a SRAM derailleur, but let’s say he is right, the 105 are very nice products. I know them from my gravel and several years later they do work nicely."
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html#ordering",
    "href": "posts/2023-04-29_ventoux-3/index.html#ordering",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "Ordering",
    "text": "Ordering\n\nbike24.de Order March 2023\n\n\n\n\n\n\n\n\nQty\nItem No.\nDescription\nPrice\n\n\n\n\n1\nSHI371053\nShimano 105 BR-R7000 Brake Caliper - rear\n38.98 EUR\n\n\n1\nSSP527134\nShimano Road Shift Cable Set - Stainless Steel\n9.74 EUR\n\n\n1\nSSP526727\nShimano Brake Cable Set Road Stainless Steel\n11.69 EUR\n\n\n1\nSRA131571\nSRAM PowerGlide Chainring 130mm - 39 teeth\n16.56 EUR\n\n\n1\nSHI196481\nShimano CS-HG400-9 Cassette 9-speed, Ratio/teeth: 11-25\n24.36 EUR\n\n\n1\nKMC107578\nKMC X9 Chain - 9-speed - silver/grey\n14.61 EUR\n\n\n1\nSHI111662\nShimano Deore RD-M592-SGS Shadow Rear\n38.98 EUR"
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html#receiving-parcel",
    "href": "posts/2023-04-29_ventoux-3/index.html#receiving-parcel",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "Receiving parcel",
    "text": "Receiving parcel"
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html#transmission",
    "href": "posts/2023-04-29_ventoux-3/index.html#transmission",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "Transmission",
    "text": "Transmission\nLet’s change everything in the transmission to start on fresh good basis.\n\nChainring\nSince SRAM and Shimano are compatible, I give a try to SRAM for the chainring. Looks light and neat.\n\n\n\nNew SRAM chainring\n\n\n\n\nRear derailleur\nDerailleur, I didn’t know what to get, I went for the Deore as I did 8,000 km with it on another bike and it stills work perfectly. The length of the derailleur, I picked a bit at random. Worth noting I am using a bottom-tube lever without indexing so I can pick a MTB derailleur without problem. Switching gear is done with some feeling, without precise intervals.\n\n\n\nSora/Deore derailleur\n\n\n\n\nCassette and chain\nChain was a KMC, recommended by the usual Danish friend[^1].\n\n\n\nCassette, old and new"
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html#rear-brake",
    "href": "posts/2023-04-29_ventoux-3/index.html#rear-brake",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "Rear Brake",
    "text": "Rear Brake\nThe front brake will come later, installing the 105 was almost identical as the Sora, except the obvious quality differences as seen on the picture. Manipulating the spring and its tension also shows how cheap the Sora is.\n\n\n\nSora/105 caliper\n\n\n\nTires\nListening to some advice online, I tried the Continental 4 seasons. Very disappointing product that last actually 8 seasons. Expensive, I got multiple punctures, not great stuff. Pros were:\n\nEasy to un/mount (luckily with the numerous punctures)\nSafe under rain condition\n\nNow using GP5000 and the first winter proved them to be safe, efficient and I got zero puncture."
  },
  {
    "objectID": "posts/2023-04-29_ventoux-3/index.html#final-result-for-now",
    "href": "posts/2023-04-29_ventoux-3/index.html#final-result-for-now",
    "title": " Maintenance on the Ventoux, part 3",
    "section": "Final result (for now)",
    "text": "Final result (for now)\n\n[^1]. Of note, the KMC chain did a great job. Lasted one year but a rough winter season. Got worn up to 60%."
  },
  {
    "objectID": "posts/2020-07-14_ventoux-2/index.html",
    "href": "posts/2020-07-14_ventoux-2/index.html",
    "title": " Rebuilding the Ventoux, part 2",
    "section": "",
    "text": "Post series\n\n\n\nThis post belongs to a series about the Ventoux:\n\nMaintenance 1 on the 2020-04-27\nMaintenance 2 this post\nMaintenance 3 on the 2023-04-29\nMaintenance 4 on the 2023-09-17"
  },
  {
    "objectID": "posts/2020-07-14_ventoux-2/index.html#the-bottom-bracket",
    "href": "posts/2020-07-14_ventoux-2/index.html#the-bottom-bracket",
    "title": " Rebuilding the Ventoux, part 2",
    "section": "The bottom bracket",
    "text": "The bottom bracket\nAfter riding the bike daily a few months, it started to present some problems. Most annoying and concerning: the bottom bracket. This piece is at the bottom of the frame, exposed to water splash and contains the axle attached to the pedals.\n\n\n\nCreator: Chris catchpole\n\n\nThe role of bottom brackets is to transmit the pedaling force to the chainring: quite important!\nIt was wobbling and I knew nothing about replacing it, nor just removing it (which turned out to be the critical problem!). It was obvious it was rusty because of its age and the location. Another issue is to find the right model. A bottom bracket is less than 20 euros but which one is compatible 30 years later with mine? My trustful Danish friend advised me wisely to look into the Sheldon Brown bottom bracket database.\n\n\nSheldon Brown looks like a wonderful man whose tireless collection of information was so valuable that beyond his death, friends continue to maintain it online. Kudos to him and them.\nDimensions from France, Italy, UK are all reported and I managed to find out that the Shimano BB-UN55 should fit (and it did).\n\nSecond Command to bike24.de (2020-07-14)\n\n\n\n\n\n\n\nItem\nQuantity\nPrice\n\n\n\n\nPTO147795 Park Tool CT-3.2 Chain Tool\n1\n34.40\n\n\nPTO111383 Park Tool BBT-22 Bottom Bracket Remover\n1\n19.65\n\n\nSHI123592 Shimano BB-UN55 Bottom Bracket JIS Square\n1\n13.75"
  },
  {
    "objectID": "posts/2020-07-14_ventoux-2/index.html#chainring-and-derailleur",
    "href": "posts/2020-07-14_ventoux-2/index.html#chainring-and-derailleur",
    "title": " Rebuilding the Ventoux, part 2",
    "section": "Chainring and derailleur",
    "text": "Chainring and derailleur\nSo far I had cleaned up the original chainring and derailleur Exage models.\n\n\n\nExage 1991 items\n\n\nBut the large chainring with its enormous 52T was too big for a commute bike. Derailleur was working ok, but I was also curious to install one, even if I had no experience. No derailleur hanger to be seen, it was directly screwed in the frame. Did a try with a Sora basic derailleur purchase."
  },
  {
    "objectID": "posts/2020-07-14_ventoux-2/index.html#handle-bar-and-tape",
    "href": "posts/2020-07-14_ventoux-2/index.html#handle-bar-and-tape",
    "title": " Rebuilding the Ventoux, part 2",
    "section": "Handle bar and tape",
    "text": "Handle bar and tape\nSince I need to order those transmission items, I took the opportunity to also changed the handle bar. I was already happy with a gravel bike which handle bar was fairly large (compare to road bike). The price was reasonable and it was worth also learning about the stem, headset config, dismantle and remounting all this.\n\nThird command to bike24.de (2020-08-01)\n\n\n\n\n\n\n\nItem\nQuantity\nPrice\n\n\n\n\nXLC ST-M15 Comp 31.8 Stem Base Angel - Length: 35° - 60 mm\n1 14,74 EUR\n\n\n\nXLC ST-L03 Ahead-Adapter for Threaded Forks\n1\n13,75 EUR\n\n\nControl Tech One Handlebar Width: 44cm\n1\n24,57 EUR\n\n\nDeda Handlebar Tape Color: Gun Barrel Grey\n1\n7,86 EUR\n\n\nDeda Handlebar Tape Color: Night Black\n1\n7,86 EUR\n\n\nFSA Spacer Polycarbonate 1 1/8” (1 pcs) Color: red\n1\n0,88 EUR\n\n\nShimano CS-HG400-9 Cassette 9-speed Ratio/teeth: 11-32\n1\n18,67 EUR\n\n\nShimano Sora Chainring for FC-3503 - 3x9-speed - black 39 teeth\n1\n11,79 EUR\n\n\nGebhardt Chainring Bolt Niro Length: 5,4 mm\n5\n14,50 EUR\n\n\nShimano Sora RD-R3000-GS Rear Derailleur 3x9 medium\n1\n22,60 EUR\n\n\n\n\nInstalling a new transmission\nI postponed the bottom bracket replacement as it was really stuck. The handle bar though was easy enough, placing the tape took me a long time because I was carefully all steps of the necessary Calvin Jones YouTube videos. The chainring I wanted only one, it is easier to maintain and it makes not much sense for a commute bike to have more. I chose a number of teeth in between my 2 current ones. I picked arbitrary 39 teeth, was a good pick. The derailleur, I knew the frame can handle cassettes of 7 speeds. But the lowest I found online was 9! I did a try and thanks to the magic of steel, I could enlarge the frame a bit and it fits. Derailleur went screwed on the frame as the original one and looked ok. The main issue was the number of links to keep on the 9S chain. I did a bit random, followed Calvin Jones advice but was not so great.\nThe derailleur was too much under tension I now observe.\n\n\nNote about the front derailleur\nConcerning the front derailleur, I removed everything. See here a close-up:\n\nHowever, I had a big crash one day, chain got suddenly stuck and I felt badly. I never understood what happened. A Russian friend advice me to install back the chain guide from the front derailleur. So I did and 4 years later this issue never happened again. I keep the front derailleur installed but removed the cable and even the lever."
  },
  {
    "objectID": "posts/2020-07-14_ventoux-2/index.html#got-the-bottom-bracket-out",
    "href": "posts/2020-07-14_ventoux-2/index.html#got-the-bottom-bracket-out",
    "title": " Rebuilding the Ventoux, part 2",
    "section": "Got the bottom bracket out!",
    "text": "Got the bottom bracket out!\nRemoving the pedals is easy but then it looks like this:\n\n\n\nBB closeup\n\n\nSee the amount of rust and how little margin there is for a bottom bracket removal tool. It never worked, despite using WD40 and forcing a lot. A better tool was needed.\n\nThe homemade BB tool\nMy step dad was working in a steel foundry. I described the problem and Sheldon Brown (yes again this amazing man) adviced a third option to remove rusty BB fro the inside. My step dad then welded a long steel bar to a heating tube with a tight bolt. It was amazing and thanks to him, I got it out!\n\n\nSetup\nOnce in place, it looked like this. The non transmission part, I attached a wrench to block it on the frame.\n\nThen I followed my step-dad advice, don’t push like mad but hammer it, the shocks will make it. He was right.\nThe BB cup, moving in motion\n\n\n\nOutcome\nCheck out this damned piece of rust !\n\nThe new bottom bracket once in place with the tool to screw it:\n\nWhat a journey, that was a massive amount of work and stress but was worth it!"
  },
  {
    "objectID": "posts/2018-03-26_latex-moderncv/index.html#install-missing-packages",
    "href": "posts/2018-03-26_latex-moderncv/index.html#install-missing-packages",
    "title": "LaTex modern CV",
    "section": "install missing packages",
    "text": "install missing packages\ntlmgr install moderncv\ntlmgr install xcolor\ntlmgr install colortbl\ntlmgr install fancyhdr\ntlmgr install microtype\ntlmgr install pgf # to install tikz\ntlmgr install textgreek\ntlmgr install fontawesome\ntlmgr install lastpage\ntlmgr install marvosym \ntlmgr install greek-fontenc\ntlmgr install babel-greek\ntlmgr update --self --all\ntlmgr path add"
  },
  {
    "objectID": "posts/2018-03-26_latex-moderncv/index.html#issue-with-fontawesome",
    "href": "posts/2018-03-26_latex-moderncv/index.html#issue-with-fontawesome",
    "title": "LaTex modern CV",
    "section": "issue with fontawesome",
    "text": "issue with fontawesome\n\nreload / install fonts fmtutil-sys --all\n\nside effect, the Roboto condensed for Robert Rudis, ggplot2 theme is now working nicely!"
  },
  {
    "objectID": "posts/2018-03-26_latex-moderncv/index.html#solve-issue-of-nfss-corrupted",
    "href": "posts/2018-03-26_latex-moderncv/index.html#solve-issue-of-nfss-corrupted",
    "title": "LaTex modern CV",
    "section": "solve issue of NFSS corrupted",
    "text": "solve issue of NFSS corrupted\nfollowing this answer:\nthe error was completed with For encoding scheme LGR the defaults cmr/m/n do not form a valid font shape\ntlmgr install cbfonts\nallows to get the Greek letters working. Why the greek-fontenc and babel-greek was not sufficient? I don’t know"
  },
  {
    "objectID": "posts/2018-03-26_latex-moderncv/index.html#workflow",
    "href": "posts/2018-03-26_latex-moderncv/index.html#workflow",
    "title": "LaTex modern CV",
    "section": "Workflow",
    "text": "Workflow\n\nbibliography from pubmed to .bib using http://www.bioinformatics.org/texmed/ as biblio.bib\nfix gamma character {I}{F}{N}γ responses to {I}{F}{N}{$\\gamma$} responses\nfirst compilation, generate biblio.aux\nin plainyr_rev.bst, specify \"Ginolhac, A\" so my name gets underlined and bolded\nrun bibtex biblio to generate biblio.bbl\nsecond compilation to generate the pdf"
  },
  {
    "objectID": "posts/2015-01-25_winter-is-coming/index.html",
    "href": "posts/2015-01-25_winter-is-coming/index.html",
    "title": "winter is coming",
    "section": "",
    "text": "Snow during winter time. How unexpected this could be?\n\n\n\nVon backhaus in white"
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html",
    "href": "posts/2020-04-27_ventoux-1/index.html",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "",
    "text": "Post series\n\n\n\nThis post belongs to a series about the Ventoux:\n\nMaintenance 1 this post\nMaintenance 2 on the 2020-07-14\nMaintenance 3 on the 2023-04-29\nMaintenance 4 on the 2023-09-17\nMany people started a project during the lock-downs due to covid-19 pandemic. In Luxembourg, we got only one and it triggered my long due bike project. I had two goals, 1) learn basic bike mechanics 2) restore my teenage bike: a Peugeot Ventoux. Learning by doing is, for me, the best way. Yes there are tons of youtube videos about everything but without the real thing to do I have no will to just watch them."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#unmounting-the-bike",
    "href": "posts/2020-04-27_ventoux-1/index.html#unmounting-the-bike",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Unmounting the bike",
    "text": "Unmounting the bike\nI never unmounted a crank set, a cassette nor even a chain. For 20 years I just put air in tubes and changed them when got a puncture. Hardly did a maintenance in bike shops neither. Thinking about it, that was not smart. I might have changed brake pads and oil chain but it sounded like too much work. I believed in eternal which didn’t go wrong while I though this way but really could have been."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#ordering-new-wheels",
    "href": "posts/2020-04-27_ventoux-1/index.html#ordering-new-wheels",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Ordering new wheels",
    "text": "Ordering new wheels\nFor the wheels, they looked crooked, worn and spokes fragile. So I decided to buy a new set. It would solved the freewheel issue. The set costs €100 which sounded just fair.\n\n\n\nNew pair of wheels with the old rear one below\n\n\nAlso add tires and tubes, cheap ones (turned out not great)."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#brakes-and-tools",
    "href": "posts/2020-04-27_ventoux-1/index.html#brakes-and-tools",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Brakes and tools",
    "text": "Brakes and tools\nBike tools are cheap and last a lifetime. Should have invested in that way earlier. Chain whip is easy to use, same for a chain tool. For the crank removal (and bottom bracket), the main issue is not the price, rather the number of standards.\nHere is what I purchased from Probikeshop on the 2020-05-13\n\n\n\n\n\n\n\n\nItem\nQuantity\nPrice (€)\n\n\n\n\nTwo wheels SHIMANO R501 - 10S(56233) Freewheel SHIMANO\n1\n102.99\n\n\nChain 9S SHIMANO HG53 DEORE / SORA - 116 links\n1\n12.49\n\n\nSHI221668 Shimano BL-R400 Brake Lever - black\n1\n28.50\n\n\nShimano PARK TOOL FR-1.3(92114) freewheel\n1\n10.49\n\n\nChain whip BBB TURNTABLE 10S BTL-11(61856)\n1\n14.49\n\n\nTires MICHELIN LITHION 2 700x25c\n1\n30.98\n\n\nAir Tube MICHELIN A1 AIRSTOP BUTYL 700x18/25c\n2\n7.98\n\n\nTwo calipers SHIMANO SORA 3000\n2\n40.90\n\n\nCable brake Kit and hose SHIMANO PTFE\n1\n30.90\n\n\nBar tape CLASSIC COMFORT\n1\n7.49\n\n\n\nI ordered new brake calipers as the current Shimano Exage1 look wasted (see details in the margin).\n\n\n\n\n\nFront brake detail"
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#mounting-the-cassette",
    "href": "posts/2020-04-27_ventoux-1/index.html#mounting-the-cassette",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Mounting the cassette",
    "text": "Mounting the cassette\nInstead of buying a new cassette and chain, I only got a new chain and cleaned the cassette. Was a lot of work and realized too late, it was totally worn out and we soon damage the brand-new chain. But, learning is long process.\n\n\n\nNew rear wheel with a worn but clean cassette"
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#cleaning-up",
    "href": "posts/2020-04-27_ventoux-1/index.html#cleaning-up",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Cleaning up",
    "text": "Cleaning up\nAfter much cleaning, mounting the wheels, removing the brakes and tape bar, looks quite nice already with new shiny wheels. The saddle is a selle royal from my old Von Backhaus, 2012 Copenhagen."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#new-chain",
    "href": "posts/2020-04-27_ventoux-1/index.html#new-chain",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "New chain",
    "text": "New chain\nNever did this, turned out to be not so complex (I now used a better chain tool). I aligned the old and new chain to have the same number of links:"
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#new-brakes-and-housing-cables",
    "href": "posts/2020-04-27_ventoux-1/index.html#new-brakes-and-housing-cables",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "New brakes and housing cables",
    "text": "New brakes and housing cables\nAnother step I experienced for the first time. New housing and cables for both front and rear brakes along the new brake commands on the handle bar. Was easy but took time because I carefully think twice before cutting anything. I added a bit of oil on the cable before putting them inside hoses.\n\n\n\nSetting up the front Sora brake"
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#first-ride",
    "href": "posts/2020-04-27_ventoux-1/index.html#first-ride",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "First ride!",
    "text": "First ride!\nWith a transmission and brakes, riding the Ventoux was feasible. Quite emotional to ride it again after so long. It was smooth and nice.\n\nPedals are from the Diverge bike I got in May 2020 and I unfortunately did not document the setup of the derailleur cable. This was quite ok, I kept the existing gear lever on the down tube, not the indexed setting but the free one. The front derailleur got a new lever (we will see that I changed my mind later on), the small chain ring (weird ovoid shape!) was too small and the big one (52T) way too big."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#tape-bar-and-fenders",
    "href": "posts/2020-04-27_ventoux-1/index.html#tape-bar-and-fenders",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Tape bar and fenders",
    "text": "Tape bar and fenders\nFinally, a new tape bar (white in homage of the original one) that took me a lot of time to install as I wanted to do it correctly (as usual see the great video from Calvin Jones about this).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe fenders are the SKS long blades and still perfectly fine more than 3 years later.\nThe side kickstand was actually too heavy and honestly quite ugly, it quickly went over to another bike.\nOf note, the wheel orange reflectors are still there with a red reflector on the saddle. Rear lamp a generous gift from my Danish friend Christian and the U-lock holder did not hold more than 2 weeks.\nBike was finished on the 16th of June, so 6 weeks after starting this project. Much more to come after this first step."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#acknowledgements",
    "href": "posts/2020-04-27_ventoux-1/index.html#acknowledgements",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nDescription image from bike wheels transformed by the fun package paintr by CJ Holmes."
  },
  {
    "objectID": "posts/2020-04-27_ventoux-1/index.html#footnotes",
    "href": "posts/2020-04-27_ventoux-1/index.html#footnotes",
    "title": " Rebuilding the Ventoux, part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuoting Shimano “EXAGE, coined from “excellent,” “exceed” and “age”. Most certainly it exceeding its age!↩︎"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html",
    "href": "posts/2023-02-05_targets-demos/index.html",
    "title": "{targets} demos",
    "section": "",
    "text": "Targets is an  package that is\n\nFunction-oriented Make-like declarative workflows for \n\nMain author: William Landau. See the targets manual for an extensive documentation.\nThe goal is to create so-called targets that are significant steps that linked between one another. Those links create the dependencies, and once one target run successfully and its upstream dependencies are up-to-date, they are no reason to run it again. Time/Computing intensive steps are then cached in the store.\nInvalidation of a target arises when:\n\nUpstream targets invalidate (or input files checksum for special format = \"file\")\nCode of the targets changed\nPackage used was updated"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#a-workflow-manager-for",
    "href": "posts/2023-02-05_targets-demos/index.html#a-workflow-manager-for",
    "title": "{targets} demos",
    "section": "",
    "text": "Targets is an  package that is\n\nFunction-oriented Make-like declarative workflows for \n\nMain author: William Landau. See the targets manual for an extensive documentation.\nThe goal is to create so-called targets that are significant steps that linked between one another. Those links create the dependencies, and once one target run successfully and its upstream dependencies are up-to-date, they are no reason to run it again. Time/Computing intensive steps are then cached in the store.\nInvalidation of a target arises when:\n\nUpstream targets invalidate (or input files checksum for special format = \"file\")\nCode of the targets changed\nPackage used was updated"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#example-dataset-datasaurus",
    "href": "posts/2023-02-05_targets-demos/index.html#example-dataset-datasaurus",
    "title": "{targets} demos",
    "section": "Example dataset: datasauRus ",
    "text": "Example dataset: datasauRus \nThe great package datasauRus offers a fake table which consists of 13 dataset (each of 142 observations) with 2 values x and y:\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# … with 1,836 more rows\nFor each the 4 demos, we use different versions of the same data:\n\nOne tabulated-separated-value (tsv) file of 1847 lines (1846 observations + 1 header)\nSame as before but with plotting functions in a separate  script\nOne folder that contains 13 tsv of 143 lines\nThree folders of 2, 4 and 7 tsv of 143 lines each"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#multiple-projects-in-one-folder",
    "href": "posts/2023-02-05_targets-demos/index.html#multiple-projects-in-one-folder",
    "title": "{targets} demos",
    "section": "Multiple projects in one folder",
    "text": "Multiple projects in one folder\nThis is supported by targets and described in the manual: projects. A config YAML file, _targets.yaml describe the 4 different projects, specifying the name of both:\n\nthe targets  script (actual definition of target)\nthe store folder name (where objects are cached and described)\n\nAdditional options or inheritance can be specified too.\nContent:\nds_linear:\n  store: _ds_1\n  script: _targets_ds_1.R\nds_fun_linear:\n  store: _ds_fun1\n  script: _targets_ds_fun1.R\nds_dynamic:\n  store: _ds_2\n  script: _targets_ds_2.R\nds_static:\n  store: _ds_3\n  script: _targets_ds_3.R\n  reporter_make: verbose_positives # do not display skipped targets"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#packages-needed",
    "href": "posts/2023-02-05_targets-demos/index.html#packages-needed",
    "title": "{targets} demos",
    "section": "Packages needed",
    "text": "Packages needed\nThose demos are using several packages, you can get the necessary ones by using renv. Once the repo cloned/downloaded:\nrenv::restore()\nto install a local library of the key packages."
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#one-file-linear-pipeline",
    "href": "posts/2023-02-05_targets-demos/index.html#one-file-linear-pipeline",
    "title": "{targets} demos",
    "section": "One file, linear pipeline",
    "text": "One file, linear pipeline\nUsing the original tsv from the package datasauRus itself. targets allows to track the timestamp of an URL.\nHere we track https://raw.githubusercontent.com/jumpingrivers/datasauRus/main/inst/extdata/DatasaurusDozen-Long.tsv.\nSee the complete targets  script in _targets_ds_1.R and displayed dependencies as directed acyclic graph:\n\n\n\nds1\n\n\ntargets encourages using literate programing where a Rmarkdown document higher level comments and code, dependencies are based on the parsing of the tar_read() and tar_load() calls within it. This can be used as smart caching system where help focusing on the analysis report, leaving the computation for the  script _targets.R.\nFor this first example, the corresponding Rmd is ds1.Rmd. It will be rendered by the pipeline (target definition in tar_render()).\nTo run this example:\n# Specify which projet to use\nSys.setenv(TAR_PROJECT = \"ds_linear\")\n# Run what is needed like make in a Makefile\ntargets::tar_make()\nFor the first run, `tar_make()1 should output something like:\n&gt; targets::tar_make()\n• start target ds_file\n• built target ds_file [0.695 seconds]\n• start target ds\n• built target ds [0.176 seconds]\n• start target anim\n• built target anim [48.762 seconds]\n• start target all_facets\n• built target all_facets [0.007 seconds]\n• start target gif\n• built target gif [0.005 seconds]\n• start target report\n• built target report [4.144 seconds]\n• end pipeline [54.083 seconds]\nThe GIF animation takes roughly one minute, so it would be cumbersome to wait this time at each Rmarkdown knitting process. It is a good case for targets, the GIF will be re-run only if needed while you polish the Rmd report.\nSee the output of re-reruning tar_make() again:\n&gt; targets::tar_make()\n✔ skip target ds_file\n✔ skip target ds\n✔ skip target anim\n✔ skip target all_facets\n✔ skip target gif\n✔ skip target report\n✔ skip pipeline [0.27 seconds]\n0.27 seconds versus 54."
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#cleaner-coding-with-sourcing-functions",
    "href": "posts/2023-02-05_targets-demos/index.html#cleaner-coding-with-sourcing-functions",
    "title": "{targets} demos",
    "section": "Cleaner coding with sourcing functions",
    "text": "Cleaner coding with sourcing functions\n_targets_ds_fun1.R is similar as _targets_ds_1.R except that plotting functions were placed in R/plotting.R which is sourced before the targets definition\nTargets definition is then cleaner to read:\nlist(\n  # track if distant file has changed\n  tar_url(ds_file, \"https://raw.githubusercontent.com/jumpingrivers/datasauRus/main/inst/extdata/DatasaurusDozen-Long.tsv\"),\n  tar_target(ds, read_tsv(ds_file, show_col_types = FALSE)),\n  tar_target(all_facets, facet_ds(ds)),\n  # animation is worth caching  ~ 1 min\n  tar_target(anim, anim_ds(ds), \n             packages = c(\"ggplot2\", \"gganimate\", \"gifski\")),\n  tar_file(gif, {\n    anim_save(\"ds.gif\", animation = anim, title_frame = TRUE)\n    # anim_save returns NULL, we need to get the file output path\n    \"ds.gif\"},\n             packages = c(\"gganimate\")),\n  tar_render(report, \"ds1.Rmd\")\n)"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#one-folder-dynamic-branching",
    "href": "posts/2023-02-05_targets-demos/index.html#one-folder-dynamic-branching",
    "title": "{targets} demos",
    "section": "One folder, dynamic branching",
    "text": "One folder, dynamic branching\nOften, input files are more than one. Of course, you don’t want to list them by hand and one want to apply similar treatment to each of them. Moving away from for loops, we embrace functional programming and let targets branching over the list of files and dynamically for it adapts to how many are present.\nThis is called dynamic branching and it contains the magic aggregation (like bind_rows()) when calling the target name.\nOn the filesystem, the folder data contains 13 files:\ndata/\n├── dset_10.tsv\n├── dset_11.tsv\n├── dset_12.tsv\n├── dset_13.tsv\n├── dset_1.tsv\n├── dset_2.tsv\n├── dset_3.tsv\n├── dset_4.tsv\n├── dset_5.tsv\n├── dset_6.tsv\n├── dset_7.tsv\n├── dset_8.tsv\n└── dset_9.tsv\nWhich once tracked by targets are:\n&gt; tar_read(dset) |&gt; \n    enframe()\n# A tibble: 13 × 2\n   name          value           \n   &lt;chr&gt;         &lt;chr&gt;           \n 1 dset_e814d3a7 data/dset_1.tsv \n 2 dset_96886f69 data/dset_10.tsv\n 3 dset_a4c9d9df data/dset_11.tsv\n 4 dset_02e8e253 data/dset_12.tsv\n 5 dset_39f18392 data/dset_13.tsv\n 6 dset_d74af7a4 data/dset_2.tsv \n 7 dset_55280675 data/dset_3.tsv \n 8 dset_80822375 data/dset_4.tsv \n 9 dset_020f0640 data/dset_5.tsv \n10 dset_0577d04d data/dset_6.tsv \n11 dset_66982b15 data/dset_7.tsv \n12 dset_3c9c9095 data/dset_8.tsv \n13 dset_fe11a7b7 data/dset_9.tsv \nFinally, the DAG is:\n\n\n\nds2\n\n\nYou see that all targets appears in blue, so outdated. This is the expected behavior of tar_files(). We don’t know in advance how many (if any) files are present, so the listing is checked all the time and downstream targets are then also outdated.\nHowever, the downstream targets are re-run only if needed\n\nIf input files changed\nIf code for those targets changed.\n\nSee example of re-running tar_make(). dset_files was run again, but no files were different so all the rest is skipped and the whole pipeline took 1.1 second.\n&gt; targets::tar_make()\n• start target dset_files\n• built target dset_files [0.702 seconds]\n✔ skip branch dset_6630d1f3\n✔ skip branch dset_f10c2c43\n✔ skip branch dset_c79e8ff6\n✔ skip branch dset_b1eac8ed\n[...]\n✔ skip branch plots_01ca2c35\n✔ skip branch plots_9fc19e45\n✔ skip branch plots_01f427e3\n✔ skip pattern plots\n✔ skip target report\n• end pipeline [1.172 seconds]\nDynamic branching scales great on the DAG since the number of branches can be reported, no additional items are created. to avoid this, we can switch to tar_files_input() which also automatically groups input files into batches to reduce overhead and increase the efficiency of parallel processing..\nSee the DAG with tar_files_input():\n\n\n\nds2"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#several-folders-dynamic-within-static-branching",
    "href": "posts/2023-02-05_targets-demos/index.html#several-folders-dynamic-within-static-branching",
    "title": "{targets} demos",
    "section": "Several folders, dynamic within static branching",
    "text": "Several folders, dynamic within static branching\nWe created a folder structure as we often have to deal with, 3 sub-folders of data:\ncircles\n├── dset_2.tsv\n└── dset_3.tsv\nlines\n├── dset_11.tsv\n├── dset_12.tsv\n├── dset_13.tsv\n├── dset_6.tsv\n├── dset_7.tsv\n├── dset_8.tsv\n└── dset_9.tsv\nothers\n├── dset_10.tsv\n├── dset_1.tsv\n├── dset_4.tsv\n└── dset_5.tsv\nEspecially with static branching, it is meaningful to check which commands are planned. See the manifest for this example:\n&gt; tar_manifest() |&gt; print(n = Inf)\n# A tibble: 21 × 3\n   name                 command                                                                                                   pattern\n   &lt;chr&gt;                &lt;chr&gt;                                                                                                     &lt;chr&gt;  \n 1 filenames_circles    \"fs::dir_ls(\\\"circles\\\", glob = \\\"*tsv\\\")\"                                                                NA     \n 2 filenames_others     \"fs::dir_ls(\\\"others\\\", glob = \\\"*tsv\\\")\"                                                                 NA     \n 3 filenames_lines      \"fs::dir_ls(\\\"lines\\\", glob = \\\"*tsv\\\")\"                                                                  NA     \n 4 files_circles        \"filenames_circles\"                                                                                       map(fi…\n 5 files_others         \"filenames_others\"                                                                                        map(fi…\n 6 files_lines          \"filenames_lines\"                                                                                         map(fi…\n 7 ds_circles           \"read_tsv(files_circles, show_col_types = FALSE)\"                                                         map(fi…\n 8 ds_others            \"read_tsv(files_others, show_col_types = FALSE)\"                                                          map(fi…\n 9 ds_lines             \"read_tsv(files_lines, show_col_types = FALSE)\"                                                           map(fi…\n10 summary_stat_circles \"summarise(ds_circles, m_x = mean(x), m_y = mean(y))\"                                                     map(ds…\n11 plots_circles        \"ggplot(ds_circles, aes(x, y)) + geom_point()\"                                                            map(ds…\n12 summary_stat_others  \"summarise(ds_others, m_x = mean(x), m_y = mean(y))\"                                                      map(ds…\n13 plots_others         \"ggplot(ds_others, aes(x, y)) + geom_point()\"                                                             map(ds…\n14 plots_lines          \"ggplot(ds_lines, aes(x, y)) + geom_point()\"                                                              map(ds…\n15 summary_stat_lines   \"summarise(ds_lines, m_x = mean(x), m_y = mean(y))\"                                                       map(ds…\n16 patch_plots_circles  \"wrap_plots(plots_circles) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\" NA     \n17 patch_plots_others   \"wrap_plots(plots_others) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\"  NA     \n18 patch_plots_lines    \"wrap_plots(plots_lines) + plot_annotation(title = stringr::str_split_i(tar_name(), \\n     \\\"_\\\", -1))\"   NA     \n19 stat_summaries       \"dplyr::bind_rows(summary_stat_lines = summary_stat_lines, \\n     summary_stat_circles = summary_stat_ci… NA     \n20 plots_agg            \"wrap_plots(list(patch_plots_lines = patch_plots_lines, \\n     patch_plots_circles = patch_plots_circles… NA     \n21 report               \"tarchetypes::tar_render_run(path = \\\"ds3.Rmd\\\", args = list(input = \\\"ds3.Rmd\\\", \\n     knit_root_dir =… NA    \nYou see that we get meaningful names based on the 3 folders listed. Still we get dynamic branching for reading files inside each folder. The same treatment is performed on each 3 input folders but when we want/need to combine the parallel branches for a relevant aggregation, we use tar_combine(). Example of both aggregating tibbles or plots are exemplified as depicted below:\n\n\n\nds3"
  },
  {
    "objectID": "posts/2023-02-05_targets-demos/index.html#demo-repository",
    "href": "posts/2023-02-05_targets-demos/index.html#demo-repository",
    "title": "{targets} demos",
    "section": "Demo repository",
    "text": "Demo repository\nAll the corresponding files are available in the Gitlab repository"
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html",
    "title": "Testing Snakemake template",
    "section": "",
    "text": "Testing software is necessary and I always do it too late. Looking William Landau coding (through watching his {targets} repo) it is clear that the best way to develop is to write as the same time:\n\ncode\nunit tests\ndocumentation\n\nWhen struggling on the first item, it appears difficult to concomitantly write the corresponding tests. Not to mention that documentation could be not even considered in the first place.\nThis works somehow but suddenly, one repo could gain attention and documentation becomes an extended README. Then later, a fresh release is proven to break things, and that’s bad when this is reported by users and not detected by the author.\nLong story short, I wanted my snakemake template for bulk RNA-seq to be tested, at least a short pipeline on fake data to catch obvious mistakes.\nBy the way, Snakemake is a Python framework for managing bioinformatic workflow. That would deserve a post on itself."
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html#rationale",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html#rationale",
    "title": "Testing Snakemake template",
    "section": "",
    "text": "Testing software is necessary and I always do it too late. Looking William Landau coding (through watching his {targets} repo) it is clear that the best way to develop is to write as the same time:\n\ncode\nunit tests\ndocumentation\n\nWhen struggling on the first item, it appears difficult to concomitantly write the corresponding tests. Not to mention that documentation could be not even considered in the first place.\nThis works somehow but suddenly, one repo could gain attention and documentation becomes an extended README. Then later, a fresh release is proven to break things, and that’s bad when this is reported by users and not detected by the author.\nLong story short, I wanted my snakemake template for bulk RNA-seq to be tested, at least a short pipeline on fake data to catch obvious mistakes.\nBy the way, Snakemake is a Python framework for managing bioinformatic workflow. That would deserve a post on itself."
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html#inspiration-from-the-experienced-people",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html#inspiration-from-the-experienced-people",
    "title": "Testing Snakemake template",
    "section": "Inspiration from the experienced people",
    "text": "Inspiration from the experienced people\nThe template is derived from this one and they implemented testing. Apart from the great looking green badge, it is what I am after: run the pipeline on small data of the Yeast genome.\n\n\n\n\n\nTest passed!\n\n\nLooking at how it is done in their  GitHub Action file, the relevant part is:\n\n    - name: Test workflow (basic model, no batch_effects)\n      uses: snakemake/snakemake-github-action@v1.22.0\n      with:\n        directory: .test\n        snakefile: workflow/Snakefile\n        args: \"--configfile .test/config_basic/config.yaml --use-conda --show-failed-logs --cores 2 --conda-cleanup-pkgs cache\"\n\nThey are doing more testing but with the same structure. The annoying part to me is the:\nuses: snakemake/snakemake-github-action@v1.22.0\nBecause it abstracts the real pipeline, it works but it has some magic inside and even going through the repo is not giving details.\nThe same happens for  with {renv} cache / restore, quarto render and publishing on GitHub pages like for this very blog.\nBut, some great things are useful, like the option --show-failed-logs that I didn’t know and that is especially relevant here.\nAnyway, the Snakemake template is on  GitLab , so the CI/CD has to happen there."
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html#cicd-configuration",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html#cicd-configuration",
    "title": "Testing Snakemake template",
    "section": " CI/CD configuration",
    "text": "CI/CD configuration\nAs said, on GiLab, the Continuous Integration/Development has no magic recipes like on . One needs to declare every step.\nThe content of the config .gitlab-ci.yml is:\nservices:\n    - name: docker:dind\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"\"\n\nstages:\n  - test\n\ntesting:\n  stage: test\n  image: snakemake/snakemake:stable\n  cache:\n    key: ${CI_JOB_NAME}\n    paths:\n      - .test/.snakemake/\n  script:\n    - cd .test\n    - snakemake --configfile config_basic/config.yaml \n      --snakefile Snakefile_test -j1 --use-singularity --show-failed-logs\n    - snakemake --configfile config_basic/config.yaml \n      --snakefile Snakefile_test -j1 --report\n    - NB_DEG=`wc -l results/diffexp/KO_vs_WT.diffexp.tsv | cut -f1 -d ' '`\n    - test \"$NB_DEG\" -eq 94 || exit 1\n    - diff results/diffexp/KO_vs_WT.diffexp.tsv expected/KO_vs_WT.diffexp.tsv\n    - diff  &lt;(grep -v seed trimmed_se/A1-1.settings)  &lt;(grep -v seed expected/A1-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/B1-1.settings)  &lt;(grep -v seed expected/B1-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/A2-1.settings)  &lt;(grep -v seed expected/A2-1.settings)\n    - diff  &lt;(grep -v seed trimmed_pe/B2-1.settings)  &lt;(grep -v seed expected/B2-1.settings)\n\n  tags:\n    - shared-cache\n  when: always\nI abstracted the specificity of our  Gitlab, suppressing this line:\n      command: [\"--registry-mirror\", \"https://docker-registry.lcsb.uni.lu\"]\nLet’s break it down\n\nDocker in Docker (dind) service\nLines 1 to 6. Nothing special, copied from our template\n\n\nStages\nLines 8-9. Here only one, but could have a second one for testing the complex design like on \n\n\nMain part\n\nimage copied over from the Github Action, useful that have a  image done.\ncache (lines 14-17). I am not using conda but singularity, the  Docker for HPC. Thus it is useful to cache the image to avoid downloading it every single time. Snakemake caches it in .snakemake/singularity/hashsum.sif checking if a new one if available. Lines 32-33 indicates that the cache is shared, so any  runner is able to access the cached data.\nscript comes in 2 steps:\n\nRunning snakemake: lines 20-23\nTesting the results obtained: lines 24-30 Testing the number of lines of the differential expression TSV, redundant with the exact expected file with a diff. And testing also the expected trimming stats. Random seed are different so need to be excluded.\n\nwhen indicates for which action the CI is triggered. Could be only on pull request, release, manual etc. Here we triggered it for any commit push to the repo."
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html#demo",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html#demo",
    "title": "Testing Snakemake template",
    "section": "Demo",
    "text": "Demo\nA testing run lasts for roughly 4 minutes which is reasonable.\n\n\n\ndemo running test\n\n\n\n\n\ndemo testing results\n\n\nWithout the cache, the download of the singularity brings the time to 6 minutes 26 seconds. See the additional pulling notification (line 4):\n[...]\n$ snakemake --configfile config_basic/config.yaml --snakefile Snakefile_test -j1 --use-singularity --show-failed-logs\nBuilding DAG of jobs...\nPulling singularity image docker://ginolhac/snake-rna-seq:0.7.\n[...]"
  },
  {
    "objectID": "posts/2023-09-07_snakemake-test-ci/index.html#gitlab-repository",
    "href": "posts/2023-09-07_snakemake-test-ci/index.html#gitlab-repository",
    "title": "Testing Snakemake template",
    "section": " Gitlab repository",
    "text": "Gitlab repository\nThis repo is public and available here"
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html",
    "href": "posts/2023-09-14_renv-python-ci/index.html",
    "title": "Deploying  and  packages",
    "section": "",
    "text": "Synchronize both  and  packages between users and  runners that actually render a teaching website using Quarto."
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html#aim",
    "href": "posts/2023-09-14_renv-python-ci/index.html#aim",
    "title": "Deploying  and  packages",
    "section": "",
    "text": "Synchronize both  and  packages between users and  runners that actually render a teaching website using Quarto."
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html#introduction",
    "href": "posts/2023-09-14_renv-python-ci/index.html#introduction",
    "title": "Deploying  and  packages",
    "section": "Introduction",
    "text": "Introduction\n{renv} is a package for managing  packages at the project scale.\n\n\n\nrenv overview (Kevin Ushey)\n\n\n\n Python packages\nThis article is succinct but describes that {renv} can manage Python packages."
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html#setup",
    "href": "posts/2023-09-14_renv-python-ci/index.html#setup",
    "title": "Deploying  and  packages",
    "section": "Setup",
    "text": "Setup\n\nInitialization\nFollowing instructions from the renv website. Using the virtual environment solution and not conda as the first one is included in python3.\nYou need to have installed:\n\nPython: python3.11-venv\nR:\n\n{reticulate}\n{renv}\n\n\n&gt; renv::use_python()\nPlease select a version of Python to use with this project: \n\n1: /usr/bin/python3\n\nSelection: 1\n- Selected \"/usr/bin/python3\" [Python 3.11.12].\n- Creating virtual environment 'renv-python-3.11' ... Done!\n- Updating Python packages ... Done!\n- Lockfile written to \"renv.lock\".\n- Activated Python 3.11.12 [virtualenv; ./renv/python/virtualenvs/renv-python-3.11]\n\n\nInstalling Python packages\nFor example, installing pandas\n&gt; reticulate::py_install(\"pandas\")\nUsing virtual environment '/xxxx/xxxx/renv/python/virtualenvs/renv-python-3.11' ...\n+ /xxxx/xxxx/renv/python/virtualenvs/renv-python-3.10/bin/python -m pip install --upgrade --no-user pandas\nCollecting pandas\n  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/fb/4f/4a4372b2e24439f559b73318683486831d75e59544ae02bf8dec8dd6f48b/pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting numpy&gt;=1.22.4 (from pandas)\n  Obtaining dependency information for numpy&gt;=1.22.4 from https://files.pythonhosted.org/packages/9b/5a/f265a1ba3641d16b5480a217a6aed08cceef09cd173b568cd5351053472a/numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.5/58.5 kB 4.0 MB/s eta 0:00:00\nCollecting python-dateutil&gt;=2.8.2 (from pandas)\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 10.5 MB/s eta 0:00:00\nCollecting pytz&gt;=2020.1 (from pandas)\n  Obtaining dependency information for pytz&gt;=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata&gt;=2022.1 (from pandas)\n  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 10.6 MB/s eta 0:00:00\nCollecting six&gt;=1.5 (from python-dateutil&gt;=2.8.2-&gt;pandas)\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 11.0 MB/s eta 0:00:00\nDownloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 10.9 MB/s eta 0:00:00\nDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.5/502.5 kB 10.5 MB/s eta 0:00:00\nInstalling collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\nSuccessfully installed numpy-1.26.0 pandas-2.1.0 python-dateutil-2.8.2 pytz-2023.3.post1 six-1.16.0 tzdata-2023.3\n\n\nSnapshot\nNow, register with {renv} what we have just installed.\nThe main function renv::snapshot() will regiter both  and  packages.\n&gt; renv::snapshot()\nThe following package(s) will be updated in the lockfile:\n\n# CRAN -----------------------------------------------------------------------\n- renv   [* -&gt; 1.0.2]\n\nDo you want to proceed? [Y/n]: \n- Lockfile written to \"/xxxx/xxxx/renv.lock\".\nThe following will be written to requirements.txt:\n- numpy==1.26.0\n- pandas==2.1.0\n- python-dateutil==2.8.2\n- pytz==2023.3.post1\n- six==1.16.0\n- tzdata==2023.3\n\nDo you want to proceed? [Y/n]: \n- Wrote Python packages to \"/xxxx/xxxx/requirements.txt\".\nChecking on the written files, requirements.txt is a rather simple text file:\n\n\nrequirements.txt\n\nnumpy==1.26.0\npandas==2.1.0\npython-dateutil==2.8.2\npytz==2023.3.post1\nsix==1.16.0\ntzdata==2023.3\n\nrenv.lock, the  relevant part:\n\n\nrenv.lock\n\n[...]\n  \"Python\": {\n    \"Version\": \"3.11.12\",\n    \"Type\": \"virtualenv\",\n    \"Name\": \"./renv/python/virtualenvs/renv-python-3.11\"\n  },\n[...]\n\n\n\nQuick test\n&gt; pd &lt;- reticulate::import(\"pandas\")\n&gt; pd$Series(list(1, 3, 6, 8))\n0 1 2 3 \n1 3 6 8\n\n\n.Rprofile\nIn theory, the sourced .Rprofile should only contains one necessary line generated by renv:\nsource(\"renv/activate.R\")\nHowever, if you encountered issue with  not finding the  environment, it could be useful to have those env variables defined:\nSys.setenv(RETICULATE_MINICONDA_PATH = \"renv/python/virtualenvs/renv-python-3.11/\")\nreticulate::use_python(\"renv/python/virtualenvs/renv-python-3.11/bin/python\")"
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html#ci-config",
    "href": "posts/2023-09-14_renv-python-ci/index.html#ci-config",
    "title": "Deploying  and  packages",
    "section": "CI config",
    "text": "CI config\nNow the Continuous Integration (CI) triggered by every commit needs to:\n\nInstall the missing packages, especially the  ones\nCache them to avoid re-installing at each commit\nRun the pipeline of building the R code (using {targets} (see previous post)\nRender the website using quarto\n\n\nCaching\nThe config is:\n  cache:\n    key: $CI_JOB_NAME\n    paths:\n      - _targets/\n      - _site/\n      - lectures_rendered/\n      - renv/python/virtualenvs/\n      - ${RENV_PATHS_CACHE}\n      - ${RENV_PATHS_LIBRARY}\n\n_targets/ folder that contains the metadata and objects done by {targets}, necessary otherwise everything is redone each time.\n_site output folder of the website, default name by quarto.\nrenv/python/virtualenvs/ cache the python install + packages\n\nRest is classic and is described here\n\n\nBuilding\n  before_script:\n    - Rscript -e \"renv::restore()\"\n  script:\n    - ./run.R\n    - source renv/python/virtualenvs/renv-python-3.11/bin/activate\n    - quarto render\n  tags:\n    - shared-cache\n  when: always\n\n./run.R runs targets::tar_make()\nThe key part is this source [venv] line 5. It allows the next line with the quarto call to be aware of where is the  install / packages 1.\nquarto render then build the website\n\n\n\nResults\nIn this screenshot, you see the first time the CI sees the need to install packages for both languages\n\n\n\nInstalling (and caching) packages in CI\n\n\nAnd now, after a successful caching, no install is required:\n\n\n\nBuilding website in CI\n\n\n\nline 40-59 is the {targets} pipeline\nline 61-63 the beginning of the quarto rendering process that involved  Python code."
  },
  {
    "objectID": "posts/2023-09-14_renv-python-ci/index.html#footnotes",
    "href": "posts/2023-09-14_renv-python-ci/index.html#footnotes",
    "title": "Deploying  and  packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe .Rprofile works only for the  part and here we have pure  code in a Quarto document↩︎"
  },
  {
    "objectID": "posts/2023-10-09_map-games/index.html",
    "href": "posts/2023-10-09_map-games/index.html",
    "title": " Maps Games",
    "section": "",
    "text": "Wandrer Earth is a website that fetch your activities (either on bike or on foot) and compare the GPS tracks with your previous ones.\nIt retains the new ridden kilometers, and a scoring system does some ranking. I am not so much into this chart but it strongly pushes you to explore\n\nNew paths\nPlan in advance to ride new streets in towns / villages\nBe exhaustive for some neighborhood / towns and why not countries!1\n\n\n\n\nOutput of a ride\n\n\nIn red the parts that I already ridden, in blue the new ones.\nI absolutely love this game and sometimes go around some blocks just to collect new streets. Of note, I export the Garmin map and add it as a layer to my GPS. Since I ride mostly with the map on, I can see red tracks displayed and can take opportunity to change my route to fetch some new meters.\n\n\nFor planning tours, I am using komoot. It is not part of the  games but fantastic for the task. Craig Durkin, the man behind Wandrer Earth has developed an extension that allows to display the ridden or unridden tracks in Komoot.\n\n\n\nOne cons that some finds is that it works only with strava. Actually all 3 websites presented here are fetching data from Strava."
  },
  {
    "objectID": "posts/2023-10-09_map-games/index.html#wandrer-earth",
    "href": "posts/2023-10-09_map-games/index.html#wandrer-earth",
    "title": " Maps Games",
    "section": "",
    "text": "Wandrer Earth is a website that fetch your activities (either on bike or on foot) and compare the GPS tracks with your previous ones.\nIt retains the new ridden kilometers, and a scoring system does some ranking. I am not so much into this chart but it strongly pushes you to explore\n\nNew paths\nPlan in advance to ride new streets in towns / villages\nBe exhaustive for some neighborhood / towns and why not countries!1\n\n\n\n\nOutput of a ride\n\n\nIn red the parts that I already ridden, in blue the new ones.\nI absolutely love this game and sometimes go around some blocks just to collect new streets. Of note, I export the Garmin map and add it as a layer to my GPS. Since I ride mostly with the map on, I can see red tracks displayed and can take opportunity to change my route to fetch some new meters.\n\n\nFor planning tours, I am using komoot. It is not part of the  games but fantastic for the task. Craig Durkin, the man behind Wandrer Earth has developed an extension that allows to display the ridden or unridden tracks in Komoot.\n\n\n\nOne cons that some finds is that it works only with strava. Actually all 3 websites presented here are fetching data from Strava."
  },
  {
    "objectID": "posts/2023-10-09_map-games/index.html#squadrats",
    "href": "posts/2023-10-09_map-games/index.html#squadrats",
    "title": " Maps Games",
    "section": "Squadrats",
    "text": "Squadrats\nSquadrats is using your GPS tracks to create, as you guessed, squares. Squares of different sizes and there are different ranking, like extending the bigger one (ÜBERSQUADRAT), having a maximum of squares, or little ones (Squadratinhos).\nThis pushes you to go farther from home, hitting just a corner of a squadrat and it’s yours. I only recently started to be interested by this game.\n\n\n\nMy general map as today"
  },
  {
    "objectID": "posts/2023-10-09_map-games/index.html#alternative-veloviewer",
    "href": "posts/2023-10-09_map-games/index.html#alternative-veloviewer",
    "title": " Maps Games",
    "section": "Alternative: VeloViewer",
    "text": "Alternative: VeloViewer\nVeloViewer offers a LOT of graphs. Actually too much for me. Tried for one year and did not renew my subscription.\n\n\n\nDashboard from the demo website"
  },
  {
    "objectID": "posts/2023-10-09_map-games/index.html#footnotes",
    "href": "posts/2023-10-09_map-games/index.html#footnotes",
    "title": " Maps Games",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTwo friends of mine have ridden &gt; 50% of all bikeable trails / roads of Luxembourg. Pretty impressive↩︎"
  },
  {
    "objectID": "posts/2024-01-09_r2u_ci/index.html",
    "href": "posts/2024-01-09_r2u_ci/index.html",
    "title": "r2u for faster continuous integration",
    "section": "",
    "text": "Rocker on DockerHub"
  },
  {
    "objectID": "posts/2024-01-09_r2u_ci/index.html#rationale",
    "href": "posts/2024-01-09_r2u_ci/index.html#rationale",
    "title": "r2u for faster continuous integration",
    "section": "Rationale",
    "text": "Rationale\n{renv} is a great tool to manage  dependencies. Especially it records versions of packages used in a project. This is a great feature for reproducibility, of course only if the renv.lock is git tracked.\nHowever, there are situations where we don’t really care about reproducibility, but we want to speed up the CI process. For example, when we built a teaching website or a Quarto blog.\nIn CI, starting from raw Ubuntu images, a renv::restore() with tidyverse and bioconductor packages can take more than 30 minutes. This is a lot of time wasted for a simple blog post or a teaching website."
  },
  {
    "objectID": "posts/2024-01-09_r2u_ci/index.html#a-failing-solution",
    "href": "posts/2024-01-09_r2u_ci/index.html#a-failing-solution",
    "title": "r2u for faster continuous integration",
    "section": "A failing solution",
    "text": "A failing solution\nOne solution is to use pre-built  packages. The Public Posit Package Manager should offer this feature (see previous post), but the different flavor of  makes it over-complicated and some past versions of packages are not available. Even more problematic is missing system libraries are not solved. Only when you do library(xml2) you realize that libxml2-dev is missing."
  },
  {
    "objectID": "posts/2024-01-09_r2u_ci/index.html#a-working-solution",
    "href": "posts/2024-01-09_r2u_ci/index.html#a-working-solution",
    "title": "r2u for faster continuous integration",
    "section": "A working solution",
    "text": "A working solution\nThis is what the {r2u} project is about: CRAN as Ubuntu Binaries.\nThis project was initiated and is maintained by Dirk Eddelbuettel and hosted on Github.\nRecently, it even got a Docker image on the Rocker project.\n\nHow to use it\nI saw this toot from Dirk on Mastodon:\n\n\nAnd decided to go for a fake DESCRIPTION file.\nI will write down the setup for a blog on Gitlab as an example.\nAll packages could be in the DESCRIPTION file but we could also cache directly in the  image some packages.\n\nDockerfile\nHere I have all kind of sources:\n\nCRAN packages\nBioconductor packages (needs BiocManager first)\nGithub packages\n\nAnd Quarto, fetch the pre-release from .\nFROM rocker/r2u:22.04\nRUN  apt-get update && \\\n  apt-get install -y \\\n  curl netbase zip git \\\n  libxml2-dev libcurl4-openssl-dev libmagick++-dev\n\nRUN install.r tidyverse rmarkdown BiocManager Rcpp V8 htmlwidgets magick \\\n  && installBioc.r ComplexHeatmap && installGithub.r koncina/ek.plot\n\nARG QUARTO_VERSION=\"1.4.537\"\nRUN curl -LO https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb && \\\n    apt-get update -qq && apt-get -y install \\\n    ./quarto-${QUARTO_VERSION}-linux-amd64.deb && rm quarto-${QUARTO_VERSION}-linux-amd64.deb \\\n    && apt autoremove -y && apt clean -y && rm -rf /var/lib/apt/lists/*\n\n\nDESCRIPTION file\nI guess it could also be shortened to the Imports field only.\nPackage: bloguilur\nTitle: bloguilur Companion\nVersion: 1.0.0\nAuthors@R: \n    person(\"Aurélien\", \"Ginolhac\", , \"aurelien.ginolhac@uni.lu\", role = c(\"aut\", \"cre\"))\nDescription: Companion website University of Luxembourg\nLicense: MIT + file LICENSE\nURL: https://r-training.pages.uni.lu/bloguilur/\nDepends:\n  R (&gt;= 4.1.0)\nImports:\n  countdown,\n  cropcircles,\n  fontawesome,\n  ggimage,\n  ggplot2,\n  glue,\n  gt,\n  gtExtras,\n  huxtable,\n  knitr,\n  ragg\nEncoding: UTF-8\nLanguage: en-US\nRoxygenNote: 7.2.3\n\n\nGitlab CI configuration\n.gitlab-ci.yml file, showing only the build_site stage:\nbuild_site:\n  stage: build_site\n  image: $CI_REGISTRY_IMAGE:latest\n  cache:\n    key: ${CI_JOB_NAME}\n    paths:\n      - _site/\n      - _freeze/\n  artifacts:\n    name: \"$CI_JOB_NAME\"\n    expire_in: 2 days\n    paths:\n      - _site/\n  before_script:\n    - |\n      Rscript -e \"read.dcf('DESCRIPTION', 'Imports') |&gt; \n        tools:::.split_dependencies() |&gt; \n        names() |&gt; \n        setdiff(tools:::.get_standard_package_names()$base) |&gt; \n        install.packages()\"\n  script:\n    - quarto render\n  tags:\n    - shared-cache\n  when: always\n\n\n\nResults\nBuilding the Docker image took 5 minutes 30 seconds.\n […] \nBuilding the website took 1 minutes 7 seconds.\n […] \nOnce the  image is there, the site building + publishing on the  pages took 1 minutes 47 seconds."
  },
  {
    "objectID": "posts/2024-01-09_r2u_ci/index.html#github-actions",
    "href": "posts/2024-01-09_r2u_ci/index.html#github-actions",
    "title": "r2u for faster continuous integration",
    "section": "GitHub  Actions",
    "text": "GitHub  Actions\nFor this blog, the config file is:\non:\n  push:\n    branches:\n      - main\n\nname: Build and Publish blog\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    container:\n      image: rocker/r2u:latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: System Dependencies\n        run: |\n          apt update -qq && apt install --yes --no-install-recommends cmake git \\\n          curl jq netbase \\\n          libxml2-dev libcurl4-openssl-dev libmagick++-dev\n      - name: Mixed packages\n        run: |\n          install.r tidyverse rmarkdown BiocManager Rcpp V8 htmlwidgets magick \\\n          && installBioc.r ComplexHeatmap && installGithub.r koncina/ek.plot\n      - name: Package Dependencies\n        run: |\n          Rscript -e \"read.dcf('DESCRIPTION', 'Imports') |&gt; \n            tools:::.split_dependencies() |&gt; \n            names() |&gt; \n            setdiff(tools:::.get_standard_package_names()$base) |&gt; \n            install.packages()\"\n      - name: Set up Quarto\n        env:\n          QUARTO_VERSION: \"1.4.538\"\n        run: |\n          curl -s -LO https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb && \\\n          apt-get update -qq && apt-get -y install \\\n          ./quarto-${QUARTO_VERSION}-linux-amd64.deb && rm quarto-${QUARTO_VERSION}-linux-amd64.deb\n      - name: Publish Quarto blog\n        uses: quarto-dev/quarto-actions/render@v2"
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html",
    "href": "posts/2024-03-25_hpc-easybuild/index.html",
    "title": "Creating modules with Easybuild ",
    "section": "",
    "text": "Photo by Omar Flores on Unsplash"
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#rationale",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#rationale",
    "title": "Creating modules with Easybuild ",
    "section": "Rationale",
    "text": "Rationale\nGetting rid of this hell, nicely summarized by xkcd:\n\n\n\nPython Environment by xkcd\n\n\nAnd this is not complete, because it misses mamba. And since it is not enough, the same team released micromamba. I attended a Python class in December 2023 where the instructor recommended micromamba even on HPC clusters. I asked if any of conda, miniconda … mamba is deprecated, the answer was “no, since they all serve different purposes”.\nTo add one more to this madness, I recently learnt about pipx which again propose Python apps in isolated environments.\nNow I understand why Python2 and 3 co-existed for so many years."
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#easybuild",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#easybuild",
    "title": "Creating modules with Easybuild ",
    "section": "Easybuild",
    "text": "Easybuild\n\nEasybuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way.\n\nI wanted to have SnakeMake running on HPC using this solution that already exists for more general software on our University HPC. Currently, I was doing:\nmodule load tools/Singularity\nconda activate snakemake\nWhich is mixing up things, I need module to get singularity1 but for snakemake I need some virtual envs. And in my home, some different Python modules, all enclosed, I was several times reaching the hard limit of 1M inodes quota.\n\n\n\n\n\nEasybuild logo\n\n\nXavier explained me how to properly create my own module with Easybuild. Thanks a LOT!"
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#easybuild-recipe",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#easybuild-recipe",
    "title": "Creating modules with Easybuild ",
    "section": "Easybuild recipe",
    "text": "Easybuild recipe\nHis first advice was to look into the available easyconfigs.\nAnd indeed, SnakeMake was present, actually not the latest version but close to. I went into the PyPi homepage and checked the sha256sum and updated to the version 8.9.0 along with the dependencies that were updated.\nThe resulting easyconfig is:\n\n\nfile: snakemake-8.9.0-foss-2023a.eb\n\neasyblock = 'PythonBundle'\n\nname = 'snakemake'\nversion = '8.9.0'\n\nhomepage = 'https://snakemake.readthedocs.io'\ndescription = \"The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\"\n\ntoolchain = {'name': 'foss', 'version': '2023a'}\n\nbuilddependencies = [\n    ('poetry', '1.5.1'),\n]\ndependencies = [\n    ('Python', '3.11.3'),\n    ('Python-bundle-PyPI', '2023.06'),\n    ('SciPy-bundle', '2023.07'),\n    ('GitPython', '3.1.40'),\n    ('IPython', '8.14.0'),\n    ('PyYAML', '6.0'),\n    ('wrapt', '1.15.0'),\n    ('PuLP', '2.8.0'),\n]\n\nuse_pip = True\nsanity_pip_check = True\n\nexts_list = [\n    ('datrie', '0.8.2', {\n        'checksums': ['525b08f638d5cf6115df6ccd818e5a01298cd230b2dac91c8ff2e6499d18765d'],\n    }),\n    ('plac', '1.4.2', {\n        'checksums': ['b0d04d9bc4875625df45982bc900e9d9826861c221850dbfda096eab82fe3330'],\n    }),\n    ('dpath', '2.1.6', {\n        'checksums': ['f1e07c72e8605c6a9e80b64bc8f42714de08a789c7de417e49c3f87a19692e47'],\n    }),\n    ('yte', '1.5.4', {\n        'checksums': ['d2d77e53eafca74f58234fcd3fea28cc0a719e4f3784911511e35e86594bc880'],\n    }),\n    ('toposort', '1.10', {\n        'checksums': ['bfbb479c53d0a696ea7402601f4e693c97b0367837c8898bc6471adfca37a6bd'],\n    }),\n    ('throttler', '1.2.2', {\n        'checksums': ['d54db406d98e1b54d18a9ba2b31ab9f093ac64a0a59d730c1cf7bb1cdfc94a58'],\n    }),\n    ('stopit', '1.1.2', {\n        'checksums': ['f7f39c583fd92027bd9d06127b259aee7a5b7945c1f1fa56263811e1e766996d'],\n    }),\n    ('ConfigArgParse', '1.7', {\n        'checksums': ['e7067471884de5478c58a511e529f0f9bd1c66bfef1dea90935438d6c23306d1'],\n    }),\n    ('argparse-dataclass', '2.0.0', {\n        'modulename': 'argparse_dataclass',\n        'source_tmpl': 'argparse_dataclass-%(version)s.tar.gz',\n        'checksums': ['09ab641c914a2f12882337b9c3e5086196dbf2ee6bf0ef67895c74002cc9297f'],\n    }),\n    ('snakemake-interface-common', '1.17.1', {\n        'modulename': 'snakemake_interface_common',\n        'source_tmpl': 'snakemake_interface_common-%(version)s.tar.gz',\n        'checksums': ['555c8218d9b68ddc1046f94a517e7d0f22e15bdc839d6ce149608d8ec137b9ae'],\n    }),\n    ('reretry', '0.11.8', {\n        'checksums': ['f2791fcebe512ea2f1d153a2874778523a8064860b591cd90afc21a8bed432e3'],\n    }),\n    ('snakemake-interface-storage-plugins', '3.1.1', {\n        'modulename': 'snakemake_interface_storage_plugins',\n        'source_tmpl': 'snakemake_interface_storage_plugins-%(version)s.tar.gz',\n        'checksums': ['d4d2b72ac964f12c5ba343639499c797316d6368dda471fba63610aec8e77cbb'],\n    }),\n    ('snakemake-interface-executor-plugins', '9.0.0', {\n        'modulename': 'snakemake_interface_executor_plugins',\n        'source_tmpl': 'snakemake_interface_executor_plugins-%(version)s.tar.gz',\n        'checksums': ['22b7337d9ea4f9e32679b96fa873337608d73f2d41443cc6bde18de4549acdb7'],\n    }),\n    ('smart-open', '6.4.0', {\n        'sources': ['smart_open-%(version)s.tar.gz'],\n        'checksums': ['be3c92c246fbe80ebce8fbacb180494a481a77fcdcb7c1aadb2ea5b9c2bee8b9'],\n\n    ('jupyter-core', '5.7.1', {\n        'modulename': 'jupyter_core',\n        'source_tmpl': 'jupyter_core-%(version)s.tar.gz',\n        'checksums': ['de61a9d7fc71240f688b2fb5ab659fbb56979458dc66a71decd098e03c79e218'],\n    }),\n    ('fastjsonschema', '2.19.1', {\n        'checksums': ['e3126a94bdc4623d3de4485f8d468a12f02a67921315ddc87836d6e456dc789d'],\n    }),\n    ('nbformat', '5.9.2', {\n        'source_tmpl': '%(name)s-%(version)s-py3-none-any.whl',\n        'checksums': ['1c5172d786a41b82bcfd0c23f9e6b6f072e8fb49c39250219e4acfff1efe89e9'],\n    }),\n    ('immutables', '0.20', {\n        'checksums': ['1d2f83e6a6a8455466cd97b9a90e2b4f7864648616dfa6b19d18f49badac3876'],\n    }),\n    ('humanfriendly', '10.0', {\n        'checksums': ['6b0b831ce8f15f7300721aa49829fc4e83921a9a301cc7f606be6686a2288ddc'],\n    }),\n    ('connection-pool', '0.0.3', {\n        'sources': ['connection_pool-%(version)s.tar.gz'],\n        'checksums': ['bf429e7aef65921c69b4ed48f3d48d3eac1383b05d2df91884705842d974d0dc'],\n    }),\n    ('conda-inject', '1.3.1', {\n        'sources': ['conda_inject-%(version)s.tar.gz'],\n        'checksums': ['9e8d902230261beba74083aae12c2c5a395e29b408469fefadc8aaf51ee441e5'],\n    }),\n    (name, version, {\n        'checksums': ['1c36d231da92a1e37ab9f96d35346f5268949fbd1cebf4c9d429816a05538066'],\n    }),\n    # Also install some of the snakemake executors\n    ('snakemake-executor-plugin-slurm-jobstep', '0.1.11', {\n        'modulename': 'snakemake_executor_plugin_slurm_jobstep',\n        'source_tmpl': 'snakemake_executor_plugin_slurm_jobstep-%(version)s.tar.gz',\n        'checksums': ['cafdac937796ab0dfc0354c42380167a44a1db00c4edc98ab736a6ace2201a94'],\n    }),\n    ('snakemake-executor-plugin-flux', '0.1.1', {\n        'modulename': 'snakemake_executor_plugin_flux',\n        'source_tmpl': 'snakemake_executor_plugin_flux-%(version)s.tar.gz',\n        'checksums': ['26655bd1cf5d7db5dfcfdfbd006c1db35968c0ad1772e0b010e64e6f71b00163'],\n    }),\n    ('snakemake-executor-plugin-slurm', '0.4.2', {\n        'modulename': 'snakemake_executor_plugin_slurm',\n        'source_tmpl': 'snakemake_executor_plugin_slurm-%(version)s.tar.gz',\n        'checksums': ['265ffff24cdaa7929769bdbe822c39d8ac059b0642e92fc6fa9e55c9cdc7d018'],\n    }),\n    ('snakemake-executor-plugin-cluster-sync', '0.1.4', {\n        'modulename': 'snakemake_executor_plugin_cluster_sync',\n        'source_tmpl': 'snakemake_executor_plugin_cluster_sync-%(version)s.tar.gz',\n        'checksums': ['6a6dcb2110d4c2ee74f9a48ea68e0fd7ddd2800672ebef00a01faa4affa835ad'],\n    }),\n    ('snakemake-executor-plugin-cluster-generic', '1.0.9', {\n        'modulename': 'snakemake_executor_plugin_cluster_generic',\n        'source_tmpl': 'snakemake_executor_plugin_cluster_generic-%(version)s.tar.gz',\n        'checksums': ['ad0dc2d8bde7d4f336364bebe11a3b2209653c481ce8fbb0ae8bec81016a9a14'],\n    }),\n    ('snakemake-interface-report-plugins', '1.0.0', {\n        'modulename': 'snakemake_interface_report_plugins',\n        'source_tmpl': 'snakemake_interface_report_plugins-%(version)s.tar.gz',\n        'checksums': ['02311cdc4bebab2a1c28469b5e6d5c6ac6e9c66998ad4e4b3229f1472127490f'],\n    }),\n]\n\nsanity_check_paths = {\n    'files': ['bin/snakemake'],\n    'dirs': ['lib/python%(pyshortver)s/site-packages/snakemake'],\n}\n\nsanity_check_commands = ['snakemake --help']\n\nmoduleclass = 'tools'\n\nNow we need to build it with actually all the FOSS 2023a toolchain."
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#slurm-launcher",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#slurm-launcher",
    "title": "Creating modules with Easybuild ",
    "section": "slurm launcher",
    "text": "slurm launcher\nXavier also advice to install the modules inside a project folder to ease collaboration and sharing.\nIt takes ~ 6 hours, so the passive launcher was:\n#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -J eb-snakemake\n#SBATCH --ntasks-per-node=1\n#SBATCH -c 2\n#SBATCH --mem=30GB\n#SBATCH --time=0-07:00:00\n#SBATCH -p batch\n\nset -euo pipefail\n\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Install SnakeMake 8.9.0 with toolchain foss/2023a\n# Author: Xavier Besseron / A. Ginolhac\n# Date: 2024-03-20\n\nmodule use \"$HOME/.local/easybuild/modules/all\"\nmodule load tools/EasyBuild/4.9.0\n\n# Installation path,\n# if you want something different than ~/.local/easybuild\n# for example to share with other users\nmkdir -p /work/projects/xxx/easybuild/\nexport EASYBUILD_PREFIX=/work/projects/xxx/easybuild/\n\n# Build directory, to setup build and avoid quota issues in home directory                                                                    \nexport EASYBUILD_BUILDPATH=\"/dev/shm\"\n\n# Build and install SnakeMake and all the required dependencies\n# This can take quite some time to compile and install everything.\n# GCCcore took 1h20min, Rust 1h10min. Total 5h40\n\n# /!\\ If you install in a project directory, you should this to avoid quota issues                                                            \nsg xxx -c 'eb snakemake-8.9.0-foss-2023a.eb -r .'\n\n\n# Use the path given in EASYBUILD_INSTALL suffixed by 'modules/all/'\n# (This can be put in your ~/.bashrc if you like)\nmodule use /work/projects/xxx/easybuild/modules/all/\n\n# Load the SnakeMake module\nmodule load tools/snakemake/8.9.0-foss-2023a\n\n# Check the SnakeMake version\nsnakemake --version\nSee the specific recommendations as comments."
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#setting",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#setting",
    "title": "Creating modules with Easybuild ",
    "section": "Setting",
    "text": "Setting\nNow that the modules are installed, we need to tell Easybuild where to find them. One can use the module use ... command for a one-time session, or use this for more persistent usage.\ncommand -v module &gt;/dev/null 2&gt;&1 && module use /work/projects/xxx/easybuild/modules/all\nThe command -v ... is to avoid running it on the access node but only computing nodes."
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#testing",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#testing",
    "title": "Creating modules with Easybuild ",
    "section": "Testing",
    "text": "Testing\n$ module spider snakemake\n\n-------------------------------------------------------------------------------------------------------------------------------------------\n  tools/snakemake:\n-------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\n\n     Versions:\n        tools/snakemake/8.4.2-foss-2023a\n        tools/snakemake/8.9.0-foss-2023a\n\n-------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"tools/snakemake\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider tools/snakemake/8.9.0-foss-2023a\n-------------------------------------------------------------------------------------------------------------------------------------------\nmodule load tools/snakemake\nsnakemake --version\n8.9.0"
  },
  {
    "objectID": "posts/2024-03-25_hpc-easybuild/index.html#footnotes",
    "href": "posts/2024-03-25_hpc-easybuild/index.html#footnotes",
    "title": "Creating modules with Easybuild ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNow apptainer↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Github\n  \n  \n    \n     Mastodon\n  \n\n      \n\n\nI am working at the University of Luxembourg; in the Departement of Life Sciences and Medecine, providing services to the researchers.\n— Acknowledgment provided by acknowledge-the-climate-crisis.org\nImage of the 🌏 🌡️ 1850-2023 from ShowYourStripes"
  },
  {
    "objectID": "about.html#licence",
    "href": "about.html#licence",
    "title": "About",
    "section": "Licence",
    "text": "Licence\n  This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License except for the borrowed and mentioned with proper source: statements."
  },
  {
    "objectID": "karate/index.html",
    "href": "karate/index.html",
    "title": "Karate tabs",
    "section": "",
    "text": "This page is dedicated to tabs from the band karate made by fans. These tabs came from the original website (today offline) made by Willem Holthuis.\nGeoff Farina is the Karate’s guitarist, many thanks to him for writing such nice songs. Have fun playing the incredible music made by Karate and please feel free to correct the tabs by sending an email to his author if you find any mistakes or new interpretations."
  },
  {
    "objectID": "karate/index.html#in-the-fisk-tank-2005",
    "href": "karate/index.html#in-the-fisk-tank-2005",
    "title": "Karate tabs",
    "section": "In the Fisk tank (2005)",
    "text": "In the Fisk tank (2005)\n\n\n\n\nSong\nAuthor\n\n\n\n\nStrange Fruit\nAurelien"
  },
  {
    "objectID": "karate/index.html#pockets-2004",
    "href": "karate/index.html#pockets-2004",
    "title": "Karate tabs",
    "section": "Pockets (2004)",
    "text": "Pockets (2004)\n\n\n\n\nSong\nAuthor\n\n\n\n\nWith Age\nTomatokiller\n\n\nTow Truck\nAurelien & Philip Tanimura\n\n\nWater\nMito Gegic\n\n\nThe State I’m in aka Goode Buy from Cobbs Creek Park\nMito Gegic\n\n\nPines\nAurelien"
  },
  {
    "objectID": "karate/index.html#some-boots-2002",
    "href": "karate/index.html#some-boots-2002",
    "title": "Karate tabs",
    "section": "Some boots (2002)",
    "text": "Some boots (2002)\n\n\n\n\nSong\nAuthor\n\n\n\n\nOriginal Spies\nPhilip Tanimura\n\n\nFirst Release\nAurelien Ginolhac & Philip Tanimura\n\n\nAirport\nPhilip Tanimura\n\n\nCorduroy\nPhilip Tanimura\n\n\nRemain Relaxed\nPhilip Tanimura & Aurelien"
  },
  {
    "objectID": "karate/index.html#cancel-sing-2001",
    "href": "karate/index.html#cancel-sing-2001",
    "title": "Karate tabs",
    "section": "Cancel / Sing (2001)",
    "text": "Cancel / Sing (2001)\n\n\n\n\nSong\nAuthor\n\n\n\n\nSing\nPhilip Tanimura"
  },
  {
    "objectID": "karate/index.html#unsolved-2000",
    "href": "karate/index.html#unsolved-2000",
    "title": "Karate tabs",
    "section": "Unsolved (2000)",
    "text": "Unsolved (2000)\n\n\n\n\nSong\nAuthor\n\n\n\n\nSmall Fires\nPhilip Tanimura\n\n\nThe lived-but-yet-named\nPhilip Tanimura\n\n\nSever\nPhilip Tanimura\n\n\nThe Roots and the Ruins\nPhilip Tanimura\n\n\nThe Roots and the Ruins (bass)\nCraig Wynne\n\n\nNumber Six\nCraig Wynne\n\n\nNumber Six (bass)\nCraig Wynne\n\n\nOne Less Blues\nAurelien & Antoine L.\n\n\nThe Angels Just Have to Show\nAurelien & Antoine L.\n\n\nThe Halo of the Strange\nAurelien & Philip Tanimura\n\n\nThis Day Next Year\nPhilip Tanimura\n\n\nThis Day Next Year (solo)\nCraig Wynne"
  },
  {
    "objectID": "karate/index.html#the-bed-is-in-the-ocean-1998",
    "href": "karate/index.html#the-bed-is-in-the-ocean-1998",
    "title": "Karate tabs",
    "section": "The Bed is in the Ocean (1998)",
    "text": "The Bed is in the Ocean (1998)\n\n\n\n\nSong\nAuthor\n\n\n\n\nThere are Ghosts\nPhilip Tanimura\n\n\nDiazapam\nPhilip Tanimura\n\n\nThe Last Wars\nPhilip Tanimura\n\n\nBass Sound\nAurelien\n\n\nUp nigths\nAurelien\n\n\nThe Same Stars\nPhilip Tanimura\n\n\nOutside is the Drama\nPhilip Tanimura\n\n\nFatal Strategies\nAurelien"
  },
  {
    "objectID": "karate/index.html#in-place-of-real-insight-1997",
    "href": "karate/index.html#in-place-of-real-insight-1997",
    "title": "Karate tabs",
    "section": "In Place of Real Insight (1997)",
    "text": "In Place of Real Insight (1997)\n\n\n\n\nSong\nAuthor\n\n\n\n\nNew Martini\nEeYoRe\n\n\nBass Sound\nAurelien\n\n\nToday or Tomorrow\nAurelien\n\n\nThis Plus Slow Song\nCraig Wynne"
  },
  {
    "objectID": "karate/index.html#operation-sand-1997",
    "href": "karate/index.html#operation-sand-1997",
    "title": "Karate tabs",
    "section": "Operation Sand (1997)",
    "text": "Operation Sand (1997)\n\n\n\n\nSong\nAuthor\n\n\n\n\nOperation Sand\nPhilip Tanimura"
  },
  {
    "objectID": "karate/index.html#karate-1995",
    "href": "karate/index.html#karate-1995",
    "title": "Karate tabs",
    "section": "Karate (1995)",
    "text": "Karate (1995)\n\n\n\n\nSong\nAuthor\n\n\n\n\nIf You Can Hold Your Breath\nPete DeStefano\n\n\nEvery Sister\nPhilip Tanimura\n\n\nCaffeine or Me?\nPhilip Tanimura"
  },
  {
    "objectID": "posts/2023-02-20_crop-circles/index.html",
    "href": "posts/2023-02-20_crop-circles/index.html",
    "title": "Crop Circles",
    "section": "",
    "text": "From the repo on GitHub.\nlibrary(cropcircles)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ggimage)\nlibrary(ragg)\n\n# breaking bad images\nx &lt;- c(1, 3, 9, 8)\n# initial fetch\n#images &lt;- glue::glue(\"https://openpsychometrics.org/tests/characters/test-resources/pics/BB/{x}.jpg\")\nimages &lt;- glue::glue(\"{x}.jpg\")\n\n# border colours\nborder_cols &lt;- colorRampPalette(c(\"black\", \"brown4\"))(4)\n  \ndf &lt;- tibble(y = 1:4, images = images) |&gt; \n  mutate(images_circle = circle_crop(images, border_size = 16, border_colour = border_cols))\n\ndf |&gt; \n  ggplot() +\n  geom_image(aes(1.5, y, image = images), size = 0.15) +\n  geom_image(aes(3.5, y, image = images_circle), size = 0.15) +\n  xlim(0, 5) +\n  ylim(0, 5) +\n  coord_fixed()"
  },
  {
    "objectID": "posts/2023-10-02_resume-parallel/index.html",
    "href": "posts/2023-10-02_resume-parallel/index.html",
    "title": "Resume long parallel jobs",
    "section": "",
    "text": "The current max wall-time of normal passive1 jobs on our job scheduler is 48 hours. In this post we will see how:"
  },
  {
    "objectID": "posts/2023-10-02_resume-parallel/index.html#real-example",
    "href": "posts/2023-10-02_resume-parallel/index.html#real-example",
    "title": "Resume long parallel jobs",
    "section": "Real example",
    "text": "Real example\nFollowing the download of FASTQ (actually 74 paired-end samples), this single-end transcriptomics study had 96 cells identified per sample. The linked GitHub repository provided how to demultiplex those cells for one sample.\n\nCorrespondences UMI - samples\nFor this task, of course we need to perform the barcode split for all 74 samples. The supplied correspondences between UMI and names were in 96 lines files like:\nAACGTGAT    P4_LyM_B1_1\nAAACATCG    P4_LyM_B1_2\nATGCCTAA    P4_LyM_B1_3\nAGTGGTCA    P4_LyM_B1_4\n[...]\nGATGAATC    P4_LyM_B1_95\nGCCAAGAC    P4_LyM_B1_96\nOn one side we have the SRA accession IDs from FASTQ filenames and on the other the UMI / sample names. To create the command lines, with use  (only partial code shown) to match those\ntibble(barcodes = fs::dir_ls(\"resources/GSE110009_RAW/\", glob = \"*.txt\")) |&gt;\n  mutate(GEO_Accession = str_extract(barcodes, \"GSM\\\\d{7}\")) |&gt; \n  right_join(sra_table, join_by(GEO_Accession == `GEO_Accession (exp)`)) -&gt; sra_geo\nsra_geo\n# A tibble: 74 × 32\n   barcodes      GEO_Accession Run   `Assay Type` AvgSpotLen   Bases BioProject BioSample   Bytes `Center Name`\n   &lt;fs::path&gt;    &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        \n 1 …283_P3-N.txt GSM2976283    SRR6… RNA-Seq             300 7.09e10 PRJNA4325… SAMN0845… 3.58e10 GEO          \n 2 …4_P3-PT1.txt GSM2976284    SRR6… RNA-Seq             300 6.01e10 PRJNA4325… SAMN0845… 3.01e10 GEO          \n 3 …5_P3-PT2.txt GSM2976285    SRR6… RNA-Seq             300 5.34e10 PRJNA4325… SAMN0845… 2.67e10 GEO          \n 4 …6_P3-PT3.txt GSM2976286    SRR6… RNA-Seq             300 6.58e10 PRJNA4325… SAMN0845… 3.26e10 GEO          \n 5 …7_P3-PT4.txt GSM2976287    SRR6… RNA-Seq             300 6.20e10 PRJNA4325… SAMN0845… 3.10e10 GEO          \n 6 …_P4-LyM1.txt GSM2976289    SRR6… RNA-Seq             300 6.30e10 PRJNA4325… SAMN0845… 3.16e10 GEO          \n 7 …291_P4-N.txt GSM2976291    SRR6… RNA-Seq             300 6.18e10 PRJNA4325… SAMN0845… 3.01e10 GEO          \n 8 …2_P4-PT1.txt GSM2976292    SRR6… RNA-Seq             300 6.49e10 PRJNA4325… SAMN0845… 3.29e10 GEO          \n 9 …3_P4-PT2.txt GSM2976293    SRR6… RNA-Seq             300 6.08e10 PRJNA4325… SAMN0845… 3.08e10 GEO          \n10 …1-1-lyM2.txt GSM2976294    SRR6… RNA-Seq             300 5.77e10 PRJNA4325… SAMN0845… 2.88e10 GEO          \n# ℹ 64 more rows\n# ℹ 22 more variables: Consent &lt;chr&gt;, `DATASTORE filetype` &lt;chr&gt;, `DATASTORE provider` &lt;chr&gt;,\n#   `DATASTORE region` &lt;chr&gt;, Experiment &lt;chr&gt;, Instrument &lt;chr&gt;, LibraryLayout &lt;chr&gt;, LibrarySelection &lt;chr&gt;,\n#   LibrarySource &lt;chr&gt;, Organism &lt;chr&gt;, Platform &lt;chr&gt;, ReleaseDate &lt;dttm&gt;, create_date &lt;dttm&gt;,\n#   version &lt;dbl&gt;, `Sample Name` &lt;chr&gt;, source_name &lt;chr&gt;, `SRA Study` &lt;chr&gt;, disease_state &lt;chr&gt;, Type &lt;chr&gt;,\n#   patient_disease &lt;chr&gt;, PATIENT_ID &lt;dbl&gt;, Tissue &lt;chr&gt;\n\n\nBuilding up the full command lines\nFrom this tibble we can use {glue} to create the commands that will be further run by parallel\n\n\n\n\n\nglue logo\n\n\nglue::glue_data(sra_geo, \n \"mkdir -p split_{Run}; srun -c 4 -n1 --cpu-bind=cores perl resources/s01.Barcode_UMI_QC_per1w_V2.pl {Run}/{Run}_1.fastq.gz {Run}/{Run}_2.fastq.gz {barcodes} split_{Run}\") |&gt; \n  write_lines(\"xxxx/split_barcodes_cmd.txt\")\nWhat we do here and to take the tibble and use an implementation of Literal String Interpolation (f strings in ). For every line, relevant column are inserted with the curly braces with the column name is used.\nFinally, the 74 lines file split_barcodes_cmd.txt looks like:\nmkdir -p split_SRR6662774; srun -c 4 -n1 --cpu-bind=cores perl resources/s01.Barcode_UMI_QC_per1w_V2.pl SRR6662774/SRR6662774_1.fastq.gz SRR6662774/SRR6662774_2.fastq.gz resources/GSE110009_RAW/GSM297628\n3_P3-N.txt split_SRR6662774\nmkdir -p split_SRR6662775; srun -c 4 -n1 --cpu-bind=cores perl resources/s01.Barcode_UMI_QC_per1w_V2.pl SRR6662775/SRR6662775_1.fastq.gz SRR6662775/SRR6662775_2.fastq.gz resources/GSE110009_RAW/GSM297628\n4_P3-PT1.txt split_SRR6662775\n[...] 70 more rows\n\n\nGet parallel to handle multiple commands\nThe command xargs is on all servers and is very useful to build on command lines.\nSomething like this to update permissions on folders is handy:\nfind /path -type d | xargs chmod g+w\nHowever, even if there is the option -P it is not so easy to control how many tasks are run in parallel. The parallel command though is easier to learn, powerful but might not be installed on all HPC.\nSee now the launcher for the passive job of treating the 74 created commands using parallel:\n#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -J split-barcode\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --mail-user=xxx@uni.lu\n#SBATCH --ntasks-per-node=12\n#SBATCH -c 4\n#SBATCH --time=2-00:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\nset -euo pipefail\n\nmodule load lang/Perl\n\nparallel --delay 0.2 --joblog slurm_split-${SLURM_JOB_ID} \\\n  -j ${SLURM_NTASKS} \"{}\" :::: split_barcodes_cmd.txt\n\nSlurm directives\nThe interesting change is that we ask for 12 tasks with each 4 cores (lines 6 & 7). We must emphasise that we then request for 48 cores (see margin).\n\n\n$ seff 1179214\nJob ID: 1179214\nCluster: aion\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 48\nCPU Utilized: 12-13:04:52\nCPU Efficiency: 25.71% of 48-19:09:36 core-walltime\nJob Wall-clock time: 1-00:23:57\nMemory Utilized: 1.12 GB\nMemory Efficiency: 1.36% of 82.03 GB\nLine 17 is to have an updated Perl interpreter pre-compiled by our infrastructure.\n\n\nParallel options\nLet’s split up the lines 19-203.\n\n--delay is recommended for filesystem that could be slower than local SSD\n--joblog is the key part of this post, it register the completed execution of commands\n-j sets how many tasks are run in parallel, using here the 12 from line 6\n\"{}\" is the parallel placeholder for where to insert command, here the full command is to be read4\nQuadruple colon :::: means read entries from the file split_barcodes_cmd.txt"
  },
  {
    "objectID": "posts/2023-10-02_resume-parallel/index.html#joblog",
    "href": "posts/2023-10-02_resume-parallel/index.html#joblog",
    "title": "Resume long parallel jobs",
    "section": "Joblog",
    "text": "Joblog\nThe text log file from parallel looks like:\nSeq     Host    Starttime       JobRuntime      Send    Receive Exitval Signal  Command\n9       :       1695819029.000    7482.000      0       4945    0       0       mkdir -p split_[...]\n3       :       1695819023.000    7819.000      0       4932    0       0       mkdir -p split_[...]\n1       :       1695819021.000   10040.000      0       4757    0       0       mkdir -p split_[...]\n[...]\nWe see that the first command took longer (10040 seconds) so finished third in the execution.\nHere the job took 24 hours and 23 minutes, no need to resume it, the booked time was sufficient."
  },
  {
    "objectID": "posts/2023-10-02_resume-parallel/index.html#launcher-script-with-exported-bash-function",
    "href": "posts/2023-10-02_resume-parallel/index.html#launcher-script-with-exported-bash-function",
    "title": "Resume long parallel jobs",
    "section": "Launcher script with exported BASH function",
    "text": "Launcher script with exported BASH function\n#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -J map-barcode\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --mail-user=xxxx@uni.lu\n#SBATCH --ntasks-per-node=4\n#SBATCH --exclusive\n#SBATCH -c 12\n#SBATCH --time=2-00:00:00\n#SBATCH -p batch\n\nprint_error_and_exit() { echo \"***ERROR*** $*\"; exit 1; }\nmodule purge || print_error_and_exit \"No 'module' command\"\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\nset -euo pipefail\n\ndoit() {\n  f=\"$1\"\n  sample=\"$(dirname \"$f\")\"\n  mkdir -p \"$sample\"\n  STAR --runThreadN \"${SLURM_CPUS_PER_TASK}\" --genomeDir GRCh37 --readFilesIn \"$f\" --readFilesCommand zcat \\\n  --outFileNamePrefix star/\"$sample\"/ --outStd Log  --outSAMtype BAM SortedByCoordinate \\\n  --alignEndsType Local --quantMode GeneCounts --twopassMode Basic\n}\n\nexport -f doit\n\n#parallel --delay 0.2 --joblog slurm_mapping-${SLURM_JOB_ID} -j ${SLURM_NTASKS} \"doit {}\" ::: split_SRR*/*/*R1.clean.fq.gz\nparallel --delay 0.2 --resume --joblog slurm_mapping-1185148 -j ${SLURM_NTASKS} \"doit {}\" ::: split_SRR*/*/*R1.clean.fq.gz\n\nSlurm directives\nNothing much than last time except we adapt to a more intense job, more more cores by task so less tasks per node. The --exclusive is to book the entire node (to get hands on the full RAM: 218.75 GB)\n\n\nDefining a BASH function\nA long line of parallel could be done and with Perl string replacements but the convenience is not meeting my expectations.\nThe parallel author himself is giving a nice answer on how to use a BASH function instead.\nThe function named doit is defined between lines 18 to 25 within curly braces.\n\n\n\n\n\n\nExport the function!\n\n\n\nAt line 27, otherwise parallel won’t be able to use it\n\n\nThe string manipulation are done in BASH where I am more familiar with and we can break long lines with backslashes.\n\n\nFirst pass of parallel\nFor which 48 hours were not enough: line 29.\nUsing the --joblog is mandatory so we log exactly which commands were done.\nIn 48 hours 5,877 mapping were performed (83.9%).\n\n\nSecond pass for the left over.\nWe are missing 1,131 mapping. I commented out the line 29 and made two important changes:\n\n--resume option, parallel is then aware it needs to log in the joblog and skip commands already performed\nUse not the jobid from slurm but the joblog file from the first pass: slurm_mapping-1185148.\n\n\n\nOutcome\nAfter 8 hours and 22 minutes, the last 1,131 mapping were done, giving the final mapping yes but also a complete job log:\nwc slurm_mapping-1185148\n  7009  70089 737229 slurm_mapping-1185148\n7,008 + header.\nMapping took 48 + 8 = 56 hours and 22 minutes."
  },
  {
    "objectID": "posts/2023-10-02_resume-parallel/index.html#footnotes",
    "href": "posts/2023-10-02_resume-parallel/index.html#footnotes",
    "title": "Resume long parallel jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ninteractive jobs are limited to 2 hours↩︎\nSee this article for explanations↩︎\nThe backslash \\ indicates that the long line is split in two but interpreted as one↩︎\nFor example, a more complex example for compressing text files, max 6 in parallel. The {} is the place holder for all filenames that end with the .tsv extension. The compression will done by gzip. The triple colon ::: to read from external commands, the quadruple :::: like the split barcode is to read from a file.\nparallel -j 6 \"gzip {}\" ::: *.tsv\n↩︎\nArtistic picture by Pop & Zebra from Unsplash↩︎"
  },
  {
    "objectID": "posts/2023-03-06_vertical-faceting/index.html",
    "href": "posts/2023-03-06_vertical-faceting/index.html",
    "title": "Vertical faceting",
    "section": "",
    "text": "The question is: how can we plot one variable for faceting vertically?\nAnd bonus, since we want through time the plot of very different measurements, we must have a free y-scale."
  },
  {
    "objectID": "posts/2023-03-06_vertical-faceting/index.html#tldr",
    "href": "posts/2023-03-06_vertical-faceting/index.html#tldr",
    "title": "Vertical faceting",
    "section": "TL;DR",
    "text": "TL;DR\nUse either:\n\nThe dir = \"v\" argument of facet_wrap()\nThe rows = vars(variable) argument of facet_grid()"
  },
  {
    "objectID": "posts/2023-03-06_vertical-faceting/index.html#demo",
    "href": "posts/2023-03-06_vertical-faceting/index.html#demo",
    "title": "Vertical faceting",
    "section": "Demo",
    "text": "Demo\nWe will use a file from the Aranet4 sensor using the mobile app provided by the company. This sensor measures 4 parameters every 2 to 10 minutes which are:\n\nCO2, using the infrared absorption property of greenhouse gases like carbon dioxide.\nTemperature in Celsius degrees.\nRelative humidity (in %).\nAtmospheric pressure in _h Pa_. This latter variable is exported but displayed on the device, we will skip it.\n\nThe CO2 measurement is performed as follows (from the user manual):\n\n\n\nCO2 aranet\n\n\nAccording to the file extension, it is a comma-separated values. But if you look at the file with a text editor, you will see that values are surrounded by double quotes and number are using commas for decimal separator. Moreover, the dates are not in a standard format unfortunately. But it is a real life example data.\nreadr will detect the double quotes and deal with it automatically but the other issues needs to be fixed:\n\nFor the Time(dd/mm/yyyy) column, the datetime format is specified.\nFor the number decimals as commas, you need to set the locale to locale(decimal_mark = \",\")\nSkip the last column (Atmospheric Pressure) as we won’t plot it.\n\n\naranet &lt;- read_csv(\"https://biostat2.uni.lu/practicals/data/Gino_2022-08-14T22_53_06+0200.csv\",\n                   col_types = cols(`Time(dd/mm/yyyy)` = col_datetime(format = \"%d/%m/%Y %H:%M:%S\")),\n                   locale = locale(decimal_mark = \",\"),\n                   col_select = -`Atmospheric pressure(hPa)`) |&gt; \n  rename(Time = `Time(dd/mm/yyyy)`)\naranet\n\n# A tibble: 4,033 × 4\n   Time                `Carbon dioxide(ppm)` `Temperature(°C)`\n   &lt;dttm&gt;                              &lt;dbl&gt;             &lt;dbl&gt;\n 1 2022-07-31 22:53:43                   457              24  \n 2 2022-07-31 22:58:43                   444              24.1\n 3 2022-07-31 23:03:43                   457              24  \n 4 2022-07-31 23:08:43                   444              24  \n 5 2022-07-31 23:13:43                   445              24  \n 6 2022-07-31 23:18:43                   454              24.1\n 7 2022-07-31 23:23:43                   432              24.1\n 8 2022-07-31 23:28:43                   442              24.1\n 9 2022-07-31 23:33:43                   469              24  \n10 2022-07-31 23:38:43                   455              24  \n# ℹ 4,023 more rows\n# ℹ 1 more variable: `Relative humidity(%)` &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2023-03-06_vertical-faceting/index.html#reshape-the-variables-in-the-long-format",
    "href": "posts/2023-03-06_vertical-faceting/index.html#reshape-the-variables-in-the-long-format",
    "title": "Vertical faceting",
    "section": "Reshape the variables in the long format",
    "text": "Reshape the variables in the long format\n\naranet |&gt;\n  pivot_longer(cols = -Time,\n               names_to = \"measure\",\n               values_to = \"value\") -&gt; aranet_long\naranet_long\n\n# A tibble: 12,099 × 3\n   Time                measure              value\n   &lt;dttm&gt;              &lt;chr&gt;                &lt;dbl&gt;\n 1 2022-07-31 22:53:43 Carbon dioxide(ppm)  457  \n 2 2022-07-31 22:53:43 Temperature(°C)       24  \n 3 2022-07-31 22:53:43 Relative humidity(%)  49  \n 4 2022-07-31 22:58:43 Carbon dioxide(ppm)  444  \n 5 2022-07-31 22:58:43 Temperature(°C)       24.1\n 6 2022-07-31 22:58:43 Relative humidity(%)  49  \n 7 2022-07-31 23:03:43 Carbon dioxide(ppm)  457  \n 8 2022-07-31 23:03:43 Temperature(°C)       24  \n 9 2022-07-31 23:03:43 Relative humidity(%)  49  \n10 2022-07-31 23:08:43 Carbon dioxide(ppm)  444  \n# ℹ 12,089 more rows"
  },
  {
    "objectID": "posts/2023-03-06_vertical-faceting/index.html#plot-the-values-per-time-and-facet-per-measurement",
    "href": "posts/2023-03-06_vertical-faceting/index.html#plot-the-values-per-time-and-facet-per-measurement",
    "title": "Vertical faceting",
    "section": "Plot the values per Time and facet per measurement",
    "text": "Plot the values per Time and facet per measurement\nMeasurement are one of the 3 variables: CO2 ppm, Temp and Relative Humidity.\nIn the facet command, you should free the y-axis with scales = \"free_y\"\nWe want variables in rows. Either facet_wrap() dir argument or facet_grid() rows = do the job.\n\nfacet_wrap() solution\nThe key part is to change the dir argument for vertical\n\nggplot(aranet_long, aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(vars(measure), scales = \"free_y\",\n             # panel titles next to the y axis\n             strip.position = \"bottom\", # use left for mimicking facet_grid below\n             dir = \"v\") +\n  scale_x_datetime(breaks = scales::date_breaks(\"1 day\"),\n                   date_labels = \"%b %d\") +\n  labs(x = NULL, y = NULL) +\n  theme_minimal(14)\n\n\n\n\n\n\n\n\n\n\nfacet_grid() solution\nfaacet_grid() is designed for two variables faceting, but we can omit one.\nThe key part is to use only the `rows`` argument for vertical panels.\n\nggplot(aranet_long, aes(x = Time, y = value)) +\n  geom_line() +\n  facet_grid(rows = vars(measure), scales = \"free_y\",\n             # panel titles next to the y axis\n             switch = \"y\") +\n  scale_x_datetime(breaks = scales::date_breaks(\"1 day\"),\n                   date_labels = \"%b %d\") +\n  labs(x = NULL, y = NULL) +\n  theme_minimal(14)"
  }
]